В данной работе будет реализована классификация видео на основе эмбедингов от VIDEOMAE Model

Теоретическая база 

Эмбединги/векторы признаков — это векторы, представляющие некоторую информацию о некоторой области/задаче, они являются выходными данными из сети или ее промежуточными значениями[2]. Векторизацию данных с помощью ембедингов можно назвать методом уменьшения размерности. Традиционные методы уменьшения размерности — PCA, LDA и т. д. — используют статистические методы для «сжатия» данных, например комбинация линейной алгебры[1]. Современные модели глубокого обучения выполняют уменьшение размерности,  давая представление входных данных, в котором соседние точки соответствуют семантически схожим первоначальных необработанных точкам данных. Например, то, что раньше было одним  вектором, представляющим слово, фразу, изображение и тп., теперь может быть представлено как плотный вектор со значительно меньшей размерностью. Например, если дать модели картинку размерностью 224×224 - получится размерность 50176, что очень много, однако сверточная модель может обработать эти данные и выдать представление изображения как признаки "высокого" уровня с размерностью гораздо меньшей, например 512. Такие данные больше представляют собой структурированный тип и для них уже можно использовать более простые модели.

Чтобы понять что содержат в себе эмбединги рассмотрим пример word-embedding. Их идея состоит в том, чтобы иметь векторы эмбедингов слов, близких по смыслу, чтобы они находились "рядом", но при этом были "далеки" от других слов. "Рядом" и "далеко" означают значение метрики расстояния, например: L2 (евклидово расстояние) или косинусное расстояние. Ключевой момент: каждое число вектора эмбединга соответствует чему-то непосредственно из первичных данных. Данный вектор не обязательно должен быть читаемым для человека, но он должен содержать информацию, с помощью которой более простая модель после эмбедингов может ее понять, например предугадать как близки слова walking-walked и как далеки walking-swam (рис. 1)

![image](https://user-images.githubusercontent.com/58371161/209578441-ca548e9a-6dbd-4d01-a586-7eb9b924322d.png)

Рис. 1 - пример word-embedding

Описание разработанной системы

В качестве эмбедингов для видео классификации будет использоваться VideoMAE[3] модель и датасет COIN. 
VideoMAE — это автокодеровщик с маскировкой видео (MAE) для видео[4]. Архитектура (рис. 2) модели очень похожа на архитектуру стандартного Vision Transformer (ViT) с декодером сверху для прогнозирования значений пикселей для замаскированных участков.

![image](https://user-images.githubusercontent.com/58371161/209579604-149dac23-fb36-4729-9f07-720d5fad734b.png)

Рис. 2 - архитектура модели VideoMAE 


Видео представляются модели в виде последовательности патчей фиксированного размера (разрешение 16x16, разешение изображения - 224x224), которые встраиваются линейно. Также добавляется токен CLS в начало последовательности, чтобы использовать его для задач классификации. 

С статье [3, с 1] говорится о 3-ёх свойствах данной модели:

1) Чрезвычайно высокое значение коэффициента маскирования (masking ratio) - от 90 % до 95 % - по-прежнему обеспечивает благоприятную производительность для VideoMAE. Это также связано с наличием большого количества фотографий, которые очень похожи особенно если берется сегмент небольшой продолжительности времени (рис.3)
2) VideoMAE достигает впечатляющих результатов на очень небольших наборах данных (например, около 3k-4k видео) без использования каких-либо дополнительных данных. Это частично объясняется сложной задачей восстановления видео для обеспечения изучения структуры высокого уровня. Кроме того, используется маскировка tubing (рис. 3)

![image](https://user-images.githubusercontent.com/58371161/209581100-a421f9c3-f412-4a5c-8583-fc9a4d708dda.png)

Рис. 3 - использование масок и временная избыточность видеоданных

Медлительность является характерна для видеоданных. Это приводит к двум важным характеристикам времени: временной избыточности и временной корреляции. Временная избыточность позволяет восстанавливать пиксели с чрезвычайно высоким коэффициентом маскирования. Временная корреляция позволяет легко реконструировать отсутствующие пиксели, находя соответствующие участки в соседних кадрах при простой (b) маскировке кадров или (c) случайной маскировке. Чтобы избежать этой простой задачи и улучшить репрезентативное представление обучения, в статье[3] используют (d) маскирование tube, где карта маскирования одинакова для всех кадров.

3) VideoMAE показывает, что качество данных (видео) важнее их количества. (рис. 4). Из рис. 4 видно что большее количество итераций обучения может способствовать повышению производительности, если уменьшить размер предтренировочного набора. Даже с предварительно обученными видео размером всего 42 000 изображений все еще возможно получить более высокую точность, чем предварительно обученные модели Kinetics с видео 240 000 (68,7% против 68,5%). Этот результат означает, что разность датасетов является еще одним важным фактором, а качество данных важнее количества данных. 

![image](https://user-images.githubusercontent.com/58371161/209580535-e8fa0c8d-a759-4126-a753-c177663b57c8.png)

Рис. 4 - Эффективность данных представлений VideoMAE, зеленый цвет означает, что модель обучена на 13 000 итерациях,
а синий - на 800.




























Источники:
1) https://towardsdatascience.com/understanding-neural-network-embeddings-851e94bc53d2
2) https://medium.com/mlearning-ai/intuition-to-neural-network-embeddings-986c6bcaa502
3) Zhan Tong, Yibing Song, Jue Wang, Limin Wang, статья "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training", 2022, https://arxiv.org/abs/2203.12602
4) https://huggingface.co/MCG-NJU/videomae-base#:~:text=Hugging%20Face%20team.-,Model%20description,pixel%20values%20for%20masked%20patches.
