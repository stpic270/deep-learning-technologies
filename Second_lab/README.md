В данной работе будет реализована классификация видео на основе эмбедингов от VIDEOMAE Model

1) Теоретическая база 

Эмбединги/векторы признаков — это векторы, представляющие некоторую информацию о некоторой области/задаче, они являются выходными данными из сети или ее промежуточными значениями[2]. Векторизацию данных с помощью ембедингов можно назвать методом уменьшения размерности. Традиционные методы уменьшения размерности — PCA, LDA и т. д. — используют статистические методы для «сжатия» данных, например комбинация линейной алгебры[1]. Современные модели глубокого обучения выполняют уменьшение размерности,  давая представление входных данных, в котором соседние точки соответствуют семантически схожим первоначальных необработанных точкам данных. Например, то, что раньше было одним  вектором, представляющим слово, фразу, изображение и тп., теперь может быть представлено как плотный вектор со значительно меньшей размерностью. Например, если дать модели картинку размерностью 224×224 - получится размерность 50176, что очень много, однако сверточная модель может обработать эти данные и выдать представление изображения как признаки "высокого" уровня с размерностью гораздо меньшей, например 512. Такие данные больше представляют собой структурированный тип и для них уже можно использовать более простые модели.

Чтобы понять что содержат в себе эмбединги рассмотрим пример word-embedding. Их идея состоит в том, чтобы иметь векторы эмбедингов слов, близких по смыслу, чтобы они находились "рядом", но при этом были "далеки" от других слов. "Рядом" и "далеко" означают значение метрики расстояния, например: L2 (евклидово расстояние) или косинусное расстояние. Ключевой момент: каждое число вектора эмбединга соответствует чему-то непосредственно из первичных данных. Данный вектор не обязательно должен быть читаемым для человека, но он должен содержать информацию, с помощью которой более простая модель после эмбедингов может ее понять, например предугадать как близки слова walking-walked и как далеки walking-swam (рис. 1)

![image](https://user-images.githubusercontent.com/58371161/209578441-ca548e9a-6dbd-4d01-a586-7eb9b924322d.png)

Рис. 1 - пример word-embedding

2. Описание разработанной системы

В качестве эмбедингов для видео классификации будет использоваться VideoMAE[3] модель и датасет COIN. 
VideoMAE — это автокодеровщик с маскировкой видео (MAE) для видео[4]. Архитектура (рис. 2) модели очень похожа на архитектуру стандартного Vision Transformer (ViT) с декодером сверху для прогнозирования значений пикселей для замаскированных участков.

![image](https://user-images.githubusercontent.com/58371161/209579604-149dac23-fb36-4729-9f07-720d5fad734b.png)

Рис. 2 - архитектура модели VideoMAE [3]

Видео представляются модели в виде последовательности патчей фиксированного размера (разрешение 16x16, разешение изображения - 224x224), которые встраиваются линейно. Также добавляется токен CLS в начало последовательности, чтобы использовать его для задач классификации. 

С статье [3, с 1] говорится о 3-ёх свойствах данной модели:

1) Чрезвычайно высокое значение коэффициента маскирования (masking ratio) - от 90 % до 95 % - по-прежнему обеспечивает благоприятную производительность для VideoMAE. Это также связано с наличием большого количества фотографий, которые очень похожи особенно если берется сегмент небольшой продолжительности времени (рис.3)
2) VideoMAE достигает впечатляющих результатов на очень небольших наборах данных (например, около 3k-4k видео) без использования каких-либо дополнительных данных. Это частично объясняется сложной задачей восстановления видео для обеспечения изучения структуры высокого уровня. Кроме того, используется маскировка tubing (рис. 3)
Медлительность является характерна для видеоданных. Это приводит к двум важным характеристикам времени: временной избыточности и временной корреляции. Временная избыточность позволяет восстанавливать пиксели с чрезвычайно высоким коэффициентом маскирования. Временная корреляция позволяет легко реконструировать отсутствующие пиксели, находя соответствующие участки в соседних кадрах при простой (b) маскировке кадров или (c) случайной маскировке. Чтобы избежать этой простой задачи и улучшить репрезентативное представление обучения, в статье[3] используют (d) маскирование tube, где карта маскирования одинакова для всех кадров.

![image](https://user-images.githubusercontent.com/58371161/209581100-a421f9c3-f412-4a5c-8583-fc9a4d708dda.png)

Рис. 3 - использование масок и временная избыточность видеоданных [3]

3) VideoMAE показывает, что качество данных (видео) важнее их количества. (рис. 4). Из рис. 4 видно что большее количество итераций обучения может способствовать повышению производительности, если уменьшить размер предтренировочного набора. Даже с предварительно обученными видео размером всего 42 000 изображений все еще возможно получить более высокую точность, чем предварительно обученные модели Kinetics с видео 240 000 (68,7% против 68,5%). Этот результат означает, что разность датасетов является еще одним важным фактором, а качество данных важнее количества данных. 

![image](https://user-images.githubusercontent.com/58371161/209580535-e8fa0c8d-a759-4126-a753-c177663b57c8.png)

Рис. 4 - Эффективность данных представлений VideoMAE, зеленый цвет означает, что модель обучена на 13 000 итерациях,
а синий - на 800 [3].

3. Результаты работы

Как говорилось выше в качестве датасеа был использован COIN. Однако в силу сложности подготовки данных (затраты интернет ресурсов, которые сильно зависят от качества видеозаписи) и изменений состояний некоторых видео, например его удаление с youtube, получилось собрать n из 11827 экземпляров. Разбиение было на k train и o val

Эмбединги были получены согласно архитектуре на рис. 2 и была построена нейронная сеть с 3 слоями, которую можно найти в ноутбуке, находящемся в данной папке. Метрики на основе полученных эмбедингов предаставлены на рис. 5, 6, 7

![image](https://user-images.githubusercontent.com/58371161/209582161-17e749da-b88e-481e-8b7d-a5531fa846e6.png)

Рис. 5 - loss

![image](https://user-images.githubusercontent.com/58371161/209582176-10e40f56-33e5-414b-bfee-ba054eaf2793.png)

Рис. 6 - accuracy

![image](https://user-images.githubusercontent.com/58371161/209582181-9a96442b-bed1-42f7-814a-579b91819adc.png)

Рис. 7 - f1

Кроме того, в качестве сравнения был использован метод опорных векторов, который показал результат accuracy в 26,5 % (код приведен в ноутбуке)

4. Выводы

На основе полученных результатах можно сделать следующие выводы:

1) Эмбединги от модели VideoMAE действительно показывают неплохие результаты даже на небольших датасетах, однако также сказывается зависимость и от качества данных, т.к. большинство видео было загружено в качестве 360p. 
2) Также мы использовали меньшее количество данных для тренировки модели, и вполне очевидно, что их увеличение сказывается прямо пропорционально, поэтому недостаточность качества метрик можно вполне компенсировать простым увеличением данных. 
3) Кроме того, использование нескольких полносвязных слоев для обучения классификации на эмбедингах показывает куда лучший результат по сравнению с методом опорных векторов.

5. Источники:
1) https://towardsdatascience.com/understanding-neural-network-embeddings-851e94bc53d2
2) https://medium.com/mlearning-ai/intuition-to-neural-network-embeddings-986c6bcaa502
3) Zhan Tong, Yibing Song, Jue Wang, Limin Wang, статья "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training", 2022, ссылка - https://arxiv.org/abs/2203.12602
4) https://huggingface.co/MCG-NJU/videomae-base#:~:text=Hugging%20Face%20team.-,Model%20description,pixel%20values%20for%20masked%20patches.
