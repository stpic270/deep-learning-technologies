{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a37f120dc4ce480399c6c04202d6acf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2cd5a223a0b4b0bbbdd3ba76f643ed2",
              "IPY_MODEL_e162120166374d28ab8808ea3a75ad1b",
              "IPY_MODEL_6c7280e027e348ec9727d76d629ef43a"
            ],
            "layout": "IPY_MODEL_fed4afbbffc74bce94e00ddcc483446d"
          }
        },
        "e2cd5a223a0b4b0bbbdd3ba76f643ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90025f5b83b14569a9b1862450350454",
            "placeholder": "​",
            "style": "IPY_MODEL_87373633c413440c90f5e9d004d2abf6",
            "value": "Epoch 41/100:  27%"
          }
        },
        "e162120166374d28ab8808ea3a75ad1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7afa0a112f194cc0a090f8f5199572c4",
            "max": 1100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eebe9db4d4464db2a301d87aab00bed7",
            "value": 300
          }
        },
        "6c7280e027e348ec9727d76d629ef43a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96eddac0af2c4684831008518fe0f0bc",
            "placeholder": "​",
            "style": "IPY_MODEL_e9da5f01e8ff411dbd4a34c718b25003",
            "value": " 300/1100 [1:05:25&lt;1:57:35,  8.82s/it, loss:18.57053]"
          }
        },
        "fed4afbbffc74bce94e00ddcc483446d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "90025f5b83b14569a9b1862450350454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87373633c413440c90f5e9d004d2abf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7afa0a112f194cc0a090f8f5199572c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eebe9db4d4464db2a301d87aab00bed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96eddac0af2c4684831008518fe0f0bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9da5f01e8ff411dbd4a34c718b25003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В данном ноутбуке представлен пример разметки текста для моделей TENER и SBERT. Для начала установим необходимые библиотеки"
      ],
      "metadata": {
        "id": "n2RYX9vRMZHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dialogue-evaluation/RuREBus # Клонирование репозитория RuREBus"
      ],
      "metadata": {
        "id": "3ykuElHFPr95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9af7f7e-27f7-4be8-9d37-e43368e263bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RuREBus'...\n",
            "remote: Enumerating objects: 247, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 247 (delta 16), reused 4 (delta 1), pack-reused 217\u001b[K\n",
            "Receiving objects: 100% (247/247), 14.22 MiB | 20.93 MiB/s, done.\n",
            "Resolving deltas: 100% (118/118), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar # Установка navec - words embedding "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckuyxsSI4Iux",
        "outputId": "f9cb953e-155b-4cc0-80fe-0255e6d68ecc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-26 13:14:13--  https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar\n",
            "Resolving storage.yandexcloud.net (storage.yandexcloud.net)... 213.180.193.243, 2a02:6b8::1d9\n",
            "Connecting to storage.yandexcloud.net (storage.yandexcloud.net)|213.180.193.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53012480 (51M) [application/x-tar]\n",
            "Saving to: ‘navec_hudlit_v1_12B_500K_300d_100q.tar’\n",
            "\n",
            "navec_hudlit_v1_12B 100%[===================>]  50.56M  16.1MB/s    in 4.0s    \n",
            "\n",
            "2023-01-26 13:14:18 (12.5 MB/s) - ‘navec_hudlit_v1_12B_500K_300d_100q.tar’ saved [53012480/53012480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка библиотек для работы с words embedding\n",
        "! pip install Navec \n",
        "! pip install slovnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmPrOFOI4K5M",
        "outputId": "198c8eb8-20e8-454b-9294-23e55a49f4ae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Navec in /usr/local/lib/python3.8/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from Navec) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: slovnet in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: navec in /usr/local/lib/python3.8/dist-packages (from slovnet) (0.10.0)\n",
            "Requirement already satisfied: razdel in /usr/local/lib/python3.8/dist-packages (from slovnet) (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from slovnet) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Распаковка текстов от RuREBus\n",
        "\n",
        "!mkdir /content/train\n",
        "!mkdir /content/test\n",
        "!mkdir /content/test/test_part_1\n",
        "\n",
        "!unzip /content/RuREBus/train_data/train_part_1.zip -d /content/train\n",
        "!unzip /content/RuREBus/train_data/train_part_2.zip -d /content/train\n",
        "!unzip /content/RuREBus/train_data/train_part_3.zip -d /content/train\n",
        "\n",
        "!unzip /content/RuREBus/test_data/test_ner_only.zip -d /content/test/test_part_1\n"
      ],
      "metadata": {
        "id": "5EnEEa914N_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b781d605-1ecb-44ba-f41b-3441e4d9363f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/RuREBus/train_data/train_part_1.zip\n",
            "   creating: /content/train/train_part_1/\n",
            "  inflating: /content/train/train_part_1/.stats_cache  \n",
            "  inflating: /content/train/train_part_1/20336081161101050428001_17_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/20336081161101050428001_17_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/20336081161101050428001_17_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/20336081161101050428001_17_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/20336081161101050428001_17_part_2.ann  \n",
            "  inflating: /content/train/train_part_1/20336081161101050428001_17_part_2.txt  \n",
            "  inflating: /content/train/train_part_1/20336241021100524345002_22_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/20336241021100524345002_22_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/20336241021100524345002_22_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/20336241021100524345002_22_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31228011021100875575102_2_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31228011021100875575102_2_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31228011021100875575102_2_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31228011021100875575102_2_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31228011021100875575102_2_part_2.ann  \n",
            "  inflating: /content/train/train_part_1/31228011021100875575102_2_part_2.txt  \n",
            "  inflating: /content/train/train_part_1/31228011026200599666019_1_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31228011026200599666019_1_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31228011027003553048070_1_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31228011027003553048070_1_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31228011027003553048070_1_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31228011027003553048070_1_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31229011061682000016007_22_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31229011061682000016007_22_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011021101006981005_6_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011021101006981005_6_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011021101017960002_10_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011021101017960002_10_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011021101017960002_10_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011021101017960002_10_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339011021101055205003_5_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011021101055205003_5_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011021101055205003_5_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011021101055205003_5_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339011023301254426027_6_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011023301254426027_6_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011023301254426027_6_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011023301254426027_6_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339011023341069366019_10_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011023341069366019_10_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011023341069366019_10_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011023341069366019_10_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339011023601075299026_18_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011023601075299026_18_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011023601075299026_18_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011023601075299026_18_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024401437224004_0_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024401437224004_0_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024401835996041_11_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024401835996041_11_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024401835996041_11_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024401835996041_11_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024401835996041_11_part_2.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024401835996041_11_part_2.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024401835996041_11_part_3.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024401835996041_11_part_3.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024402435090006_3_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024402435090006_3_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024402435254030_9_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024402435254030_9_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024402635718027_4_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024402635718027_4_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024402635718027_4_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024402635718027_4_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024501597340068_23_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024501597340068_23_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024501597340068_23_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024501597340068_23_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339011024501815733001_11_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011024501815733001_11_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011025602668992001_3_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011025602668992001_3_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011025602668992002_14_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011025602668992002_14_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339011025602985792023_8.ann  \n",
            "  inflating: /content/train/train_part_1/31339011025602985792023_8.txt  \n",
            "  inflating: /content/train/train_part_1/31339011027001622075023_8_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011027001622075023_8_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011027002955451037_10_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011027002955451037_10_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011027002955451037_10_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011027002955451037_10_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339011027002955451038_11_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011027002955451038_11_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011027003753699008_6_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011027003753699008_6_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011027003753699008_6_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011027003753699008_6_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339011035008863746094_64_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011035008863746094_64_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011061674000266022_9_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339011061674000266022_9_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339011061674000266022_9_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339011061674000266022_9_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339031027003353080021_8_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339031027003353080021_8_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339031027003353080021_8_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339031027003353080021_8_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339031061674000266004_14_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339031061674000266004_14_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339031061674000266004_14_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/31339031061674000266004_14_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/31339031061674000266004_14_part_2.ann  \n",
            "  inflating: /content/train/train_part_1/31339031061674000266004_14_part_2.txt  \n",
            "  inflating: /content/train/train_part_1/31339071021100998093010_11_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339071021100998093010_11_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339201027003553048003_18_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339201027003553048003_18_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339201103338000281001_45_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339201103338000281001_45_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339221025603182330021_22_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339221025603182330021_22_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339271061688000208009_18_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339271061688000208009_18_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/31339361025602669828056_25_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/31339361025602669828056_25_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/32339011025002514085015_23_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/32339011025002514085015_23_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/32339011035002600632033_11_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/32339011035002600632033_11_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/32339031035002351603028_18_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/32339031035002351603028_18_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/32339031095658000745005_27_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/32339031095658000745005_27_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/32339061021100935426018_20_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/32339061021100935426018_20_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/32339081026201270260029_37_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/32339081026201270260029_37_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/32339081026201270260029_37_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/32339081026201270260029_37_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/32339091024501206520016_20_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/32339091024501206520016_20_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/32339091024501206520016_20_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/32339091024501206520016_20_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/32339091024501206520016_20_part_2.ann  \n",
            "  inflating: /content/train/train_part_1/32339091024501206520016_20_part_2.txt  \n",
            "  inflating: /content/train/train_part_1/32339091025601812334001_24_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/32339091025601812334001_24_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/32339091035001600171120_7_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/32339091035001600171120_7_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/32339291021100935426126_24_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/32339291021100935426126_24_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/34339011023302353337011_7_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/34339011023302353337011_7_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/34339011023302353337011_7_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/34339011023302353337011_7_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/34339011023302353337015_10_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/34339011023302353337015_10_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/34339011023302353337015_10_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/34339011023302353337015_10_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/34339011054443168933006_6_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/34339011054443168933006_6_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/34339011054443168933006_6_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/34339011054443168933006_6_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/34339011054443168933006_6_part_2.ann  \n",
            "  inflating: /content/train/train_part_1/34339011054443168933006_6_part_2.txt  \n",
            "  inflating: /content/train/train_part_1/34339031023302352303014_14_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/34339031023302352303014_14_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/34339031023302352303014_14_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/34339031023302352303014_14_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/34339031025003215764009_2_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/34339031025003215764009_2_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/34339031025003215764009_2_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/34339031025003215764009_2_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/34339271023600990820002_6_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/34339271023600990820002_6_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/34339271023600990820002_6_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/34339271023600990820002_6_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/34339291023600645023003_2.ann  \n",
            "  inflating: /content/train/train_part_1/34339291023600645023003_2.txt  \n",
            "  inflating: /content/train/train_part_1/35228011034521000381004_2_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/35228011034521000381004_2_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/35228011034521000381004_2_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/35228011034521000381004_2_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/35228011056218015424007_2_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/35228011056218015424007_2_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/35228011056218015424007_2_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/35228011056218015424007_2_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/35339011023600937348001_0_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/35339011023600937348001_0_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/35339011023600937348001_0_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/35339011023600937348001_0_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/35339011023600937348001_0_part_2.ann  \n",
            "  inflating: /content/train/train_part_1/35339011023600937348001_0_part_2.txt  \n",
            "  inflating: /content/train/train_part_1/35339011023601233809004_4_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/35339011023601233809004_4_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/35339011023601233809004_4_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/35339011023601233809004_4_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/35339011023601233809004_4_part_2.ann  \n",
            "  inflating: /content/train/train_part_1/35339011023601233809004_4_part_2.txt  \n",
            "  inflating: /content/train/train_part_1/35339011023601315297001_0_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/35339011023601315297001_0_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/35339011023601315297001_0_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/35339011023601315297001_0_part_1.txt  \n",
            "  inflating: /content/train/train_part_1/35339011054460385484004_3_part_0.ann  \n",
            "  inflating: /content/train/train_part_1/35339011054460385484004_3_part_0.txt  \n",
            "  inflating: /content/train/train_part_1/35339011054460385484004_3_part_1.ann  \n",
            "  inflating: /content/train/train_part_1/35339011054460385484004_3_part_1.txt  \n",
            "Archive:  /content/RuREBus/train_data/train_part_2.zip\n",
            "   creating: /content/train/train_part_2/\n",
            "  inflating: /content/train/train_part_2/20103011022200910379001_0_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/20103011022200910379001_0_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/20103011022200910379001_0_part_1.ann  \n",
            "  inflating: /content/train/train_part_2/20103011022200910379001_0_part_1.txt  \n",
            "  inflating: /content/train/train_part_2/20103011022200910379001_0_part_2.ann  \n",
            "  inflating: /content/train/train_part_2/20103011022200910379001_0_part_2.txt  \n",
            "  inflating: /content/train/train_part_2/20103011022200910379001_0_part_3.ann  \n",
            "  inflating: /content/train/train_part_2/20103011022200910379001_0_part_3.txt  \n",
            "  inflating: /content/train/train_part_2/20103011022200910379001_0_part_4.ann  \n",
            "  inflating: /content/train/train_part_2/20103011022200910379001_0_part_4.txt  \n",
            "  inflating: /content/train/train_part_2/31228011021100998093018_1_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/31228011021100998093018_1_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/31228011021100998093018_1_part_1.ann  \n",
            "  inflating: /content/train/train_part_2/31228011021100998093018_1_part_1.txt  \n",
            "  inflating: /content/train/train_part_2/31228011022200507823012_5_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/31228011022200507823012_5_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/31339011021100971352016_7_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/31339011021100971352016_7_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/31339011021101017960011_17_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/31339011021101017960011_17_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/31339011021101017960011_17_part_1.ann  \n",
            "  inflating: /content/train/train_part_2/31339011021101017960011_17_part_1.txt  \n",
            "  inflating: /content/train/train_part_2/31339011022200507823001_8_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/31339011022200507823001_8_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/31339011022200507823002_9_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/31339011022200507823002_9_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/31339011022200507823003_10_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/31339011022200507823003_10_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/31339011022200507823004_11_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/31339011022200507823004_11_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/31339011022200507823007_12_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/31339011022200507823007_12_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/31339011024401236034002_4_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/31339011024401236034002_4_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/31339011024401236034002_4_part_1.ann  \n",
            "  inflating: /content/train/train_part_2/31339011024401236034002_4_part_1.txt  \n",
            "  inflating: /content/train/train_part_2/31339011024401236034002_4_part_2.ann  \n",
            "  inflating: /content/train/train_part_2/31339011024401236034002_4_part_2.txt  \n",
            "  inflating: /content/train/train_part_2/31339181022200507823011_13.ann  \n",
            "  inflating: /content/train/train_part_2/31339181022200507823011_13.txt  \n",
            "  inflating: /content/train/train_part_2/32339271155074010354061_17_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/32339271155074010354061_17_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/32339271155074010354061_17_part_1.ann  \n",
            "  inflating: /content/train/train_part_2/32339271155074010354061_17_part_1.txt  \n",
            "  inflating: /content/train/train_part_2/34339011023302753869046_28_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/34339011023302753869046_28_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/34339011023302753869046_28_part_1.ann  \n",
            "  inflating: /content/train/train_part_2/34339011023302753869046_28_part_1.txt  \n",
            "  inflating: /content/train/train_part_2/34339011053303011376002_2_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/34339011053303011376002_2_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/34339011053303011376002_2_part_1.ann  \n",
            "  inflating: /content/train/train_part_2/34339011053303011376002_2_part_1.txt  \n",
            "  inflating: /content/train/train_part_2/34339011053303011376002_2_part_2.ann  \n",
            "  inflating: /content/train/train_part_2/34339011053303011376002_2_part_2.txt  \n",
            "  inflating: /content/train/train_part_2/34339011055001023890011_10_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/34339011055001023890011_10_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/35339031024501816602002_6_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/35339031024501816602002_6_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/35339031024501816602002_6_part_1.ann  \n",
            "  inflating: /content/train/train_part_2/35339031024501816602002_6_part_1.txt  \n",
            "  inflating: /content/train/train_part_2/35339051053302139351050_17_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/35339051053302139351050_17_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/35339051053302139351050_17_part_1.ann  \n",
            "  inflating: /content/train/train_part_2/35339051053302139351050_17_part_1.txt  \n",
            "  inflating: /content/train/train_part_2/35339221023600934378001_6_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/35339221023600934378001_6_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/35339221023600934378001_6_part_1.ann  \n",
            "  inflating: /content/train/train_part_2/35339221023600934378001_6_part_1.txt  \n",
            "  inflating: /content/train/train_part_2/35339241024501416565010_4_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/35339241024501416565010_4_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/35339291024501763550002_1_part_0.ann  \n",
            "  inflating: /content/train/train_part_2/35339291024501763550002_1_part_0.txt  \n",
            "  inflating: /content/train/train_part_2/35339291024501763550002_1_part_1.ann  \n",
            "  inflating: /content/train/train_part_2/35339291024501763550002_1_part_1.txt  \n",
            "  inflating: /content/train/train_part_2/35339291024501763550002_1_part_2.ann  \n",
            "  inflating: /content/train/train_part_2/35339291024501763550002_1_part_2.txt  \n",
            "Archive:  /content/RuREBus/train_data/train_part_3.zip\n",
            "   creating: /content/train/train_part_3/\n",
            "  inflating: /content/train/train_part_3/20103011022200910379001_0_part_5.ann  \n",
            "  inflating: /content/train/train_part_3/20103011022200910379001_0_part_5.txt  \n",
            "  inflating: /content/train/train_part_3/20336011022200910379002_6_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/20336011022200910379002_6_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/20336011022200910379002_6_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/20336011022200910379002_6_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/20336011022200910379002_6_part_2.ann  \n",
            "  inflating: /content/train/train_part_3/20336011022200910379002_6_part_2.txt  \n",
            "  inflating: /content/train/train_part_3/20336011022200910379002_6_part_3.ann  \n",
            "  inflating: /content/train/train_part_3/20336011022200910379002_6_part_3.txt  \n",
            "  inflating: /content/train/train_part_3/20336011022200910379005_7_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/20336011022200910379005_7_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/20336271161101050428002_24_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/20336271161101050428002_24_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/20336271161101050428002_24_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/20336271161101050428002_24_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/31228011022200507823012_5.ann  \n",
            "  inflating: /content/train/train_part_3/31228011022200507823012_5.txt  \n",
            "  inflating: /content/train/train_part_3/31228011026200851214048_3_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31228011026200851214048_3_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31228011027002952877017_1_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31228011027002952877017_1_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31229011035617270281001_2_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31229011035617270281001_2_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31229011035617270281001_2_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/31229011035617270281001_2_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/31229011035617270281001_2_part_2.ann  \n",
            "  inflating: /content/train/train_part_3/31229011035617270281001_2_part_2.txt  \n",
            "  inflating: /content/train/train_part_3/31229011061682000016007_22_part_2.ann  \n",
            "  inflating: /content/train/train_part_3/31229011061682000016007_22_part_2.txt  \n",
            "  inflating: /content/train/train_part_3/31339011022200507823001_8.ann  \n",
            "  inflating: /content/train/train_part_3/31339011022200507823001_8.txt  \n",
            "  inflating: /content/train/train_part_3/31339011022200507823002_9.ann  \n",
            "  inflating: /content/train/train_part_3/31339011022200507823002_9.txt  \n",
            "  inflating: /content/train/train_part_3/31339011022200507823003_10.ann  \n",
            "  inflating: /content/train/train_part_3/31339011022200507823003_10.txt  \n",
            "  inflating: /content/train/train_part_3/31339011022200507823004_11.ann  \n",
            "  inflating: /content/train/train_part_3/31339011022200507823004_11.txt  \n",
            "  inflating: /content/train/train_part_3/31339011022200507823007_12.ann  \n",
            "  inflating: /content/train/train_part_3/31339011022200507823007_12.txt  \n",
            "  inflating: /content/train/train_part_3/31339011024401437224004_0.ann  \n",
            "  inflating: /content/train/train_part_3/31339011024401437224004_0.txt  \n",
            "  inflating: /content/train/train_part_3/31339011027003753699005_4_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/31339011027003753699005_4_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/31339011061675000067027_12_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31339011061675000067027_12_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31339011061685002510007_6_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31339011061685002510007_6_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31339031051672045150024_22_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31339031051672045150024_22_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31339031061687000209002_2_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31339031061687000209002_2_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31339061024501948020025_29_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31339061024501948020025_29_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31339061026201402358003_30_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31339061026201402358003_30_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31339061026201402358003_30_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/31339061026201402358003_30_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/31339081025603216638012_17_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31339081025603216638012_17_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31339081027003553048077_13_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31339081027003553048077_13_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31339081027003553048077_13_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/31339081027003553048077_13_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/31339091035619460645006_11_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/31339091035619460645006_11_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/31339121024402635718058_11_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/31339121024402635718058_11_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/31339121024402635718058_11_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/31339121024402635718058_11_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/32339031033302400120031_10_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/32339031033302400120031_10_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/32339031155658034322051_26_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/32339031155658034322051_26_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/32339031155658034322051_26_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/32339031155658034322051_26_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/32339061035001850773033_6_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/32339061035001850773033_6_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/32339061035001850773033_6_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/32339061035001850773033_6_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/32339061035001850773033_6_part_2.ann  \n",
            "  inflating: /content/train/train_part_3/32339061035001850773033_6_part_2.txt  \n",
            "  inflating: /content/train/train_part_3/32339061037000087706001_48_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/32339061037000087706001_48_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/35339011023601511405002_0.ann  \n",
            "  inflating: /content/train/train_part_3/35339011023601511405002_0.txt  \n",
            "  inflating: /content/train/train_part_3/35339011055006363147010_20_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/35339011055006363147010_20_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/35339011055010814033010_10_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/35339011055010814033010_10_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/35339011056210024749011_5_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/35339011056210024749011_5_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/35339011065040000761006_13_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/35339011065040000761006_13_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/35339011065040000761006_13_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/35339011065040000761006_13_part_1.txt  \n",
            "  inflating: /content/train/train_part_3/35339011176234029399002_2_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/35339011176234029399002_2_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/35339031055006363114008_28_part_0.ann  \n",
            "  inflating: /content/train/train_part_3/35339031055006363114008_28_part_0.txt  \n",
            "  inflating: /content/train/train_part_3/35339031055006363114008_28_part_1.ann  \n",
            "  inflating: /content/train/train_part_3/35339031055006363114008_28_part_1.txt  \n",
            "Archive:  /content/RuREBus/test_data/test_ner_only.zip\n",
            "  inflating: /content/test/test_part_1/20226011033302006726002_2_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20226011033302006726002_2_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20226011033302006726002_2_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20226011033302006726002_2_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/20226011033302006726002_2_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/20226011033302006726002_2_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/20226251023601532646004_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20226251023601532646004_9_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20226251023601532646004_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20226251023601532646004_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/20227011054408684110014_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20227011054408684110014_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20227011054408684110014_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20227011054408684110014_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/20227011054408684110014_7_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/20227011054408684110014_7_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/20227011054408684110020_10_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/20227011054408684110020_10_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/20227011054408684110020_10_part_1_.ann  \n",
            "  inflating: /content/test/test_part_1/20227011054408684110020_10_part_1_.txt  \n",
            "  inflating: /content/test/test_part_1/20336011021602838069001_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20336011021602838069001_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20336011021602838069001_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20336011021602838069001_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/20336011024500521517001_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20336011024500521517001_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20336011024500521517001_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20336011024500521517001_4_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/20336011024500528722001_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20336011024500528722001_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20336011024500528722001_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20336011024500528722001_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/20336011031100404488001_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20336011031100404488001_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20336011031100404488001_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20336011031100404488001_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/20336011105658021160002_26_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20336011105658021160002_26_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20336011105658021160002_26_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20336011105658021160002_26_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/20336011161101050428003_13_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/20336011161101050428003_13_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/20336011161101050428003_13_part_1_.ann  \n",
            "  inflating: /content/test/test_part_1/20336011161101050428003_13_part_1_.txt  \n",
            "  inflating: /content/test/test_part_1/20336031021602841215001_37_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20336031021602841215001_37_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20336031021602841215001_37_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20336031021602841215001_37_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/20336031021602841215001_37_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/20336031021602841215001_37_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/20336031021602841215001_42_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20336031021602841215001_42_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20336031021602841215001_42_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20336031021602841215001_42_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/20336061021602833196004_60_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20336061021602833196004_60_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20336061021602833196004_60_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20336061021602833196004_60_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/20336131041100439302002_18_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/20336131041100439302002_18_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/20336131041100439302002_18_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/20336131041100439302002_18_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011021101055205007_1_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31228011021101055205007_1_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31228011021101055205007_1_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31228011021101055205007_1_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011023600510515027_2_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31228011023600510515027_2_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31228011023600510515027_2_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31228011023600510515027_2_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011023600510515027_2_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31228011023600510515027_2_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31228011023600510515027_2_part_3.ann  \n",
            "  inflating: /content/test/test_part_1/31228011023600510515027_2_part_3.txt  \n",
            "  inflating: /content/test/test_part_1/31228011024401437389022_2_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/31228011024401437389022_2_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/31228011024401437389022_2_part_1_.ann  \n",
            "  inflating: /content/test/test_part_1/31228011024401437389022_2_part_1_.txt  \n",
            "  inflating: /content/test/test_part_1/31228011024401437389041_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31228011024401437389041_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31228011024401437389041_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31228011024401437389041_4_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011024401437389041_4_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31228011024401437389041_4_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31228011024402238915001_1_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31228011024402238915001_1_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31228011024402238915001_1_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31228011024402238915001_1_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011024402238915001_1_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31228011024402238915001_1_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31228011026200599666027_2_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31228011026200599666027_2_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31228011026200599666027_2_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31228011026200599666027_2_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011026200662234141_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31228011026200662234141_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31228011026200662234141_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31228011026200662234141_4_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011026200662234141_4_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31228011026200662234141_4_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31228011026200743667014_2_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31228011026200743667014_2_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31228011026200743667014_2_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31228011026200743667014_2_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011027003154485091_1_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31228011027003154485091_1_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31228011027003154485091_1_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31228011027003154485091_1_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011027003154485091_1_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31228011027003154485091_1_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31228011027003353080067_1_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31228011027003353080067_1_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31228011027003353080067_1_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31228011027003353080067_1_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011027003353080067_1_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31228011027003353080067_1_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31228011027003353915017_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31228011027003353915017_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31228011027003353915017_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31228011027003353915017_4_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011067014000019043_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31228011067014000019043_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31228011067014000019043_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31228011067014000019043_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31228011067014000019043_5_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31228011067014000019043_5_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31228011067014000019047_6_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/31228011067014000019047_6_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/31228011067014000019047_6_part_1_.ann  \n",
            "  inflating: /content/test/test_part_1/31228011067014000019047_6_part_1_.txt  \n",
            "  inflating: /content/test/test_part_1/31229011025602369671002_1.ann  \n",
            "  inflating: /content/test/test_part_1/31229011025602369671002_1.txt  \n",
            "  inflating: /content/test/test_part_1/31229011027003753699014_2_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31229011027003753699014_2_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31229011027003753699014_2_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31229011027003753699014_2_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31229011027003753699014_2_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31229011027003753699014_2_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31338011024501598307019_2_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31338011024501598307019_2_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31338011024501598307019_2_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31338011024501598307019_2_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100875575008_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100875575008_9_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100875575008_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100875575008_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100950507031_11.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100950507031_11.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100950507033_12.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100950507033_12.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100950507037_13.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100950507037_13.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100950507039_15.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100950507039_15.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100950507056_19.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100950507056_19.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100971352021_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100971352021_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100971352021_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100971352021_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100987258005_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100987258005_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100987258005_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100987258005_5_part_1.txt  \n",
            " extracting: /content/test/test_part_1/31339011021100987258005_5_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100987258005_5_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100998093008_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100998093008_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100998093008_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100998093008_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100998093015_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100998093015_9_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100998093015_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100998093015_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021100998093015_9_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021100998093015_9_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101006981007_8_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101006981007_8_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101006981035_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101006981035_9_part_0.txt  \n",
            " extracting: /content/test/test_part_1/31339011021101006981035_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101006981035_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101017960001_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101017960001_9_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101017960001_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101017960001_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101017960004_12_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101017960004_12_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101017960004_12_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101017960004_12_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101017960004_12_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101017960004_12_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101043182006_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101043182006_9_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101043182006_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101043182006_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101043182007_10_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101043182007_10_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101043182007_10_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101043182007_10_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101043182007_10_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101043182007_10_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101055205011_9.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101055205011_9.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101055205013_11.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101055205013_11.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101067591005_3_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101067591005_3_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101067591005_3_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101067591005_3_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101087039006_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101087039006_9_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101087039006_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101087039006_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101087039006_9_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101087039006_9_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101096488010_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101096488010_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101096488010_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101096488010_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011021101096488010_5_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011021101096488010_5_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023300714051063_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023300714051063_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023300714051063_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023300714051063_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023300921907180_22_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023300921907180_22_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023300921907180_22_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023300921907180_22_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023600612430007_18_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023600612430007_18_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023600612430007_18_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023600612430007_18_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023601075299005_14_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023601075299005_14_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023601075299005_14_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023601075299005_14_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023601075299005_14_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023601075299005_14_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023601233919005_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023601233919005_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023601233919005_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023601233919005_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011023601313691003_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023601313691003_9_part_0.txt  \n",
            " extracting: /content/test/test_part_1/31339011023601313691003_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011023601313691003_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401236034003_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401236034003_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401236034003_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401236034003_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401236034013_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401236034013_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401236034013_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401236034013_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133020_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133020_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133020_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133020_4_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133023_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133023_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133023_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133023_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133023_5_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133023_5_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133023_5_part_3.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401236133023_5_part_3.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401437224015_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401437224015_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401437224015_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401437224015_4_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401437224023_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401437224023_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401437224023_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401437224023_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401637226006_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401637226006_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024401637226006_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024401637226006_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445011_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445011_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445011_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445011_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445011_5_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445011_5_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445020_10_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445020_10_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445020_10_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445020_10_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445020_10_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402032445020_10_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402435090056_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402435090056_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402435090056_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402435090056_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402632759021_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402632759021_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402632759021_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402632759021_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402635718007_3_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402635718007_3_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402635718007_3_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402635718007_3_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024402635718007_3_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024402635718007_3_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501454581002_6_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501454581002_6_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501454581128_30_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501454581128_30_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501454581128_30_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501454581128_30_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501483874023_16_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501483874023_16_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501483874023_16_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501483874023_16_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501525289014_6_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501525289014_6_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501765991011_10_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501765991011_10_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501765991011_10_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501765991011_10_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501765991011_10_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501765991011_10_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501765991011_10_part_3.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501765991011_10_part_3.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501767861056_26_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501767861056_26_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501767861056_26_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501767861056_26_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501815733056_31_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501815733056_31_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501815733056_31_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501815733056_31_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501815733067_38_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501815733067_38_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024501815733067_38_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024501815733067_38_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024502022423027_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024502022423027_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024502022423027_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024502022423027_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024502051716078_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024502051716078_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011024502051716078_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011024502051716078_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025000845814002_6.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025000845814002_6.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025002864710036_12_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025002864710036_12_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025002864710036_12_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025002864710036_12_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025002864710036_12_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025002864710036_12_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025600545520010_32_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025600545520010_32_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025600545520010_32_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025600545520010_32_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025600579058006_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025600579058006_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025600579058006_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025600579058006_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025600579058014_12_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025600579058014_12_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025600579058014_12_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025600579058014_12_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025600579058014_12_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025600579058014_12_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025602370045003_10_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025602370045003_10_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025602370045003_10_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025602370045003_10_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025602395697047_10_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025602395697047_10_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025602395697047_10_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025602395697047_10_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025602443327007_11.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025602443327007_11.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025602445527012_16_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025602445527012_16_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025602445527014_18_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025602445527014_18_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025602445527014_18_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025602445527014_18_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025602668992072_26.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025602668992072_26.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025602831319017_11.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025602831319017_11.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603181009050_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603181009050_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603181009050_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603181009050_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603182330007_7.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603182330007_7.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603182330050_17.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603182330050_17.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603267689003_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603267689003_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603267689003_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603267689003_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396005_13_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396005_13_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396005_13_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396005_13_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396005_13_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396005_13_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396005_13_part_3.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396005_13_part_3.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396015_58_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396015_58_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396018_72_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396018_72_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396018_72_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011025603271396018_72_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011026200557855009_13.ann  \n",
            "  inflating: /content/test/test_part_1/31339011026200557855009_13.txt  \n",
            "  inflating: /content/test/test_part_1/31339011026200597103018_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011026200597103018_8_part_0.txt  \n",
            " extracting: /content/test/test_part_1/31339011026200597103018_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011026200597103018_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011026200779439016_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011026200779439016_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011026200779439016_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011026200779439016_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011026201400774054_25_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011026201400774054_25_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011026201400774054_25_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011026201400774054_25_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011026201401104001_12_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011026201401104001_12_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011026201401104001_12_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011026201401104001_12_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027000570607010_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027000570607010_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027000570607010_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027000570607010_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027000570607010_6_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027000570607010_6_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027000570838033_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027000570838033_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027000570838033_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027000570838033_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027000570838035_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027000570838035_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027000570838035_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027000570838035_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027000570838036_8.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027000570838036_8.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027001622075018_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027001622075018_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027001622075018_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027001622075018_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027001622075026_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027001622075026_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027001622075026_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027001622075026_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877002_3_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877002_3_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877002_3_part_1_.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877002_3_part_1_.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877002_3_part_2_.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877002_3_part_2_.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877003_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877003_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877003_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877003_4_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877010_7_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877010_7_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877014_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877014_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877014_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027002952877014_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027002955451032_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027002955451032_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027002955451032_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027002955451032_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027002955451033_7.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027002955451033_7.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673014_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673014_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673014_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673014_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673014_6_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673014_6_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673021_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673021_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673021_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673021_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673034_14.ann  \n",
            "  inflating: /content/test/test_part_1/31339011027003155673034_14.txt  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216020_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216020_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216020_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216020_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216020_5_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216020_5_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216023_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216023_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216023_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216023_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216023_6_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216023_6_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216045_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216045_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216045_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011033301001216045_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011034521000733011_14_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011034521000733011_14_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011034521000733011_14_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011034521000733011_14_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011034521000733012_15_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011034521000733012_15_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011034521000733012_15_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011034521000733012_15_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011034545002359018_7.ann  \n",
            "  inflating: /content/test/test_part_1/31339011034545002359018_7.txt  \n",
            "  inflating: /content/test/test_part_1/31339011034587000689004_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011034587000689004_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011034587000689004_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011034587000689004_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746014_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746014_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746014_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746014_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746056_34_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746056_34_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746056_34_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746056_34_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746056_34_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746056_34_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746073_55_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746073_55_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746073_55_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011035008863746073_55_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011035619460645003_3_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011035619460645003_3_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011035619460645003_3_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011035619460645003_3_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011035619460645016_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011035619460645016_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011035619460645016_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011035619460645016_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011051660042378021_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011051660042378021_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011051660042378021_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011051660042378021_4_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011051660042378021_4_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011051660042378021_4_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011053300018738001_13_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011053300018738001_13_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011053300018738001_13_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011053300018738001_13_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011053300018738001_13_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011053300018738001_13_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061105009954011_9.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061105009954011_9.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061665000011010_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061665000011010_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061665000011010_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061665000011010_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061665000022011_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061665000022011_9_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061665000022011_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061665000022011_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061665000022011_9_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061665000022011_9_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061665000650010_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061665000650010_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061665000650010_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061665000650010_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061672000026002_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061672000026002_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061672000026002_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061672000026002_4_part_1.txt  \n",
            " extracting: /content/test/test_part_1/31339011061672000026002_4_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061672000026002_4_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061672000026004_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061672000026004_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061672000026004_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061672000026004_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061672000026004_6_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061672000026004_6_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061672000345003_2_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061672000345003_2_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061672000345003_2_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061672000345003_2_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061672000345008_5.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061672000345008_5.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061672000532010_3_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061672000532010_3_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061672000532010_3_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061672000532010_3_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762002_3.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762002_3.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762007_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762007_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762007_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762007_4_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762007_4_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762007_4_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762017_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762017_9_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762017_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061673000762017_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061674000266019_7.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061674000266019_7.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061674000266021_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061674000266021_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061674000266021_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061674000266021_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061674000266026_12.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061674000266026_12.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061674001290010_6_part_0_.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061674001290010_6_part_0_.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061675000408005_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061675000408005_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061675000408005_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061675000408005_5_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280004_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280004_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280004_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280004_4_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280004_4_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280004_4_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280012_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280012_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280012_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280012_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280012_7_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061683000280012_7_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510004_3_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510004_3_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510004_3_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510004_3_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510004_3_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510004_3_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510012_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510012_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510012_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510012_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510012_11_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061685002510012_11_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061688000296038_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061688000296038_9_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061688000296038_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061688000296038_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061688000296038_9_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061688000296038_9_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061689006422012_14_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061689006422012_14_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339011061689006422012_14_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339011061689006422012_14_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339031023301107631019_13_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339031023301107631019_13_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339031023301107631019_13_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339031023301107631019_13_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339031023301107631019_13_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339031023301107631019_13_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339031024402032600038_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339031024402032600038_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339031024402032600038_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339031024402032600038_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339031024402032600038_11_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339031024402032600038_11_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339031024402238915018_12_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339031024402238915018_12_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339031024402238915018_12_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339031024402238915018_12_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339031024501414959052_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339031024501414959052_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339031024501414959052_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339031024501414959052_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339031024501416224035_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339031024501416224035_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339031024501416224035_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339031024501416224035_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339031025602443327003_20_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339031025602443327003_20_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339031025602443327003_20_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339031025602443327003_20_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339031026201403304020_12_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339031026201403304020_12_part_0.txt  \n",
            " extracting: /content/test/test_part_1/31339031026201403304020_12_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339031026201403304020_12_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339031027000570838050_10_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339031027000570838050_10_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339031027000570838050_10_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339031027000570838050_10_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339031027000570838050_10_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339031027000570838050_10_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339031027003353080018_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339031027003353080018_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339031027003353080018_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339031027003353080018_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339031035616670715014_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339031035616670715014_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339031035616670715014_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339031035616670715014_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339031061677001132009_6.ann  \n",
            "  inflating: /content/test/test_part_1/31339031061677001132009_6.txt  \n",
            "  inflating: /content/test/test_part_1/31339031061677001132018_10_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339031061677001132018_10_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339031061677001132018_10_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339031061677001132018_10_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339041025602669828071_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339041025602669828071_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339041025602669828071_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339041025602669828071_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339051024402435090022_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339051024402435090022_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339051024402435090022_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339051024402435090022_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339051027003155673054_21_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339051027003155673054_21_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339051027003155673054_21_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339051027003155673054_21_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339051027003155673054_21_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339051027003155673054_21_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339061021100875575005_11.ann  \n",
            "  inflating: /content/test/test_part_1/31339061021100875575005_11.txt  \n",
            "  inflating: /content/test/test_part_1/31339061021100971352017_14.ann  \n",
            "  inflating: /content/test/test_part_1/31339061021100971352017_14.txt  \n",
            "  inflating: /content/test/test_part_1/31339061021101055205009_12_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339061021101055205009_12_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339061021101055205009_12_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339061021101055205009_12_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024401836524016_16_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024401836524016_16_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024401836524016_16_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024401836524016_16_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024402038000011_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024402038000011_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024402038000011_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024402038000011_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024402038000011_8_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024402038000011_8_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024402632759024_12_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024402632759024_12_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024402632759024_12_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024402632759024_12_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024402632759024_12_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024402632759024_12_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024402634827001_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024402634827001_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024402634827001_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024402634827001_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024501525289020_19_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024501525289020_19_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024501525289020_19_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024501525289020_19_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339061024501525289020_19_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339061024501525289020_19_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339061061677001132014_17_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339061061677001132014_17_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339061061677001132014_17_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339061061677001132014_17_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339061061688000296022_13_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339061061688000296022_13_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339061061688000296022_13_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339061061688000296022_13_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339061061688000296022_13_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339061061688000296022_13_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339061111105000698009_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339061111105000698009_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339061111105000698009_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339061111105000698009_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339061111105000698009_8_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339061111105000698009_8_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339071035616670715011_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339071035616670715011_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339071035616670715011_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339071035616670715011_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339071037000408048019_10_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339071037000408048019_10_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339071037000408048019_10_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339071037000408048019_10_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339081024401236034004_13_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339081024401236034004_13_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339081024401236034004_13_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339081024401236034004_13_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339081024401236034012_15_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339081024401236034012_15_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339081024401236034012_15_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339081024401236034012_15_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339081024501453130075_38_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339081024501453130075_38_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339081024501453130075_38_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339081024501453130075_38_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339081027001622075032_13.ann  \n",
            "  inflating: /content/test/test_part_1/31339081027001622075032_13.txt  \n",
            "  inflating: /content/test/test_part_1/31339081027003353080041_13_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339081027003353080041_13_part_0.txt  \n",
            " extracting: /content/test/test_part_1/31339081027003353080041_13_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339081027003353080041_13_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339081027003753699016_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339081027003753699016_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339081027003753699016_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339081027003753699016_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339081051653038250006_6_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339081051653038250006_6_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339081051653038250006_6_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339081051653038250006_6_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339081061672000532002_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339081061672000532002_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339081061672000532002_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339081061672000532002_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339081061672000532002_11_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339081061672000532002_11_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339081061672000532003_12.ann  \n",
            "  inflating: /content/test/test_part_1/31339081061672000532003_12.txt  \n",
            "  inflating: /content/test/test_part_1/31339081061688000296004_15_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339081061688000296004_15_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339081061688000296004_15_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339081061688000296004_15_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091021100950507061_20.ann  \n",
            "  inflating: /content/test/test_part_1/31339091021100950507061_20.txt  \n",
            "  inflating: /content/test/test_part_1/31339091021100971352019_15_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091021100971352019_15_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091021100971352019_15_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091021100971352019_15_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091021100971352019_15_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339091021100971352019_15_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339091023341069366005_27_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091023341069366005_27_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091023341069366005_27_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091023341069366005_27_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091023341069366005_27_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339091023341069366005_27_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339091023341069366018_29_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091023341069366018_29_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091023341069366018_29_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091023341069366018_29_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091024400761220022_4_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091024400761220022_4_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091024400761220022_4_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091024400761220022_4_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091024502022423010_14.ann  \n",
            "  inflating: /content/test/test_part_1/31339091024502022423010_14.txt  \n",
            "  inflating: /content/test/test_part_1/31339091027001622075008_14_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091027001622075008_14_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091027001622075008_14_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091027001622075008_14_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091027003154485085_27_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091027003154485085_27_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091027003154485085_27_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091027003154485085_27_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091027003154485085_27_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339091027003154485085_27_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339091027003352837008_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091027003352837008_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091027003352837008_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091027003352837008_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091027003353080003_15_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091027003353080003_15_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091027003353080003_15_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091027003353080003_15_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091027003553048079_15_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091027003553048079_15_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091027003553048079_15_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091027003553048079_15_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091033303207024009_27_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091033303207024009_27_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091033303207024009_27_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091033303207024009_27_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091034587000689007_25_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091034587000689007_25_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091034587000689007_25_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091034587000689007_25_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091034587000689007_25_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339091034587000689007_25_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339091053302138460001_22_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091053302138460001_22_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091053302138460001_22_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091053302138460001_22_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091053302138460001_22_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339091053302138460001_22_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339091067026000414016_33_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091067026000414016_33_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091067026000414016_33_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091067026000414016_33_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339091067026000414016_33_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339091067026000414016_33_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339091067026000414029_34_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339091067026000414029_34_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339091067026000414029_34_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339091067026000414029_34_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339111033300203683001_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339111033300203683001_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339111033300203683001_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339111033300203683001_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339111033300203683001_8_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339111033300203683001_8_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339121023301253337004_13_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339121023301253337004_13_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339121023301253337004_13_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339121023301253337004_13_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339121027003353080012_20_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339121027003353080012_20_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339121027003353080012_20_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339121027003353080012_20_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339131024502051716072_24_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339131024502051716072_24_part_0.txt  \n",
            " extracting: /content/test/test_part_1/31339131024502051716072_24_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339131024502051716072_24_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339131061105009954014_14.ann  \n",
            "  inflating: /content/test/test_part_1/31339131061105009954014_14.txt  \n",
            "  inflating: /content/test/test_part_1/31339141027002952877015_12_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339141027002952877015_12_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339141027002952877015_12_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339141027002952877015_12_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339141027002952877015_12_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339141027002952877015_12_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339161023600612430011_28_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161023600612430011_28_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161023600612430011_28_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161023600612430011_28_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161024401236133063_12_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161024401236133063_12_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161024401236133063_12_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161024401236133063_12_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161024401436311014_9_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161024401436311014_9_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161024401436311014_9_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161024401436311014_9_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161024401436311014_9_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339161024401436311014_9_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339161026200838751052_16_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161026200838751052_16_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161026200838751052_16_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161026200838751052_16_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161027003353080029_22_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161027003353080029_22_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161027003353080029_22_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161027003353080029_22_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161027003353080029_22_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339161027003353080029_22_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339161027003353080029_22_part_3.ann  \n",
            "  inflating: /content/test/test_part_1/31339161027003353080029_22_part_3.txt  \n",
            "  inflating: /content/test/test_part_1/31339161027003753699009_14_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161027003753699009_14_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161027003753699009_14_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161027003753699009_14_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161051660042378001_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161051660042378001_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161051660042378001_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161051660042378001_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161051660042378001_7_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339161051660042378001_7_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339161055010814253062_13_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161055010814253062_13_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161055010814253062_13_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161055010814253062_13_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061665000650014_11_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061665000650014_11_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061665000650014_11_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061665000650014_11_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061674000266018_18_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061674000266018_18_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061674000266018_18_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061674000266018_18_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061675000067019_22_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061675000067019_22_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061675000067019_22_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061675000067019_22_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061675000067019_22_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061675000067019_22_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061686000342002_2_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061686000342002_2_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061686000342002_2_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061686000342002_2_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061686000342002_2_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061686000342002_2_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061688000230002_8_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061688000230002_8_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339161061688000230002_8_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339161061688000230002_8_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339181025602669828061_16.ann  \n",
            "  inflating: /content/test/test_part_1/31339181025602669828061_16.txt  \n",
            "  inflating: /content/test/test_part_1/31339191027003753699011_15_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339191027003753699011_15_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339191027003753699011_15_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339191027003753699011_15_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339191027003753699011_15_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339191027003753699011_15_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339201023600532163003_7_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339201023600532163003_7_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339201023600532163003_7_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339201023600532163003_7_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339201027002952877006_14_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339201027002952877006_14_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339201027002952877006_14_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339201027002952877006_14_part_1.txt  \n",
            " extracting: /content/test/test_part_1/31339201027002952877006_14_part_2.ann  \n",
            " extracting: /content/test/test_part_1/31339201027002952877006_14_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339211027001622075013_20.ann  \n",
            "  inflating: /content/test/test_part_1/31339211027001622075013_20.txt  \n",
            "  inflating: /content/test/test_part_1/31339221023301107631027_19_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339221023301107631027_19_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339221023301107631027_19_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339221023301107631027_19_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339221023601033818162_13_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339221023601033818162_13_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339221023601033818162_13_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339221023601033818162_13_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339221023601072990004_15_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339221023601072990004_15_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339221023601072990004_15_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339221023601072990004_15_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339221025603182330049_24_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339221025603182330049_24_part_0.txt  \n",
            " extracting: /content/test/test_part_1/31339221025603182330049_24_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339221025603182330049_24_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339221026200621809012_29_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339221026200621809012_29_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339221026200621809012_29_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339221026200621809012_29_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339221027003155673053_28.ann  \n",
            "  inflating: /content/test/test_part_1/31339221027003155673053_28.txt  \n",
            "  inflating: /content/test/test_part_1/31339221061674000266007_20_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339221061674000266007_20_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339221061674000266007_20_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339221061674000266007_20_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339221061674000266007_20_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339221061674000266007_20_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339231055010814253009_15_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339231055010814253009_15_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339231055010814253009_15_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339231055010814253009_15_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339231055010814253009_15_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339231055010814253009_15_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339241023600608536002_13_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339241023600608536002_13_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339241023600608536002_13_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339241023600608536002_13_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339241023600935962012_18_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339241023600935962012_18_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339241023600935962012_18_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339241023600935962012_18_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339241023601034698015_20_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339241023601034698015_20_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339241023601034698015_20_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339241023601034698015_20_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339241023601034698015_20_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339241023601034698015_20_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339241027003353915010_17_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339241027003353915010_17_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339241027003353915010_17_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339241027003353915010_17_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339241051660041740038_26.ann  \n",
            "  inflating: /content/test/test_part_1/31339241051660041740038_26.txt  \n",
            "  inflating: /content/test/test_part_1/31339241051660042675006_8.ann  \n",
            "  inflating: /content/test/test_part_1/31339241051660042675006_8.txt  \n",
            "  inflating: /content/test/test_part_1/31339241061675000067037_23_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339241061675000067037_23_part_0.txt  \n",
            " extracting: /content/test/test_part_1/31339241061675000067037_23_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339241061675000067037_23_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339251033301001216016_20_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339251033301001216016_20_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339251033301001216016_20_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339251033301001216016_20_part_1.txt  \n",
            " extracting: /content/test/test_part_1/31339251033301001216016_20_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339251033301001216016_20_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/31339251034545002359057_24_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339251034545002359057_24_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339251034545002359057_24_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339251034545002359057_24_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339251036226000590013_18_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339251036226000590013_18_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339251036226000590013_18_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339251036226000590013_18_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339271024402635718036_15_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339271024402635718036_15_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339271024402635718036_15_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/31339271024402635718036_15_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/31339271024501414959015_27.ann  \n",
            "  inflating: /content/test/test_part_1/31339271024501414959015_27.txt  \n",
            "  inflating: /content/test/test_part_1/31339271025602443327002_35_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/31339271025602443327002_35_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/31339291061674000266032_22_part_2.ann  \n",
            "  inflating: /content/test/test_part_1/31339291061674000266032_22_part_2.txt  \n",
            "  inflating: /content/test/test_part_1/32228011033302400120158_3_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/32228011033302400120158_3_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/32228011033302400120158_3_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/32228011033302400120158_3_part_1.txt  \n",
            "  inflating: /content/test/test_part_1/32339011021100517140001_5_part_0.ann  \n",
            "  inflating: /content/test/test_part_1/32339011021100517140001_5_part_0.txt  \n",
            "  inflating: /content/test/test_part_1/32339011021100517140001_5_part_1.ann  \n",
            "  inflating: /content/test/test_part_1/32339011021100517140001_5_part_1.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка библиотек для преобработки слов\n",
        "! wget https://rusvectores.org/static/models/udpipe_syntagrus.model \n",
        "! pip install ufal.udpipe\n",
        "! pip install 'gensim==3.7.0'"
      ],
      "metadata": {
        "id": "M4SRkCoS8RPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b4ed34-0bd0-4aa1-8485-99e6735e9d3a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-26 13:15:42--  https://rusvectores.org/static/models/udpipe_syntagrus.model\n",
            "Resolving rusvectores.org (rusvectores.org)... 172.104.228.108\n",
            "Connecting to rusvectores.org (rusvectores.org)|172.104.228.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40616122 (39M)\n",
            "Saving to: ‘udpipe_syntagrus.model’\n",
            "\n",
            "udpipe_syntagrus.mo 100%[===================>]  38.73M  11.9MB/s    in 3.3s    \n",
            "\n",
            "2023-01-26 13:15:46 (11.9 MB/s) - ‘udpipe_syntagrus.model’ saved [40616122/40616122]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ufal.udpipe\n",
            "  Downloading ufal.udpipe-1.2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (848 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.5/848.5 KB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ufal.udpipe\n",
            "Successfully installed ufal.udpipe-1.2.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.7.0\n",
            "  Downloading gensim-3.7.0.tar.gz (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim==3.7.0) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.7.0) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim==3.7.0) (1.15.0)\n",
            "Requirement already satisfied: smart_open>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from gensim==3.7.0) (6.3.0)\n",
            "Building wheels for collected packages: gensim\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim: filename=gensim-3.7.0-cp38-cp38-linux_x86_64.whl size=26637957 sha256=6de3e4608ea1cbe77e7c57915a8afc0cc3b1c131c05a4c512c61d82ff34cbcb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/31/83/bc43b55289ab054832a2fde35d7493862be92517be005c0165\n",
            "Successfully built gensim\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install \"fastNLP==0.5\" # Установка fastNLP "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCioAfQ34PrQ",
        "outputId": "26c119b9-89e1-49e9-d08b-a38aaabe44c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastNLP==0.5\n",
            "  Downloading FastNLP-0.5.0-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.2/270.2 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.8/dist-packages (from fastNLP==0.5) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from fastNLP==0.5) (1.13.1+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fastNLP==0.5) (2.25.1)\n",
            "Requirement already satisfied: prettytable>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from fastNLP==0.5) (3.6.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (from fastNLP==0.5) (3.4.4)\n",
            "Requirement already satisfied: nltk>=3.4.1 in /usr/local/lib/python3.8/dist-packages (from fastNLP==0.5) (3.7)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.8/dist-packages (from fastNLP==0.5) (1.21.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.4.1->fastNLP==0.5) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.4.1->fastNLP==0.5) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.4.1->fastNLP==0.5) (2022.6.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prettytable>=0.7.2->fastNLP==0.5) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->fastNLP==0.5) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fastNLP==0.5) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fastNLP==0.5) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fastNLP==0.5) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fastNLP==0.5) (1.24.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (8.1.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (3.0.11)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (1.10.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (1.0.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (6.3.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy->fastNLP==0.5) (2.4.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy->fastNLP==0.5) (3.0.9)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->fastNLP==0.5) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->fastNLP==0.5) (0.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy->fastNLP==0.5) (2.0.1)\n",
            "Installing collected packages: fastNLP\n",
            "Successfully installed fastNLP-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuttqtUb4RvM",
        "outputId": "bfc59dc5-9441-4a79-d824-3e4ba93d3c08"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except:\n",
        "    from fastNLP.core.utils import _pseudo_tqdm as tqdm\n",
        "import warnings\n",
        "\n",
        "from fastNLP.core.batch import DataSetIter, BatchIter\n",
        "from fastNLP.core.callback import CallbackManager, CallbackException, Callback\n",
        "from fastNLP.core.dataset import DataSet\n",
        "from fastNLP.core.losses import _prepare_losser\n",
        "from fastNLP.core.metrics import _prepare_metrics\n",
        "from fastNLP.core.optimizer import Optimizer\n",
        "from fastNLP.core.sampler import Sampler\n",
        "from fastNLP.core.sampler import RandomSampler\n",
        "from fastNLP.core.tester import Tester\n",
        "from fastNLP.core.utils import _CheckError\n",
        "from fastNLP.core.utils import _build_args\n",
        "from fastNLP.core.utils import _check_forward_error\n",
        "from fastNLP.core.utils import _check_loss_evaluate\n",
        "from fastNLP.core.utils import _move_dict_value_to_device\n",
        "from fastNLP.core.utils import _get_func_signature\n",
        "from fastNLP.core.utils import _get_model_device\n",
        "from fastNLP.core.utils import _move_model_to_device\n",
        "from fastNLP.core._parallel_utils import _model_contains_inner_module\n",
        "from fastNLP.core._logger import logger\n",
        "\n",
        "import codecs"
      ],
      "metadata": {
        "id": "z3UmD7jv4TqM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import fastNLP\n",
        "from fastNLP.embeddings import CNNCharEmbedding\n",
        "from fastNLP.embeddings import StaticEmbedding, StackEmbedding\n",
        "\n",
        "from subprocess import getoutput\n",
        "getoutput(\"git clone -l -s https://github.com/fastnlp/TENER\")\n",
        "\n",
        "from fastNLP.embeddings import CNNCharEmbedding\n",
        "from fastNLP import cache_results\n",
        "from fastNLP import Trainer, GradientClipCallback, WarmupCallback\n",
        "from torch import optim\n",
        "from fastNLP import SpanFPreRecMetric, BucketSampler\n",
        "from fastNLP.io.pipe.conll import OntoNotesNERPipe\n",
        "from fastNLP.embeddings import StaticEmbedding, StackEmbedding, LSTMCharEmbedding\n",
        "from TENER.modules.TransformerEmbedding import TransformerCharEmbed\n",
        "\n",
        "from TENER.modules.callbacks import EvaluateCallback\n",
        "\n",
        "from fastNLP.modules import ConditionalRandomField, allowed_transitions\n",
        "from TENER.modules.transformer import TransformerEncoder\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from navec import Navec\n",
        "import argparse\n",
        "\n",
        "from collections import Counter\n",
        "from fastNLP.io.pipe import Conll2003NERPipe\n",
        "\n",
        "import pickle\n",
        "import json"
      ],
      "metadata": {
        "id": "PpxwJPyN4rxj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_token(token, misc):\n",
        "    out_token = token.strip().replace(' ', '')\n",
        "    if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
        "        return None\n",
        "    return out_token"
      ],
      "metadata": {
        "id": "heSTg-jj42Ac"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Слудеющая функция обрабатывает текст, можно выделить часть речи или убрать пунктуацию. На вход подается строка и pipeline"
      ],
      "metadata": {
        "id": "2-cgbnJiQgCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process1(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
        "    entities = {'PROPN'}\n",
        "    named = False\n",
        "    memory = []\n",
        "    mem_case = None\n",
        "    mem_number = None\n",
        "    tagged_propn = []\n",
        "\n",
        "    # обрабатываем текст, получаем результат в формате conllu:\n",
        "    processed = pipeline.process(text)\n",
        "    \n",
        "\n",
        "    # пропускаем строки со служебной информацией:\n",
        "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
        "\n",
        "    # извлекаем из обработанного текста токены\n",
        "    tagged = [w.split('\\t') for w in content if w]\n",
        "\n",
        "    for t in tagged:\n",
        "        if len(t) != 10:\n",
        "            continue\n",
        "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
        "        \n",
        "        token = clean_token(token, misc)\n",
        "        tagged_propn.append(token.lower()+'_'+pos)\n",
        "        \n",
        "    if not keep_punct: \n",
        "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
        "\n",
        "    if not keep_pos:\n",
        "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
        "        \n",
        "    return tagged_propn"
      ],
      "metadata": {
        "id": "OnabFHDH8VH7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующая функция также принимает строку и  параметры, которые затем обновляются и будут использоваться для создания датасета."
      ],
      "metadata": {
        "id": "nlaf85ReRB70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ufal.udpipe import Model, Pipeline\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import gensim\n",
        "\n",
        "def tag_ud(dictionary, all_words, instances, char_id, text='Текст нужно передать функции в виде строки!', \n",
        "           model='model'):\n",
        "  \n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    dictionary - список tuple от уникальных слов всех текстов и индекса navec\n",
        "    all_words - список слов из всех текстов\n",
        "    instances - словарь, который будет использоваться в дальнейшем для создания датасета\n",
        "    char_id - словарь с диапазоном символов, который занимает строка\n",
        "    text - строка\n",
        "    model - модель для pipeline\n",
        "\n",
        "    Return:\n",
        "    возвращаются расширенные dictionary, all_words, instances, char_id\n",
        "    \"\"\"\n",
        "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
        "    ind, ind2 = 0, 0\n",
        "    \n",
        "    for line in text:\n",
        "        \n",
        "        output = process1(process_pipeline, text=line, keep_pos=False, keep_punct=False) # Обработка строки\n",
        "        \n",
        "        if len(output) == 0:\n",
        "          continue\n",
        "\n",
        "        ords = [] # список содержащий индексы, которые будут подаваться в embedding\n",
        "        \n",
        "        ind2 += len([*line])\n",
        "        char_id.append((ind, ind2)) # добавление диапазона\n",
        "        ind = ind2\n",
        "\n",
        "        for word in output:\n",
        "          \n",
        "          # Проверка наличия слова в navec и извлечение индекса\n",
        "          no_unk_flag = False \n",
        "          \n",
        "          if word in navec:\n",
        "            id = navec.vocab[word]\n",
        "            no_unk_flag = True\n",
        "          else: id = navec.vocab['<unk>']\n",
        "\n",
        "          ords.append(id)\n",
        "\n",
        "          all_words.append(word) # добавление слова\n",
        "          if word in dictionary or no_unk_flag == False: continue\n",
        "          dictionary.append((word, id)) # добавление tuple (слово, индекс)\n",
        "\n",
        "        # Добавление данных\n",
        "        instances['sentence'].append(output)\n",
        "        instances['ords'].append(ords)\n",
        "        instances['seq_len'].append(len(ords))\n",
        "        instances['label'].append([0] * len(ords))\n",
        "        instances['flag'].append([True] *len(ords))\n",
        "        \n",
        "       \n",
        "        \n",
        "    return dictionary, all_words, instances, char_id"
      ],
      "metadata": {
        "id": "8ts835ia6OHl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastNLP import Padder, EngChar2DPadder, SequentialSampler, AutoPadder\n",
        "from fastNLP import Instance, DataSet\n",
        "from navec import Navec\n",
        "# загрузка Navec\n",
        "path = '/content/navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
        "navec = Navec.load(path)"
      ],
      "metadata": {
        "id": "QJvTeSRS6CLk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следубщая функция возвращает списки путей к файлам text и ann, принимая на вход ссылку на исходную папку. Исходная папка должна содежать еще одни папки с нужнымы файлами "
      ],
      "metadata": {
        "id": "ymct5dVrOggC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def txt_ann_file_paths(path):\n",
        "\n",
        "  txt_path_spisok = []\n",
        "\n",
        "  for folder in os.listdir(path):\n",
        "\n",
        "    folder_path = os.path.join(path, folder)\n",
        "    for f in os.listdir(folder_path):\n",
        "      if 'txt' in f:\n",
        "        file_txt_path = os.path.join(folder_path, f)\n",
        "        txt_path_spisok.append(file_txt_path)\n",
        "\n",
        "  ann_path_spisok = []\n",
        "\n",
        "  for path in txt_path_spisok:\n",
        "    ann_path_spisok.append(path.replace('txt', 'ann'))\n",
        "\n",
        "  return txt_path_spisok, ann_path_spisok\n"
      ],
      "metadata": {
        "id": "LCmgDlXPG7nP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_instances_for_dataset(txt_path_spisok, dictionary=None, all_words=None, char_id=None, instances=None):\n",
        "  \n",
        "  #Функция, которая работает с tag_ud - инициализирует переменные и модель для pipline, делит текст на строки\n",
        "\n",
        "  count = 0 # Количество текстов\n",
        "  model = Model.load('/content/udpipe_syntagrus.model')\n",
        "  if dictionary==None or all_words==None or char_id==None or instances==None:\n",
        "    dictionary = []\n",
        "    all_words = []\n",
        "    char_id = dict()\n",
        "    instances = dict(sentence=[], ords=[], label=[], seq_len=[], flag= [])\n",
        "\n",
        "  for path in txt_path_spisok:\n",
        "\n",
        "    print(f'Documents are downloaded {count} of {len(txt_path_spisok)}')\n",
        "    char_id[str(count)] = []\n",
        "    \n",
        "\n",
        "    with open(path, 'r') as text:\n",
        "      text = text.readlines()\n",
        "      \n",
        "      dictionary, all_words, instances, char_id[str(count)] = tag_ud(dictionary=dictionary, all_words=all_words, \n",
        "                                                        instances=instances, char_id=char_id[str(count)],\n",
        "                                                        text=text, model=model)\n",
        "    count += 1\n",
        "\n",
        "  return dictionary, all_words, instances, char_id"
      ],
      "metadata": {
        "id": "nUbRN8xJ6bub"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующая функция извлекает данные из аннтоаций, которые будут использоваться при дальнейшей разметке"
      ],
      "metadata": {
        "id": "dQKBmV3MYvcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_spans(ann_path_spisok):\n",
        "\n",
        "  class_spisok = [] # Список всех классов во всех ann\n",
        "  spans = dict() # словарь который будет содержать разметку для каждого txt файла\n",
        "  md = Model.load('/content/udpipe_syntagrus.model')\n",
        "\n",
        "  for i, path in enumerate(ann_path_spisok):\n",
        "    print(i)\n",
        "    with open(path, 'r') as f:\n",
        "\n",
        "      lines = f.readlines()\n",
        "      span = []\n",
        "      for line in lines:\n",
        "     \n",
        "        process_pipeline = Pipeline(md, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu') \n",
        "        l = process1(process_pipeline, text=line, keep_pos=False, keep_punct=False) # такая же предобработка строки ann, как и txt\n",
        "\n",
        "        # Добавление только NER классов, слов и их диапазона\n",
        "        if 'r' in l[0]:\n",
        "          continue\n",
        "\n",
        "        span_start =int(l[2])\n",
        "        span_end =int(l[3])\n",
        "        \n",
        "        cl = l[1].upper()\n",
        "\n",
        "        span.append([span_start, span_end, cl] + l[4:])\n",
        "        class_spisok.append(cl)\n",
        "\n",
        "    span = sorted(span)\n",
        "    spans[str(i)] = span\n",
        "\n",
        "  return spans, class_spisok"
      ],
      "metadata": {
        "id": "snMO8biav_UN"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/train'\n",
        "test_path = '/content/test'\n",
        "\n",
        "# Создание списков train и test списков с ann и txt файлами\n",
        "train_txt_path_spisok, train_ann_path_spisok = txt_ann_file_paths(train_path)\n",
        "test_txt_path_spisok, test_ann_path_spisok = txt_ann_file_paths(test_path)\n",
        "\n",
        "# Создание необходимых переменных, описанных в tag_ud\n",
        "train_dictionary, train_all_words, train_instances, train_char_id = create_instances_for_dataset(sorted(train_txt_path_spisok))\n",
        "test_dictionary, test_all_words, test_instances, test_char_id = create_instances_for_dataset(sorted(test_txt_path_spisok))\n",
        "\n",
        "# Создание диапазонов из ann файлов\n",
        "train_spans, train_class_spisok_number = create_spans(sorted(train_ann_path_spisok))\n",
        "test_spans, test_class_spisok_number = create_spans(sorted(test_ann_path_spisok))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgBONx5LYvL2",
        "outputId": "0574452a-9c0e-420c-fcc2-f70f607628a9"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents are downloaded 0 of 188\n",
            "Documents are downloaded 1 of 188\n",
            "Documents are downloaded 2 of 188\n",
            "Documents are downloaded 3 of 188\n",
            "Documents are downloaded 4 of 188\n",
            "Documents are downloaded 5 of 188\n",
            "Documents are downloaded 6 of 188\n",
            "Documents are downloaded 7 of 188\n",
            "Documents are downloaded 8 of 188\n",
            "Documents are downloaded 9 of 188\n",
            "Documents are downloaded 10 of 188\n",
            "Documents are downloaded 11 of 188\n",
            "Documents are downloaded 12 of 188\n",
            "Documents are downloaded 13 of 188\n",
            "Documents are downloaded 14 of 188\n",
            "Documents are downloaded 15 of 188\n",
            "Documents are downloaded 16 of 188\n",
            "Documents are downloaded 17 of 188\n",
            "Documents are downloaded 18 of 188\n",
            "Documents are downloaded 19 of 188\n",
            "Documents are downloaded 20 of 188\n",
            "Documents are downloaded 21 of 188\n",
            "Documents are downloaded 22 of 188\n",
            "Documents are downloaded 23 of 188\n",
            "Documents are downloaded 24 of 188\n",
            "Documents are downloaded 25 of 188\n",
            "Documents are downloaded 26 of 188\n",
            "Documents are downloaded 27 of 188\n",
            "Documents are downloaded 28 of 188\n",
            "Documents are downloaded 29 of 188\n",
            "Documents are downloaded 30 of 188\n",
            "Documents are downloaded 31 of 188\n",
            "Documents are downloaded 32 of 188\n",
            "Documents are downloaded 33 of 188\n",
            "Documents are downloaded 34 of 188\n",
            "Documents are downloaded 35 of 188\n",
            "Documents are downloaded 36 of 188\n",
            "Documents are downloaded 37 of 188\n",
            "Documents are downloaded 38 of 188\n",
            "Documents are downloaded 39 of 188\n",
            "Documents are downloaded 40 of 188\n",
            "Documents are downloaded 41 of 188\n",
            "Documents are downloaded 42 of 188\n",
            "Documents are downloaded 43 of 188\n",
            "Documents are downloaded 44 of 188\n",
            "Documents are downloaded 45 of 188\n",
            "Documents are downloaded 46 of 188\n",
            "Documents are downloaded 47 of 188\n",
            "Documents are downloaded 48 of 188\n",
            "Documents are downloaded 49 of 188\n",
            "Documents are downloaded 50 of 188\n",
            "Documents are downloaded 51 of 188\n",
            "Documents are downloaded 52 of 188\n",
            "Documents are downloaded 53 of 188\n",
            "Documents are downloaded 54 of 188\n",
            "Documents are downloaded 55 of 188\n",
            "Documents are downloaded 56 of 188\n",
            "Documents are downloaded 57 of 188\n",
            "Documents are downloaded 58 of 188\n",
            "Documents are downloaded 59 of 188\n",
            "Documents are downloaded 60 of 188\n",
            "Documents are downloaded 61 of 188\n",
            "Documents are downloaded 62 of 188\n",
            "Documents are downloaded 63 of 188\n",
            "Documents are downloaded 64 of 188\n",
            "Documents are downloaded 65 of 188\n",
            "Documents are downloaded 66 of 188\n",
            "Documents are downloaded 67 of 188\n",
            "Documents are downloaded 68 of 188\n",
            "Documents are downloaded 69 of 188\n",
            "Documents are downloaded 70 of 188\n",
            "Documents are downloaded 71 of 188\n",
            "Documents are downloaded 72 of 188\n",
            "Documents are downloaded 73 of 188\n",
            "Documents are downloaded 74 of 188\n",
            "Documents are downloaded 75 of 188\n",
            "Documents are downloaded 76 of 188\n",
            "Documents are downloaded 77 of 188\n",
            "Documents are downloaded 78 of 188\n",
            "Documents are downloaded 79 of 188\n",
            "Documents are downloaded 80 of 188\n",
            "Documents are downloaded 81 of 188\n",
            "Documents are downloaded 82 of 188\n",
            "Documents are downloaded 83 of 188\n",
            "Documents are downloaded 84 of 188\n",
            "Documents are downloaded 85 of 188\n",
            "Documents are downloaded 86 of 188\n",
            "Documents are downloaded 87 of 188\n",
            "Documents are downloaded 88 of 188\n",
            "Documents are downloaded 89 of 188\n",
            "Documents are downloaded 90 of 188\n",
            "Documents are downloaded 91 of 188\n",
            "Documents are downloaded 92 of 188\n",
            "Documents are downloaded 93 of 188\n",
            "Documents are downloaded 94 of 188\n",
            "Documents are downloaded 95 of 188\n",
            "Documents are downloaded 96 of 188\n",
            "Documents are downloaded 97 of 188\n",
            "Documents are downloaded 98 of 188\n",
            "Documents are downloaded 99 of 188\n",
            "Documents are downloaded 100 of 188\n",
            "Documents are downloaded 101 of 188\n",
            "Documents are downloaded 102 of 188\n",
            "Documents are downloaded 103 of 188\n",
            "Documents are downloaded 104 of 188\n",
            "Documents are downloaded 105 of 188\n",
            "Documents are downloaded 106 of 188\n",
            "Documents are downloaded 107 of 188\n",
            "Documents are downloaded 108 of 188\n",
            "Documents are downloaded 109 of 188\n",
            "Documents are downloaded 110 of 188\n",
            "Documents are downloaded 111 of 188\n",
            "Documents are downloaded 112 of 188\n",
            "Documents are downloaded 113 of 188\n",
            "Documents are downloaded 114 of 188\n",
            "Documents are downloaded 115 of 188\n",
            "Documents are downloaded 116 of 188\n",
            "Documents are downloaded 117 of 188\n",
            "Documents are downloaded 118 of 188\n",
            "Documents are downloaded 119 of 188\n",
            "Documents are downloaded 120 of 188\n",
            "Documents are downloaded 121 of 188\n",
            "Documents are downloaded 122 of 188\n",
            "Documents are downloaded 123 of 188\n",
            "Documents are downloaded 124 of 188\n",
            "Documents are downloaded 125 of 188\n",
            "Documents are downloaded 126 of 188\n",
            "Documents are downloaded 127 of 188\n",
            "Documents are downloaded 128 of 188\n",
            "Documents are downloaded 129 of 188\n",
            "Documents are downloaded 130 of 188\n",
            "Documents are downloaded 131 of 188\n",
            "Documents are downloaded 132 of 188\n",
            "Documents are downloaded 133 of 188\n",
            "Documents are downloaded 134 of 188\n",
            "Documents are downloaded 135 of 188\n",
            "Documents are downloaded 136 of 188\n",
            "Documents are downloaded 137 of 188\n",
            "Documents are downloaded 138 of 188\n",
            "Documents are downloaded 139 of 188\n",
            "Documents are downloaded 140 of 188\n",
            "Documents are downloaded 141 of 188\n",
            "Documents are downloaded 142 of 188\n",
            "Documents are downloaded 143 of 188\n",
            "Documents are downloaded 144 of 188\n",
            "Documents are downloaded 145 of 188\n",
            "Documents are downloaded 146 of 188\n",
            "Documents are downloaded 147 of 188\n",
            "Documents are downloaded 148 of 188\n",
            "Documents are downloaded 149 of 188\n",
            "Documents are downloaded 150 of 188\n",
            "Documents are downloaded 151 of 188\n",
            "Documents are downloaded 152 of 188\n",
            "Documents are downloaded 153 of 188\n",
            "Documents are downloaded 154 of 188\n",
            "Documents are downloaded 155 of 188\n",
            "Documents are downloaded 156 of 188\n",
            "Documents are downloaded 157 of 188\n",
            "Documents are downloaded 158 of 188\n",
            "Documents are downloaded 159 of 188\n",
            "Documents are downloaded 160 of 188\n",
            "Documents are downloaded 161 of 188\n",
            "Documents are downloaded 162 of 188\n",
            "Documents are downloaded 163 of 188\n",
            "Documents are downloaded 164 of 188\n",
            "Documents are downloaded 165 of 188\n",
            "Documents are downloaded 166 of 188\n",
            "Documents are downloaded 167 of 188\n",
            "Documents are downloaded 168 of 188\n",
            "Documents are downloaded 169 of 188\n",
            "Documents are downloaded 170 of 188\n",
            "Documents are downloaded 171 of 188\n",
            "Documents are downloaded 172 of 188\n",
            "Documents are downloaded 173 of 188\n",
            "Documents are downloaded 174 of 188\n",
            "Documents are downloaded 175 of 188\n",
            "Documents are downloaded 176 of 188\n",
            "Documents are downloaded 177 of 188\n",
            "Documents are downloaded 178 of 188\n",
            "Documents are downloaded 179 of 188\n",
            "Documents are downloaded 180 of 188\n",
            "Documents are downloaded 181 of 188\n",
            "Documents are downloaded 182 of 188\n",
            "Documents are downloaded 183 of 188\n",
            "Documents are downloaded 184 of 188\n",
            "Documents are downloaded 185 of 188\n",
            "Documents are downloaded 186 of 188\n",
            "Documents are downloaded 187 of 188\n",
            "Documents are downloaded 0 of 544\n",
            "Documents are downloaded 1 of 544\n",
            "Documents are downloaded 2 of 544\n",
            "Documents are downloaded 3 of 544\n",
            "Documents are downloaded 4 of 544\n",
            "Documents are downloaded 5 of 544\n",
            "Documents are downloaded 6 of 544\n",
            "Documents are downloaded 7 of 544\n",
            "Documents are downloaded 8 of 544\n",
            "Documents are downloaded 9 of 544\n",
            "Documents are downloaded 10 of 544\n",
            "Documents are downloaded 11 of 544\n",
            "Documents are downloaded 12 of 544\n",
            "Documents are downloaded 13 of 544\n",
            "Documents are downloaded 14 of 544\n",
            "Documents are downloaded 15 of 544\n",
            "Documents are downloaded 16 of 544\n",
            "Documents are downloaded 17 of 544\n",
            "Documents are downloaded 18 of 544\n",
            "Documents are downloaded 19 of 544\n",
            "Documents are downloaded 20 of 544\n",
            "Documents are downloaded 21 of 544\n",
            "Documents are downloaded 22 of 544\n",
            "Documents are downloaded 23 of 544\n",
            "Documents are downloaded 24 of 544\n",
            "Documents are downloaded 25 of 544\n",
            "Documents are downloaded 26 of 544\n",
            "Documents are downloaded 27 of 544\n",
            "Documents are downloaded 28 of 544\n",
            "Documents are downloaded 29 of 544\n",
            "Documents are downloaded 30 of 544\n",
            "Documents are downloaded 31 of 544\n",
            "Documents are downloaded 32 of 544\n",
            "Documents are downloaded 33 of 544\n",
            "Documents are downloaded 34 of 544\n",
            "Documents are downloaded 35 of 544\n",
            "Documents are downloaded 36 of 544\n",
            "Documents are downloaded 37 of 544\n",
            "Documents are downloaded 38 of 544\n",
            "Documents are downloaded 39 of 544\n",
            "Documents are downloaded 40 of 544\n",
            "Documents are downloaded 41 of 544\n",
            "Documents are downloaded 42 of 544\n",
            "Documents are downloaded 43 of 544\n",
            "Documents are downloaded 44 of 544\n",
            "Documents are downloaded 45 of 544\n",
            "Documents are downloaded 46 of 544\n",
            "Documents are downloaded 47 of 544\n",
            "Documents are downloaded 48 of 544\n",
            "Documents are downloaded 49 of 544\n",
            "Documents are downloaded 50 of 544\n",
            "Documents are downloaded 51 of 544\n",
            "Documents are downloaded 52 of 544\n",
            "Documents are downloaded 53 of 544\n",
            "Documents are downloaded 54 of 544\n",
            "Documents are downloaded 55 of 544\n",
            "Documents are downloaded 56 of 544\n",
            "Documents are downloaded 57 of 544\n",
            "Documents are downloaded 58 of 544\n",
            "Documents are downloaded 59 of 544\n",
            "Documents are downloaded 60 of 544\n",
            "Documents are downloaded 61 of 544\n",
            "Documents are downloaded 62 of 544\n",
            "Documents are downloaded 63 of 544\n",
            "Documents are downloaded 64 of 544\n",
            "Documents are downloaded 65 of 544\n",
            "Documents are downloaded 66 of 544\n",
            "Documents are downloaded 67 of 544\n",
            "Documents are downloaded 68 of 544\n",
            "Documents are downloaded 69 of 544\n",
            "Documents are downloaded 70 of 544\n",
            "Documents are downloaded 71 of 544\n",
            "Documents are downloaded 72 of 544\n",
            "Documents are downloaded 73 of 544\n",
            "Documents are downloaded 74 of 544\n",
            "Documents are downloaded 75 of 544\n",
            "Documents are downloaded 76 of 544\n",
            "Documents are downloaded 77 of 544\n",
            "Documents are downloaded 78 of 544\n",
            "Documents are downloaded 79 of 544\n",
            "Documents are downloaded 80 of 544\n",
            "Documents are downloaded 81 of 544\n",
            "Documents are downloaded 82 of 544\n",
            "Documents are downloaded 83 of 544\n",
            "Documents are downloaded 84 of 544\n",
            "Documents are downloaded 85 of 544\n",
            "Documents are downloaded 86 of 544\n",
            "Documents are downloaded 87 of 544\n",
            "Documents are downloaded 88 of 544\n",
            "Documents are downloaded 89 of 544\n",
            "Documents are downloaded 90 of 544\n",
            "Documents are downloaded 91 of 544\n",
            "Documents are downloaded 92 of 544\n",
            "Documents are downloaded 93 of 544\n",
            "Documents are downloaded 94 of 544\n",
            "Documents are downloaded 95 of 544\n",
            "Documents are downloaded 96 of 544\n",
            "Documents are downloaded 97 of 544\n",
            "Documents are downloaded 98 of 544\n",
            "Documents are downloaded 99 of 544\n",
            "Documents are downloaded 100 of 544\n",
            "Documents are downloaded 101 of 544\n",
            "Documents are downloaded 102 of 544\n",
            "Documents are downloaded 103 of 544\n",
            "Documents are downloaded 104 of 544\n",
            "Documents are downloaded 105 of 544\n",
            "Documents are downloaded 106 of 544\n",
            "Documents are downloaded 107 of 544\n",
            "Documents are downloaded 108 of 544\n",
            "Documents are downloaded 109 of 544\n",
            "Documents are downloaded 110 of 544\n",
            "Documents are downloaded 111 of 544\n",
            "Documents are downloaded 112 of 544\n",
            "Documents are downloaded 113 of 544\n",
            "Documents are downloaded 114 of 544\n",
            "Documents are downloaded 115 of 544\n",
            "Documents are downloaded 116 of 544\n",
            "Documents are downloaded 117 of 544\n",
            "Documents are downloaded 118 of 544\n",
            "Documents are downloaded 119 of 544\n",
            "Documents are downloaded 120 of 544\n",
            "Documents are downloaded 121 of 544\n",
            "Documents are downloaded 122 of 544\n",
            "Documents are downloaded 123 of 544\n",
            "Documents are downloaded 124 of 544\n",
            "Documents are downloaded 125 of 544\n",
            "Documents are downloaded 126 of 544\n",
            "Documents are downloaded 127 of 544\n",
            "Documents are downloaded 128 of 544\n",
            "Documents are downloaded 129 of 544\n",
            "Documents are downloaded 130 of 544\n",
            "Documents are downloaded 131 of 544\n",
            "Documents are downloaded 132 of 544\n",
            "Documents are downloaded 133 of 544\n",
            "Documents are downloaded 134 of 544\n",
            "Documents are downloaded 135 of 544\n",
            "Documents are downloaded 136 of 544\n",
            "Documents are downloaded 137 of 544\n",
            "Documents are downloaded 138 of 544\n",
            "Documents are downloaded 139 of 544\n",
            "Documents are downloaded 140 of 544\n",
            "Documents are downloaded 141 of 544\n",
            "Documents are downloaded 142 of 544\n",
            "Documents are downloaded 143 of 544\n",
            "Documents are downloaded 144 of 544\n",
            "Documents are downloaded 145 of 544\n",
            "Documents are downloaded 146 of 544\n",
            "Documents are downloaded 147 of 544\n",
            "Documents are downloaded 148 of 544\n",
            "Documents are downloaded 149 of 544\n",
            "Documents are downloaded 150 of 544\n",
            "Documents are downloaded 151 of 544\n",
            "Documents are downloaded 152 of 544\n",
            "Documents are downloaded 153 of 544\n",
            "Documents are downloaded 154 of 544\n",
            "Documents are downloaded 155 of 544\n",
            "Documents are downloaded 156 of 544\n",
            "Documents are downloaded 157 of 544\n",
            "Documents are downloaded 158 of 544\n",
            "Documents are downloaded 159 of 544\n",
            "Documents are downloaded 160 of 544\n",
            "Documents are downloaded 161 of 544\n",
            "Documents are downloaded 162 of 544\n",
            "Documents are downloaded 163 of 544\n",
            "Documents are downloaded 164 of 544\n",
            "Documents are downloaded 165 of 544\n",
            "Documents are downloaded 166 of 544\n",
            "Documents are downloaded 167 of 544\n",
            "Documents are downloaded 168 of 544\n",
            "Documents are downloaded 169 of 544\n",
            "Documents are downloaded 170 of 544\n",
            "Documents are downloaded 171 of 544\n",
            "Documents are downloaded 172 of 544\n",
            "Documents are downloaded 173 of 544\n",
            "Documents are downloaded 174 of 544\n",
            "Documents are downloaded 175 of 544\n",
            "Documents are downloaded 176 of 544\n",
            "Documents are downloaded 177 of 544\n",
            "Documents are downloaded 178 of 544\n",
            "Documents are downloaded 179 of 544\n",
            "Documents are downloaded 180 of 544\n",
            "Documents are downloaded 181 of 544\n",
            "Documents are downloaded 182 of 544\n",
            "Documents are downloaded 183 of 544\n",
            "Documents are downloaded 184 of 544\n",
            "Documents are downloaded 185 of 544\n",
            "Documents are downloaded 186 of 544\n",
            "Documents are downloaded 187 of 544\n",
            "Documents are downloaded 188 of 544\n",
            "Documents are downloaded 189 of 544\n",
            "Documents are downloaded 190 of 544\n",
            "Documents are downloaded 191 of 544\n",
            "Documents are downloaded 192 of 544\n",
            "Documents are downloaded 193 of 544\n",
            "Documents are downloaded 194 of 544\n",
            "Documents are downloaded 195 of 544\n",
            "Documents are downloaded 196 of 544\n",
            "Documents are downloaded 197 of 544\n",
            "Documents are downloaded 198 of 544\n",
            "Documents are downloaded 199 of 544\n",
            "Documents are downloaded 200 of 544\n",
            "Documents are downloaded 201 of 544\n",
            "Documents are downloaded 202 of 544\n",
            "Documents are downloaded 203 of 544\n",
            "Documents are downloaded 204 of 544\n",
            "Documents are downloaded 205 of 544\n",
            "Documents are downloaded 206 of 544\n",
            "Documents are downloaded 207 of 544\n",
            "Documents are downloaded 208 of 544\n",
            "Documents are downloaded 209 of 544\n",
            "Documents are downloaded 210 of 544\n",
            "Documents are downloaded 211 of 544\n",
            "Documents are downloaded 212 of 544\n",
            "Documents are downloaded 213 of 544\n",
            "Documents are downloaded 214 of 544\n",
            "Documents are downloaded 215 of 544\n",
            "Documents are downloaded 216 of 544\n",
            "Documents are downloaded 217 of 544\n",
            "Documents are downloaded 218 of 544\n",
            "Documents are downloaded 219 of 544\n",
            "Documents are downloaded 220 of 544\n",
            "Documents are downloaded 221 of 544\n",
            "Documents are downloaded 222 of 544\n",
            "Documents are downloaded 223 of 544\n",
            "Documents are downloaded 224 of 544\n",
            "Documents are downloaded 225 of 544\n",
            "Documents are downloaded 226 of 544\n",
            "Documents are downloaded 227 of 544\n",
            "Documents are downloaded 228 of 544\n",
            "Documents are downloaded 229 of 544\n",
            "Documents are downloaded 230 of 544\n",
            "Documents are downloaded 231 of 544\n",
            "Documents are downloaded 232 of 544\n",
            "Documents are downloaded 233 of 544\n",
            "Documents are downloaded 234 of 544\n",
            "Documents are downloaded 235 of 544\n",
            "Documents are downloaded 236 of 544\n",
            "Documents are downloaded 237 of 544\n",
            "Documents are downloaded 238 of 544\n",
            "Documents are downloaded 239 of 544\n",
            "Documents are downloaded 240 of 544\n",
            "Documents are downloaded 241 of 544\n",
            "Documents are downloaded 242 of 544\n",
            "Documents are downloaded 243 of 544\n",
            "Documents are downloaded 244 of 544\n",
            "Documents are downloaded 245 of 544\n",
            "Documents are downloaded 246 of 544\n",
            "Documents are downloaded 247 of 544\n",
            "Documents are downloaded 248 of 544\n",
            "Documents are downloaded 249 of 544\n",
            "Documents are downloaded 250 of 544\n",
            "Documents are downloaded 251 of 544\n",
            "Documents are downloaded 252 of 544\n",
            "Documents are downloaded 253 of 544\n",
            "Documents are downloaded 254 of 544\n",
            "Documents are downloaded 255 of 544\n",
            "Documents are downloaded 256 of 544\n",
            "Documents are downloaded 257 of 544\n",
            "Documents are downloaded 258 of 544\n",
            "Documents are downloaded 259 of 544\n",
            "Documents are downloaded 260 of 544\n",
            "Documents are downloaded 261 of 544\n",
            "Documents are downloaded 262 of 544\n",
            "Documents are downloaded 263 of 544\n",
            "Documents are downloaded 264 of 544\n",
            "Documents are downloaded 265 of 544\n",
            "Documents are downloaded 266 of 544\n",
            "Documents are downloaded 267 of 544\n",
            "Documents are downloaded 268 of 544\n",
            "Documents are downloaded 269 of 544\n",
            "Documents are downloaded 270 of 544\n",
            "Documents are downloaded 271 of 544\n",
            "Documents are downloaded 272 of 544\n",
            "Documents are downloaded 273 of 544\n",
            "Documents are downloaded 274 of 544\n",
            "Documents are downloaded 275 of 544\n",
            "Documents are downloaded 276 of 544\n",
            "Documents are downloaded 277 of 544\n",
            "Documents are downloaded 278 of 544\n",
            "Documents are downloaded 279 of 544\n",
            "Documents are downloaded 280 of 544\n",
            "Documents are downloaded 281 of 544\n",
            "Documents are downloaded 282 of 544\n",
            "Documents are downloaded 283 of 544\n",
            "Documents are downloaded 284 of 544\n",
            "Documents are downloaded 285 of 544\n",
            "Documents are downloaded 286 of 544\n",
            "Documents are downloaded 287 of 544\n",
            "Documents are downloaded 288 of 544\n",
            "Documents are downloaded 289 of 544\n",
            "Documents are downloaded 290 of 544\n",
            "Documents are downloaded 291 of 544\n",
            "Documents are downloaded 292 of 544\n",
            "Documents are downloaded 293 of 544\n",
            "Documents are downloaded 294 of 544\n",
            "Documents are downloaded 295 of 544\n",
            "Documents are downloaded 296 of 544\n",
            "Documents are downloaded 297 of 544\n",
            "Documents are downloaded 298 of 544\n",
            "Documents are downloaded 299 of 544\n",
            "Documents are downloaded 300 of 544\n",
            "Documents are downloaded 301 of 544\n",
            "Documents are downloaded 302 of 544\n",
            "Documents are downloaded 303 of 544\n",
            "Documents are downloaded 304 of 544\n",
            "Documents are downloaded 305 of 544\n",
            "Documents are downloaded 306 of 544\n",
            "Documents are downloaded 307 of 544\n",
            "Documents are downloaded 308 of 544\n",
            "Documents are downloaded 309 of 544\n",
            "Documents are downloaded 310 of 544\n",
            "Documents are downloaded 311 of 544\n",
            "Documents are downloaded 312 of 544\n",
            "Documents are downloaded 313 of 544\n",
            "Documents are downloaded 314 of 544\n",
            "Documents are downloaded 315 of 544\n",
            "Documents are downloaded 316 of 544\n",
            "Documents are downloaded 317 of 544\n",
            "Documents are downloaded 318 of 544\n",
            "Documents are downloaded 319 of 544\n",
            "Documents are downloaded 320 of 544\n",
            "Documents are downloaded 321 of 544\n",
            "Documents are downloaded 322 of 544\n",
            "Documents are downloaded 323 of 544\n",
            "Documents are downloaded 324 of 544\n",
            "Documents are downloaded 325 of 544\n",
            "Documents are downloaded 326 of 544\n",
            "Documents are downloaded 327 of 544\n",
            "Documents are downloaded 328 of 544\n",
            "Documents are downloaded 329 of 544\n",
            "Documents are downloaded 330 of 544\n",
            "Documents are downloaded 331 of 544\n",
            "Documents are downloaded 332 of 544\n",
            "Documents are downloaded 333 of 544\n",
            "Documents are downloaded 334 of 544\n",
            "Documents are downloaded 335 of 544\n",
            "Documents are downloaded 336 of 544\n",
            "Documents are downloaded 337 of 544\n",
            "Documents are downloaded 338 of 544\n",
            "Documents are downloaded 339 of 544\n",
            "Documents are downloaded 340 of 544\n",
            "Documents are downloaded 341 of 544\n",
            "Documents are downloaded 342 of 544\n",
            "Documents are downloaded 343 of 544\n",
            "Documents are downloaded 344 of 544\n",
            "Documents are downloaded 345 of 544\n",
            "Documents are downloaded 346 of 544\n",
            "Documents are downloaded 347 of 544\n",
            "Documents are downloaded 348 of 544\n",
            "Documents are downloaded 349 of 544\n",
            "Documents are downloaded 350 of 544\n",
            "Documents are downloaded 351 of 544\n",
            "Documents are downloaded 352 of 544\n",
            "Documents are downloaded 353 of 544\n",
            "Documents are downloaded 354 of 544\n",
            "Documents are downloaded 355 of 544\n",
            "Documents are downloaded 356 of 544\n",
            "Documents are downloaded 357 of 544\n",
            "Documents are downloaded 358 of 544\n",
            "Documents are downloaded 359 of 544\n",
            "Documents are downloaded 360 of 544\n",
            "Documents are downloaded 361 of 544\n",
            "Documents are downloaded 362 of 544\n",
            "Documents are downloaded 363 of 544\n",
            "Documents are downloaded 364 of 544\n",
            "Documents are downloaded 365 of 544\n",
            "Documents are downloaded 366 of 544\n",
            "Documents are downloaded 367 of 544\n",
            "Documents are downloaded 368 of 544\n",
            "Documents are downloaded 369 of 544\n",
            "Documents are downloaded 370 of 544\n",
            "Documents are downloaded 371 of 544\n",
            "Documents are downloaded 372 of 544\n",
            "Documents are downloaded 373 of 544\n",
            "Documents are downloaded 374 of 544\n",
            "Documents are downloaded 375 of 544\n",
            "Documents are downloaded 376 of 544\n",
            "Documents are downloaded 377 of 544\n",
            "Documents are downloaded 378 of 544\n",
            "Documents are downloaded 379 of 544\n",
            "Documents are downloaded 380 of 544\n",
            "Documents are downloaded 381 of 544\n",
            "Documents are downloaded 382 of 544\n",
            "Documents are downloaded 383 of 544\n",
            "Documents are downloaded 384 of 544\n",
            "Documents are downloaded 385 of 544\n",
            "Documents are downloaded 386 of 544\n",
            "Documents are downloaded 387 of 544\n",
            "Documents are downloaded 388 of 544\n",
            "Documents are downloaded 389 of 544\n",
            "Documents are downloaded 390 of 544\n",
            "Documents are downloaded 391 of 544\n",
            "Documents are downloaded 392 of 544\n",
            "Documents are downloaded 393 of 544\n",
            "Documents are downloaded 394 of 544\n",
            "Documents are downloaded 395 of 544\n",
            "Documents are downloaded 396 of 544\n",
            "Documents are downloaded 397 of 544\n",
            "Documents are downloaded 398 of 544\n",
            "Documents are downloaded 399 of 544\n",
            "Documents are downloaded 400 of 544\n",
            "Documents are downloaded 401 of 544\n",
            "Documents are downloaded 402 of 544\n",
            "Documents are downloaded 403 of 544\n",
            "Documents are downloaded 404 of 544\n",
            "Documents are downloaded 405 of 544\n",
            "Documents are downloaded 406 of 544\n",
            "Documents are downloaded 407 of 544\n",
            "Documents are downloaded 408 of 544\n",
            "Documents are downloaded 409 of 544\n",
            "Documents are downloaded 410 of 544\n",
            "Documents are downloaded 411 of 544\n",
            "Documents are downloaded 412 of 544\n",
            "Documents are downloaded 413 of 544\n",
            "Documents are downloaded 414 of 544\n",
            "Documents are downloaded 415 of 544\n",
            "Documents are downloaded 416 of 544\n",
            "Documents are downloaded 417 of 544\n",
            "Documents are downloaded 418 of 544\n",
            "Documents are downloaded 419 of 544\n",
            "Documents are downloaded 420 of 544\n",
            "Documents are downloaded 421 of 544\n",
            "Documents are downloaded 422 of 544\n",
            "Documents are downloaded 423 of 544\n",
            "Documents are downloaded 424 of 544\n",
            "Documents are downloaded 425 of 544\n",
            "Documents are downloaded 426 of 544\n",
            "Documents are downloaded 427 of 544\n",
            "Documents are downloaded 428 of 544\n",
            "Documents are downloaded 429 of 544\n",
            "Documents are downloaded 430 of 544\n",
            "Documents are downloaded 431 of 544\n",
            "Documents are downloaded 432 of 544\n",
            "Documents are downloaded 433 of 544\n",
            "Documents are downloaded 434 of 544\n",
            "Documents are downloaded 435 of 544\n",
            "Documents are downloaded 436 of 544\n",
            "Documents are downloaded 437 of 544\n",
            "Documents are downloaded 438 of 544\n",
            "Documents are downloaded 439 of 544\n",
            "Documents are downloaded 440 of 544\n",
            "Documents are downloaded 441 of 544\n",
            "Documents are downloaded 442 of 544\n",
            "Documents are downloaded 443 of 544\n",
            "Documents are downloaded 444 of 544\n",
            "Documents are downloaded 445 of 544\n",
            "Documents are downloaded 446 of 544\n",
            "Documents are downloaded 447 of 544\n",
            "Documents are downloaded 448 of 544\n",
            "Documents are downloaded 449 of 544\n",
            "Documents are downloaded 450 of 544\n",
            "Documents are downloaded 451 of 544\n",
            "Documents are downloaded 452 of 544\n",
            "Documents are downloaded 453 of 544\n",
            "Documents are downloaded 454 of 544\n",
            "Documents are downloaded 455 of 544\n",
            "Documents are downloaded 456 of 544\n",
            "Documents are downloaded 457 of 544\n",
            "Documents are downloaded 458 of 544\n",
            "Documents are downloaded 459 of 544\n",
            "Documents are downloaded 460 of 544\n",
            "Documents are downloaded 461 of 544\n",
            "Documents are downloaded 462 of 544\n",
            "Documents are downloaded 463 of 544\n",
            "Documents are downloaded 464 of 544\n",
            "Documents are downloaded 465 of 544\n",
            "Documents are downloaded 466 of 544\n",
            "Documents are downloaded 467 of 544\n",
            "Documents are downloaded 468 of 544\n",
            "Documents are downloaded 469 of 544\n",
            "Documents are downloaded 470 of 544\n",
            "Documents are downloaded 471 of 544\n",
            "Documents are downloaded 472 of 544\n",
            "Documents are downloaded 473 of 544\n",
            "Documents are downloaded 474 of 544\n",
            "Documents are downloaded 475 of 544\n",
            "Documents are downloaded 476 of 544\n",
            "Documents are downloaded 477 of 544\n",
            "Documents are downloaded 478 of 544\n",
            "Documents are downloaded 479 of 544\n",
            "Documents are downloaded 480 of 544\n",
            "Documents are downloaded 481 of 544\n",
            "Documents are downloaded 482 of 544\n",
            "Documents are downloaded 483 of 544\n",
            "Documents are downloaded 484 of 544\n",
            "Documents are downloaded 485 of 544\n",
            "Documents are downloaded 486 of 544\n",
            "Documents are downloaded 487 of 544\n",
            "Documents are downloaded 488 of 544\n",
            "Documents are downloaded 489 of 544\n",
            "Documents are downloaded 490 of 544\n",
            "Documents are downloaded 491 of 544\n",
            "Documents are downloaded 492 of 544\n",
            "Documents are downloaded 493 of 544\n",
            "Documents are downloaded 494 of 544\n",
            "Documents are downloaded 495 of 544\n",
            "Documents are downloaded 496 of 544\n",
            "Documents are downloaded 497 of 544\n",
            "Documents are downloaded 498 of 544\n",
            "Documents are downloaded 499 of 544\n",
            "Documents are downloaded 500 of 544\n",
            "Documents are downloaded 501 of 544\n",
            "Documents are downloaded 502 of 544\n",
            "Documents are downloaded 503 of 544\n",
            "Documents are downloaded 504 of 544\n",
            "Documents are downloaded 505 of 544\n",
            "Documents are downloaded 506 of 544\n",
            "Documents are downloaded 507 of 544\n",
            "Documents are downloaded 508 of 544\n",
            "Documents are downloaded 509 of 544\n",
            "Documents are downloaded 510 of 544\n",
            "Documents are downloaded 511 of 544\n",
            "Documents are downloaded 512 of 544\n",
            "Documents are downloaded 513 of 544\n",
            "Documents are downloaded 514 of 544\n",
            "Documents are downloaded 515 of 544\n",
            "Documents are downloaded 516 of 544\n",
            "Documents are downloaded 517 of 544\n",
            "Documents are downloaded 518 of 544\n",
            "Documents are downloaded 519 of 544\n",
            "Documents are downloaded 520 of 544\n",
            "Documents are downloaded 521 of 544\n",
            "Documents are downloaded 522 of 544\n",
            "Documents are downloaded 523 of 544\n",
            "Documents are downloaded 524 of 544\n",
            "Documents are downloaded 525 of 544\n",
            "Documents are downloaded 526 of 544\n",
            "Documents are downloaded 527 of 544\n",
            "Documents are downloaded 528 of 544\n",
            "Documents are downloaded 529 of 544\n",
            "Documents are downloaded 530 of 544\n",
            "Documents are downloaded 531 of 544\n",
            "Documents are downloaded 532 of 544\n",
            "Documents are downloaded 533 of 544\n",
            "Documents are downloaded 534 of 544\n",
            "Documents are downloaded 535 of 544\n",
            "Documents are downloaded 536 of 544\n",
            "Documents are downloaded 537 of 544\n",
            "Documents are downloaded 538 of 544\n",
            "Documents are downloaded 539 of 544\n",
            "Documents are downloaded 540 of 544\n",
            "Documents are downloaded 541 of 544\n",
            "Documents are downloaded 542 of 544\n",
            "Documents are downloaded 543 of 544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание словарей из индекса в слова\n",
        "class2idx = dict(O=0, MET=1, ECO=2, BIN=3, CMP=4, QUA=5, ACT=6, INST=7, SOC=8)\n",
        "idx2class = {index:word for word, index in zip(class2idx.keys(), class2idx.values())}"
      ],
      "metadata": {
        "id": "s01vevPWviFh"
      },
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующая функция проставляет метки для словарей "
      ],
      "metadata": {
        "id": "Vgagiij2ayt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_labels(class2idx, spans, char_id, instances):\n",
        "\n",
        "  def change_label(id, idx):\n",
        "    # Изменение label, которое было получено при инициализации и смена флага\n",
        "    # id - индекс строки, idx - индекс слова \n",
        "    cl = class2idx[s[2]]\n",
        "    add_condition = instances['flag'][id][idx]\n",
        "\n",
        "    if add_condition:\n",
        "      sp.append(0)\n",
        "      instances['label'][id][idx] += cl\n",
        "      instances['flag'][id][idx] = False\n",
        "\n",
        "  instances = copy.deepcopy(instances)\n",
        "  add_to_i = 0 \n",
        "  sp = []\n",
        "  for c in range(len(spans.keys())): # итерация по spans каждого ann документа\n",
        "    for s in spans[str(c)]: # всех Итерация span в одном документе\n",
        "      char_id_text = char_id[str(c)] \n",
        "      for i, idd in enumerate(char_id_text): # итерация по диапазонам каждой строки txt документа\n",
        "\n",
        "        start, end = idd\n",
        "        \n",
        "        id = i + add_to_i\n",
        "        # Проверка входит ли диапазон из ann в диапазон строки из txt\n",
        "        if start <= s[0] <= end and start<= s[1] <= end + 2:\n",
        "          sentence = instances['sentence'][id]  \n",
        "          \n",
        "          for w in s[3:]:\n",
        "\n",
        "            # Иногда строка span может \"пройти\" мимо диапазона текста или нужные слова отличаются по написанию в txt и ann      \n",
        "            if w not in sentence:\n",
        "              \n",
        "              for back_step in range(1, id+1):\n",
        "\n",
        "                new_id = id - back_step\n",
        "                new_sentence = instances['sentence'][new_id]\n",
        "\n",
        "                if w in instances['sentence'][new_id]:\n",
        "                  idx = new_sentence.index(w.lower())\n",
        "\n",
        "                  # Вызов смены индекса\n",
        "                  change_label(new_id, idx)\n",
        "                  break\n",
        "\n",
        "                if new_id == 0 or back_step>1:\n",
        "                  break \n",
        "              \n",
        "              continue\n",
        "            \n",
        "            idx = sentence.index(w.lower())\n",
        "            \n",
        "            change_label(id, idx)\n",
        "\n",
        "          if s == spans[str(c)][-1]:\n",
        "            add_to_i += len(char_id_text)\n",
        "\n",
        "          break\n",
        "  \n",
        "\n",
        "  return instances"
      ],
      "metadata": {
        "id": "RiWKEszy-8So"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание spans для будущей разметки\n",
        "train_spans, train_class_spisok_number = create_spans(sorted(train_ann_path_spisok))\n",
        "test_spans, test_class_spisok_number = create_spans(sorted(test_ann_path_spisok))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS41iTSdp8Mx",
        "outputId": "c9c7e0ee-b92b-4306-9d8d-920dd9e0bc83"
      },
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Разметка по классам\n",
        "train_instances = create_labels(class2idx, train_spans, train_char_id, train_instances)\n",
        "test_instances = create_labels(class2idx, test_spans, test_char_id, test_instances)"
      ],
      "metadata": {
        "id": "oTTirCzcxxzR"
      },
      "execution_count": 368,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение на диск параметров, нужных для будущего датасета\n",
        "\n",
        "train_json_string = train_instances\n",
        "\n",
        "test_json_string = test_instances\n",
        "\n",
        "with open('/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Train/train_instances.json', 'w') as f:\n",
        "  json.dump(train_json_string, f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Train/train_class_spisok_number.json', 'w') as f:\n",
        "  json.dump(train_class_spisok_number, f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Train/train_all_words.json', 'w') as f:\n",
        "  json.dump(train_all_words, f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Train/train_dictionary.json', 'w') as f:\n",
        "  json.dump(train_dictionary, f)\n",
        "\n",
        "\n",
        "with open('/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Test/test_instances.json', 'w') as f:\n",
        "  json.dump(test_json_string, f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Test/test_class_spisok_number.json', 'w') as f:\n",
        "  json.dump(test_class_spisok_number, f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Test/test_all_words.json', 'w') as f:\n",
        "  json.dump(test_all_words, f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Test/test_dictionary.json', 'w') as f:\n",
        "  json.dump(test_dictionary, f)"
      ],
      "metadata": {
        "id": "4NHfNp1NiD35"
      },
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация параметров, loaded_train_instances, loaded_test_instances используются сразу в других ноутбуках\n",
        "filepath = '/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Train/train_instances.json'\n",
        "with open(filepath, encoding='utf-8') as f:\n",
        "  loaded_train_instances = json.load(f)\n",
        "\n",
        "filepath = '/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Train/train_class_spisok_number.json'\n",
        "with open(filepath, encoding='utf-8') as f:\n",
        "  train_class_spisok_number = json.load(f)\n",
        "\n",
        "filepath = '/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Train/train_all_words.json'\n",
        "with open(filepath, encoding='utf-8') as f:\n",
        "  loaded_train_all_words = json.load(f)\n",
        "\n",
        "filepath = '/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Train/train_dictionary.json'\n",
        "with open(filepath, encoding='utf-8') as f:\n",
        "  loaded_train_dictionary = json.load(f)\n",
        "\n",
        "\n",
        "filepath = '/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Test/test_instances.json'\n",
        "with open(filepath, encoding='utf-8') as f:\n",
        "  loaded_test_instances = json.load(f)\n",
        "\n",
        "filepath = '/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Test/test_class_spisok_number.json'\n",
        "with open(filepath, encoding='utf-8') as f:\n",
        "  test_class_spisok_number = json.load(f)\n",
        "\n",
        "filepath = '/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Test/test_all_words.json'\n",
        "with open(filepath, encoding='utf-8') as f:\n",
        "  loaded_test_all_words = json.load(f)\n",
        "\n",
        "filepath = '/content/gdrive/MyDrive/3_4_lab/TENER/Last_instances_json/Test/test_dictionary.json'\n",
        "with open(filepath, encoding='utf-8') as f:\n",
        "  loaded_test_dictionary = json.load(f)\n"
      ],
      "metadata": {
        "id": "y_0kVysOvXCm"
      },
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание словарей класс - индекс, с переименованием класса - необходимо в некоторых библиотеках fastNLP\n",
        "class2idx = dict(O=0, IMET=1, ECO=2, BIN=3, SMP=4, BUA=5, ICT=6, INST=7, SOC=8)\n",
        "idx2class = {index:word for word, index in zip(class2idx.keys(), class2idx.values())}\n",
        "\n",
        "train_class_O_number = len(loaded_train_all_words) - len(train_class_spisok_number)\n",
        "test_class_O_number = len(loaded_test_all_words) - len(test_class_spisok_number)\n",
        "\n",
        "train_class_spisok_number += ['O'] * train_class_O_number\n",
        "test_class_spisok_number += ['O'] * test_class_O_number\n",
        "\n",
        "train_class_freq = Counter(train_class_spisok_number)\n",
        "test_class_freq = Counter(test_class_spisok_number)"
      ],
      "metadata": {
        "id": "JNBkwb87jPZn"
      },
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание datasets для тренировки и теста с использованием DataSet от fastNLP\n",
        "\n",
        "dt_train = DataSet(loaded_train_instances)\n",
        "\n",
        "dt_train.rename_field('ords', 'chars')\n",
        "dt_train.rename_field('label', 'target')\n",
        "\n",
        "dt_train.set_padder('chars', AutoPadder())\n",
        "dt_train.set_input('chars', 'target')\n",
        "dt_train.set_target('seq_len')\n",
        "dt_train.set_pad_val('chars', 500001)\n",
        "\n",
        "\n",
        "dt_test = DataSet(loaded_test_instances)\n",
        "\n",
        "dt_test.rename_field('ords', 'chars')\n",
        "dt_test.rename_field('label', 'target')\n",
        "\n",
        "dt_test.set_padder('chars', AutoPadder())\n",
        "dt_test.set_input('chars', 'target')\n",
        "dt_test.set_target('seq_len', 'target')\n",
        "dt_test.set_pad_val('chars', 500001)\n"
      ],
      "metadata": {
        "id": "NZh_AqmYBTDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f910dd2-5791-4bd0-d441-bb2068b74968"
      },
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+------------------+---------+--------------------+--------------------+--------------------+\n",
              "| sentence         | seq_len | flag               | chars              | target             |\n",
              "+------------------+---------+--------------------+--------------------+--------------------+\n",
              "| ['докумен...     | 3       | [True, True, Tr... | [112995, 331177... | [0, 0, 0]          |\n",
              "| ['админис...     | 3       | [True, True, Fa... | [16879, 62910, ... | [0, 0, 2]          |\n",
              "| ['постано...     | 1       | [True]             | [324895]           | [0]                |\n",
              "| ['от', '22', ... | 7       | [True, True, Tr... | [274784, 500000... | [0, 0, 0, 0, 0,... |\n",
              "| ['о', 'бюдж...   | 5       | [True, True, Tr... | [252874, 51049,... | [0, 0, 0, 0, 0]... |\n",
              "| ['на', 'пер...   | 5       | [True, True, Tr... | [225007, 296266... | [0, 0, 0, 0, 0]... |\n",
              "| ['в', 'соот...   | 27      | [True, True, Tr... | [51199, 410755,... | [0, 0, 0, 0, 0,... |\n",
              "| ['1', 'утве...   | 13      | [True, False, T... | [500000, 457821... | [0, 3, 0, 0, 0,... |\n",
              "| ['2', 'конт...   | 21      | [True, True, Tr... | [500000, 178907... | [0, 0, 0, 0, 0,... |\n",
              "| ['3', 'наст...   | 11      | [True, True, Tr... | [500000, 234093... | [0, 0, 0, 0, 1,... |\n",
              "| ['губерна...     | 2       | [True, False]      | [98431, 255249]... | [0, 1]             |\n",
              "| ['с.ю.орло...    | 1       | [True]             | [500000]           | [0]                |\n",
              "| ...              | ...     | ...                | ...                | ...                |\n",
              "+------------------+---------+--------------------+--------------------+--------------------+"
            ]
          },
          "metadata": {},
          "execution_count": 373
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание словаря слово - индекс и списка частоты слов для библиотеки CNNCharEmbedding\n",
        "word2idx = dict()\n",
        "word2idx['<pad>'] = 500001\n",
        "word2idx['<unk>'] = 500000\n",
        "for t in loaded_train_dictionary:\n",
        "\n",
        "  word, id = t\n",
        "  word2idx[word] = id\n",
        "\n",
        "for t in loaded_test_dictionary:\n",
        "\n",
        "  word, id = t\n",
        "  word2idx[word] = id\n",
        "\n",
        "idx2word = {index:word for word, index in zip(word2idx.keys(), word2idx.values())}\n",
        "train_freq = Counter(loaded_train_all_words)"
      ],
      "metadata": {
        "id": "aVqvCtVHkqpt"
      },
      "execution_count": 374,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class voc:\n",
        "\n",
        "  def __init__(self, word2idx, idx2word, word_count, max_size=None, min_freq=None, padding='<pad>', unknown='<unk>', rebuild=False, _no_create_word=Counter()):\n",
        "\n",
        "    self._word2idx = word2idx\n",
        "    self._idx2word = idx2word\n",
        "    self.max_size = max_size\n",
        "    self.min_freq = min_freq\n",
        "    self.padding = padding\n",
        "    self.unknown = unknown\n",
        "    self.rebuild = rebuild\n",
        "    self._no_create_word = _no_create_word\n",
        "    self.word_count = word_count\n",
        "    self.padding_idx = 0\n",
        "    self.unknown_idx = 0\n",
        "\n",
        "  def __iter__(self):\n",
        "        \n",
        "        for word, index in self._word2idx.items():\n",
        "            yield self._idx2word[index], index\n",
        "\n",
        "  def _is_word_no_create_entry(self, word):\n",
        "        \n",
        "        return word in self._no_create_word\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self._word2idx)\n"
      ],
      "metadata": {
        "id": "F-TuQYlNCr29"
      },
      "execution_count": 375,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# упаковка словарей для CNNCharEmbedding\n",
        "dict_for_embed = voc(word2idx=word2idx, idx2word=idx2word,word_count=train_freq)"
      ],
      "metadata": {
        "id": "Stt-PHfwC5bq"
      },
      "execution_count": 376,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Библиотека CNNCharEmbedding от fastNLP, изменный для работы с Navec\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from fastNLP.embeddings.embedding import TokenEmbedding\n",
        "from fastNLP.embeddings.static_embedding import StaticEmbedding\n",
        "from fastNLP.embeddings.utils import _construct_char_vocab_from_vocab\n",
        "from fastNLP.embeddings.utils import get_embeddings\n",
        "from fastNLP.core import logger\n",
        "from fastNLP.core.vocabulary import Vocabulary\n",
        "\n",
        "class CNNCharEmbedding(TokenEmbedding):\n",
        "    r\"\"\"\n",
        "    使用CNN生成character embedding。CNN的结构为, embed(x) -> Dropout(x) -> CNN(x) -> activation(x) -> pool -> fc -> Dropout.\n",
        "    不同的kernel大小的fitler结果是concat起来然后通过一层fully connected layer, 然后输出word的表示。\n",
        "\n",
        "    Example::\n",
        "\n",
        "        >>> import torch\n",
        "        >>> from fastNLP import Vocabulary\n",
        "        >>> from fastNLP.embeddings import CNNCharEmbedding\n",
        "        >>> vocab = Vocabulary().add_word_lst(\"The whether is good .\".split())\n",
        "        >>> embed = CNNCharEmbedding(vocab, embed_size=50)\n",
        "        >>> words = torch.LongTensor([[vocab.to_index(word) for word in \"The whether is good .\".split()]])\n",
        "        >>> outputs = embed(words)\n",
        "        >>> outputs.size()\n",
        "        >>> # torch.Size([1, 5，50])\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab: Vocabulary, embed_size: int = 50, char_emb_size: int = 50, word_dropout: float = 0,\n",
        "                 dropout: float = 0, filter_nums: List[int] = (40, 30, 20), kernel_sizes: List[int] = (5, 3, 1),\n",
        "                 pool_method: str = 'max', activation='relu', min_char_freq: int = 2, pre_train_char_embed: str = None,\n",
        "                 requires_grad:bool=True, include_word_start_end:bool=True):\n",
        "        \n",
        "        super(CNNCharEmbedding, self).__init__(vocab, word_dropout=word_dropout, dropout=dropout)\n",
        "        \n",
        "        for kernel in kernel_sizes:\n",
        "            assert kernel % 2 == 1, \"Only odd kernel is allowed.\"\n",
        "        \n",
        "        assert pool_method in ('max', 'avg')\n",
        "        self.pool_method = pool_method\n",
        "        \n",
        "        if isinstance(activation, str):\n",
        "            if activation.lower() == 'relu':\n",
        "                self.activation = F.relu\n",
        "            elif activation.lower() == 'sigmoid':\n",
        "                self.activation = F.sigmoid\n",
        "            elif activation.lower() == 'tanh':\n",
        "                self.activation = F.tanh\n",
        "        elif activation is None:\n",
        "            self.activation = lambda x: x\n",
        "        elif callable(activation):\n",
        "            self.activation = activation\n",
        "        else:\n",
        "            raise Exception(\n",
        "                \"Undefined activation function: choose from: [relu, tanh, sigmoid, or a callable function]\")\n",
        "        \n",
        "        logger.info(\"Start constructing character vocabulary.\")\n",
        "        \n",
        "        self.char_vocab = _construct_char_vocab_from_vocab(vocab, min_freq=min_char_freq,\n",
        "                                                           include_word_start_end=include_word_start_end)\n",
        "        self.char_pad_index = self.char_vocab.padding_idx\n",
        "        \n",
        "        logger.info(f\"In total, there are {len(self.char_vocab)} distinct characters.\")\n",
        "        \n",
        "        max_word_len = max(map(lambda x: len(x[0]), vocab))\n",
        "        if include_word_start_end:\n",
        "            max_word_len += 2\n",
        "        \n",
        "        self.register_buffer('words_to_chars_embedding', torch.full((500002, max_word_len),\n",
        "                                                                fill_value=self.char_pad_index, dtype=torch.long))\n",
        "        self.register_buffer('word_lengths', torch.zeros(500002).long())\n",
        "        \n",
        "        for word, index in vocab:\n",
        "            \n",
        "            if include_word_start_end:\n",
        "                word = ['<bow>'] + list(word) + ['<eow>']\n",
        "            \n",
        "            self.words_to_chars_embedding[index, :len(word)] = torch.LongTensor([self.char_vocab.to_index(c) for c in word])\n",
        "            self.word_lengths[index] = len(word)\n",
        "        \n",
        "        if pre_train_char_embed:\n",
        "            self.char_embedding = StaticEmbedding(self.char_vocab, model_dir_or_name=pre_train_char_embed)\n",
        "        else:\n",
        "            self.char_embedding = get_embeddings((len(self.char_vocab), char_emb_size))\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(\n",
        "            self.char_embedding.embedding_dim, filter_nums[i], kernel_size=kernel_sizes[i], bias=True,\n",
        "            padding=kernel_sizes[i] // 2)\n",
        "            for i in range(len(kernel_sizes))])\n",
        "        self._embed_size = embed_size\n",
        "        self.fc = nn.Linear(sum(filter_nums), embed_size)\n",
        "        self.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, words):\n",
        "        r\"\"\"\n",
        "        \n",
        "\n",
        "        :param words: [batch_size, max_len]\n",
        "        :return: [batch_size, max_len, embed_size]\n",
        "        \"\"\"\n",
        "        words = self.drop_word(words)\n",
        "        batch_size, max_len = words.size()\n",
        "        chars = self.words_to_chars_embedding[words]  # batch_size x max_len x max_word_len\n",
        "        word_lengths = self.word_lengths[words]  # batch_size x max_len\n",
        "        max_word_len = word_lengths.max()\n",
        "        chars = chars[:, :, :max_word_len]\n",
        "        \n",
        "        chars_masks = chars.eq(self.char_pad_index)  # batch_size x max_len x max_word_len \n",
        "        chars = self.char_embedding(chars)  # batch_size x max_len x max_word_len x embed_size\n",
        "        chars = self.dropout(chars)\n",
        "        reshaped_chars = chars.reshape(batch_size * max_len, max_word_len, -1)\n",
        "        reshaped_chars = reshaped_chars.transpose(1, 2)  # B' x E x M\n",
        "        conv_chars = [conv(reshaped_chars).transpose(1, 2).reshape(batch_size, max_len, max_word_len, -1)\n",
        "                      for conv in self.convs]\n",
        "        conv_chars = torch.cat(conv_chars, dim=-1).contiguous()  # B x max_len x max_word_len x sum(filters)\n",
        "        conv_chars = self.activation(conv_chars)\n",
        "        if self.pool_method == 'max':\n",
        "            conv_chars = conv_chars.masked_fill(chars_masks.unsqueeze(-1), float('-inf'))\n",
        "            chars, _ = torch.max(conv_chars, dim=-2)  # batch_size x max_len x sum(filters)\n",
        "        else:\n",
        "            conv_chars = conv_chars.masked_fill(chars_masks.unsqueeze(-1), 0)\n",
        "            chars = torch.sum(conv_chars, dim=-2) / chars_masks.eq(False).sum(dim=-1, keepdim=True).float()\n",
        "        chars = self.fc(chars)\n",
        "        return self.dropout(chars)\n"
      ],
      "metadata": {
        "id": "ij1UM0JdoaO5"
      },
      "execution_count": 377,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_embed = CNNCharEmbedding(vocab=dict_for_embed, embed_size=90, char_emb_size=90, \n",
        "                              filter_nums=[90], kernel_sizes=[3], word_dropout=0, \n",
        "                              dropout=0.1, pool_method='max', \n",
        "                              include_word_start_end=False, min_char_freq=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxhlCLYYD496",
        "outputId": "439e7a71-0d66-4f0c-bca5-9e3a6c5a4e59"
      },
      "execution_count": 378,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start constructing character vocabulary.\n",
            "In total, there are 62 distinct characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.container import ModuleList\n",
        "from slovnet.model.emb import NavecEmbedding\n",
        "# Создание embedding для модели, использующий CNNCharEmbedding и Navec\n",
        "class embed:\n",
        "\n",
        "  def __init__(self, word_embed, char_embed, p=0.1):\n",
        "\n",
        "    self.word_embed = nn.Sequential(word_embed, nn.Dropout(p=p))\n",
        "    self.char_embed = char_embed\n",
        "    \n",
        "    self.embed = nn.ModuleList([self.word_embed, self.char_embed])\n",
        "\n",
        "    self.embed_size = 330\n",
        "\n",
        "  def forward(self, id):\n",
        "\n",
        "    assert type(id) == torch.Tensor, 'emb only processes torch.Tensor'\n",
        "    assert len(id.shape) == 2, f'emb only processes Tensor with 2 dimensions, your id.shape = {id.shape}'\n",
        "\n",
        "    out = self.embed[0](id)\n",
        "    for embed in self.embed[1:]:\n",
        "\n",
        "      out = torch.cat((out, embed(id)), -1)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "QDHZNYSyVg5q"
      },
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка работы embedding\n",
        "\n",
        "word_embed = NavecEmbedding(navec)\n",
        "\n",
        "embedding = embed(word_embed=word_embed, char_embed=char_embed)\n",
        "count = 0\n",
        "for id, y in DataSetIter(dt_train, sampler=SequentialSampler(), batch_size=2):\n",
        "\n",
        "  words = id['chars']\n",
        "  count += 1\n",
        "  out = embedding.forward(words)\n",
        "  if count == 100:\n",
        "    break\n",
        "\n",
        "count = 0\n",
        "for id, y in DataSetIter(dt_test, sampler=SequentialSampler(), batch_size=2):\n",
        "\n",
        "  words = id['chars']\n",
        "  count += 1\n",
        "  out = embedding.forward(words)\n",
        "\n",
        "  if count == 100:\n",
        "    break  "
      ],
      "metadata": {
        "id": "aiZpdntfduXJ"
      },
      "execution_count": 380,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastNLP.core.vocabulary import Vocabulary\n",
        "\n",
        "class tar(Vocabulary):\n",
        "\n",
        "  def __init__(self, word2idx, idx2word, word_count, max_size=None, min_freq=None, \n",
        "               padding='<pad>', unknown='<unk>', rebuild=False, _no_create_word=Counter()):\n",
        "\n",
        "    self._word2idx = word2idx\n",
        "    self._idx2word = idx2word\n",
        "    self.max_size = max_size\n",
        "    self.min_freq = min_freq\n",
        "    self.padding = padding\n",
        "    self.unknown = unknown\n",
        "    self.rebuild = rebuild\n",
        "    self._no_create_word = _no_create_word\n",
        "    self.word_count = word_count\n",
        "    self.padding = None\n",
        "    self.unknown = None\n",
        "\n",
        "  def __iter__(self):\n",
        "        \n",
        "        for index in range(len(self._word2idx)):\n",
        "            yield self._idx2word[index], index\n",
        "\n",
        "  def _is_word_no_create_entry(self, word):\n",
        "        \n",
        "        return word in self._no_create_word\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self._word2idx)\n",
        "\n",
        "  def items(self):\n",
        "        return self._idx2word.items()"
      ],
      "metadata": {
        "id": "f8iuz7mthNsb"
      },
      "execution_count": 381,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# упаковка меток и словарей \n",
        "tar_for_embed = tar(word2idx=class2idx, idx2word=idx2class,word_count=train_class_freq)"
      ],
      "metadata": {
        "id": "BRufQjcLiMw1"
      },
      "execution_count": 382,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дальше идут функции, необходимые для работы Trainer от fastNLP. Сам Trainer был немного изменен в некоторых строках для работы с Navec"
      ],
      "metadata": {
        "id": "EmwnwEJ2jwLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_CHECK_BATCH_SIZE = 2\n",
        "DEFAULT_CHECK_NUM_BATCH = 2\n",
        "\n",
        "\n",
        "def _get_value_info(_dict):\n",
        "    # given a dict value, return information about this dict's value. Return list of str\n",
        "    strs = []\n",
        "    for key, value in _dict.items():\n",
        "        _str = ''\n",
        "        if isinstance(value, torch.Tensor):\n",
        "            _str += \"\\t{}: (1)type:torch.Tensor (2)dtype:{}, (3)shape:{} \".format(key,\n",
        "                                                                                  value.dtype, value.size())\n",
        "        elif isinstance(value, np.ndarray):\n",
        "            _str += \"\\t{}: (1)type:numpy.ndarray (2)dtype:{}, (3)shape:{} \".format(key,\n",
        "                                                                                   value.dtype, value.shape)\n",
        "        else:\n",
        "            _str += \"\\t{}: type:{}\".format(key, type(value))\n",
        "        strs.append(_str)\n",
        "    return strs\n",
        "\n",
        "\n",
        "from numbers import Number\n",
        "from fastNLP.core.batch import _to_tensor\n",
        "\n",
        "\n",
        "def _check_code(dataset, model, losser, metrics, forward_func, batch_size=DEFAULT_CHECK_BATCH_SIZE,\n",
        "                dev_data=None, metric_key=None, check_level=0):\n",
        "    # check get_loss 方法\n",
        "    model_device = _get_model_device(model=model)\n",
        "    def _iter():\n",
        "        start_idx = 0\n",
        "        while start_idx<len(dataset):\n",
        "            batch_x = {}\n",
        "            batch_y = {}\n",
        "            for field_name, field in dataset.get_all_fields().items():\n",
        "                indices = list(range(start_idx, min(start_idx+batch_size, len(dataset))))\n",
        "                if field.is_target or field.is_input:\n",
        "                    batch = field.get(indices)\n",
        "                    if field.dtype is not None and \\\n",
        "                            issubclass(field.dtype, Number) and not isinstance(batch, torch.Tensor):\n",
        "                        batch, _ = _to_tensor(batch, field.dtype)\n",
        "                    if field.is_target:\n",
        "                        batch_y[field_name] = batch\n",
        "                    if field.is_input:\n",
        "                        batch_x[field_name] = batch\n",
        "            yield (batch_x, batch_y)\n",
        "            start_idx += batch_size\n",
        "\n",
        "    for batch_count, (batch_x, batch_y) in enumerate(_iter()):\n",
        "        _move_dict_value_to_device(batch_x, batch_y, device=model_device)\n",
        "        # forward check\n",
        "        if batch_count == 0:\n",
        "            info_str = \"\"\n",
        "            input_fields = _get_value_info(batch_x)\n",
        "            target_fields = _get_value_info(batch_y)\n",
        "            if len(input_fields) > 0:\n",
        "                info_str += \"input fields after batch(if batch size is {}):\\n\".format(batch_size)\n",
        "                info_str += \"\\n\".join(input_fields)\n",
        "                info_str += '\\n'\n",
        "            else:\n",
        "                raise RuntimeError(\"There is no input field.\")\n",
        "            if len(target_fields) > 0:\n",
        "                info_str += \"target fields after batch(if batch size is {}):\\n\".format(batch_size)\n",
        "                info_str += \"\\n\".join(target_fields)\n",
        "                info_str += '\\n'\n",
        "            else:\n",
        "                info_str += 'There is no target field.'\n",
        "            logger.info(info_str)\n",
        "            _check_forward_error(forward_func=forward_func, dataset=dataset,\n",
        "                                 batch_x=batch_x, check_level=check_level)\n",
        "        refined_batch_x = _build_args(forward_func, **batch_x)\n",
        "        pred_dict = model(**refined_batch_x)\n",
        "        func_signature = _get_func_signature(forward_func)\n",
        "        if not isinstance(pred_dict, dict):\n",
        "            raise TypeError(f\"The return value of {func_signature} should be `dict`, not `{type(pred_dict)}`.\")\n",
        "        \n",
        "        # loss check\n",
        "        try:\n",
        "            loss = losser(pred_dict, batch_y)\n",
        "            # check loss output\n",
        "            if batch_count == 0:\n",
        "                if not isinstance(loss, torch.Tensor):\n",
        "                    raise TypeError(\n",
        "                        f\"The return value of {_get_func_signature(losser.get_loss)} should be `torch.Tensor`, \"\n",
        "                        f\"but got `{type(loss)}`.\")\n",
        "                if len(loss.size()) != 0:\n",
        "                    raise ValueError(\n",
        "                        f\"The size of return value of {_get_func_signature(losser.get_loss)} is {loss.size()}, \"\n",
        "                        f\"should be torch.size([])\")\n",
        "            loss.backward()\n",
        "        except _CheckError as e:\n",
        "            # TODO: another error raised if _CheckError caught\n",
        "            pre_func_signature = _get_func_signature(forward_func)\n",
        "            _check_loss_evaluate(prev_func_signature=pre_func_signature, func_signature=e.func_signature,\n",
        "                                 check_res=e.check_res, pred_dict=pred_dict, target_dict=batch_y,\n",
        "                                 dataset=dataset, check_level=check_level)\n",
        "        model.zero_grad()\n",
        "        if batch_count + 1 >= DEFAULT_CHECK_NUM_BATCH:\n",
        "            break\n",
        "    \n",
        "    if dev_data is not None:\n",
        "        tester = Tester(data=dev_data[:batch_size * DEFAULT_CHECK_NUM_BATCH], model=model, metrics=metrics,\n",
        "                        batch_size=batch_size, verbose=-1, use_tqdm=False)\n",
        "        evaluate_results = tester.test()\n",
        "        _check_eval_results(metrics=evaluate_results, metric_key=metric_key, metric_list=metrics)\n",
        "\n",
        "\n",
        "def _check_eval_results(metrics, metric_key, metric_list):\n",
        "    # metrics: tester返回的结果\n",
        "    # metric_key: 一个用来做筛选的指标，来自Trainer的初始化\n",
        "    # metric_list: 多个用来做评价的指标，来自Trainer的初始化\n",
        "    if isinstance(metrics, tuple):\n",
        "        loss, metrics = metrics\n",
        "    \n",
        "    if isinstance(metrics, dict):\n",
        "        metric_dict = list(metrics.values())[0]  # 取第一个metric\n",
        "        \n",
        "        if metric_key is None:\n",
        "            indicator_val, indicator = list(metric_dict.values())[0], list(metric_dict.keys())[0]\n",
        "        else:\n",
        "            # metric_key is set\n",
        "            if metric_key not in metric_dict:\n",
        "                raise RuntimeError(f\"metric key {metric_key} not found in {metric_dict}\")\n",
        "            indicator_val = metric_dict[metric_key]\n",
        "            indicator = metric_key\n",
        "    else:\n",
        "        raise RuntimeError(\"Invalid metrics type. Expect {}, got {}\".format((tuple, dict), type(metrics)))\n",
        "    return indicator, indicator_val\n",
        "\n",
        "class Trainer(object):\n",
        "    \"\"\"\n",
        "    Trainer在fastNLP中用于组织单任务的训练过程，可以避免用户在不同训练任务中重复撰写\n",
        "        (1) epoch循环;\n",
        "        (2) 将数据分成不同的Batch;\n",
        "        (3) 对Batch进行pad;\n",
        "        (4) 每个epoch结束或一定step后进行验证集验证;\n",
        "        (5) 保存获得更好验证性能的模型等。\n",
        "    \n",
        "    详细的介绍参见 :mod:`fastNLP.core.trainer`\n",
        "    \"\"\"\n",
        "    def __init__(self, train_data, model, optimizer=None, loss=None,\n",
        "                 batch_size=32, sampler=None, drop_last=False, update_every=1,\n",
        "                 num_workers=0, n_epochs=10, print_every=5,\n",
        "                 dev_data=None, metrics=None, metric_key=None,\n",
        "                 validate_every=-1, save_path=None, use_tqdm=True, device=None,\n",
        "                 callbacks=None, check_code_level=0, **kwargs):\n",
        "        \"\"\"\n",
        "        \n",
        "        :param train_data: 训练集， :class:`~fastNLP.DataSet` 类型。\n",
        "        :param nn.modules model: 待训练的模型\n",
        "        :param optimizer: `torch.optim.Optimizer` 优化器。如果为None，则Trainer使用默认的Adam(model.parameters(), lr=4e-3)这个优化器\n",
        "        :param int batch_size: 训练和验证的时候的batch大小。\n",
        "        :param loss: 使用的 :class:`~fastNLP.core.losses.LossBase` 对象。当为None时，默认使用 :class:`~fastNLP.LossInForward`\n",
        "        :param sampler: Batch数据生成的顺序， :class:`~fastNLP.Sampler` 类型。如果为None，默认使用 :class:`~fastNLP.RandomSampler`\n",
        "        :param drop_last: 如果最后一个batch没有正好为batch_size这么多数据，就扔掉最后一个batch\n",
        "        :param num_workers: int, 有多少个线程来进行数据pad处理。\n",
        "        :param update_every: int, 多少步更新一次梯度。用于希望累计梯度的场景，比如需要128的batch_size, 但是直接设为128\n",
        "            会导致内存不足，通过设置batch_size=32, update_every=4达到目的。当optimizer为None时，该参数无效。\n",
        "        :param int n_epochs: 需要优化迭代多少次。\n",
        "        :param int print_every: 多少次反向传播更新tqdm显示的loss; 如果use_tqdm=False, 则多少次反向传播打印loss。\n",
        "        :param dev_data: 用于做验证的DataSet， :class:`~fastNLP.DataSet` 类型。\n",
        "        :param metrics: 验证的评估函数。可以只使用一个 :class:`Metric<fastNLP.core.metrics.MetricBase>` ，\n",
        "            也可以使用多个 :class:`Metric<fastNLP.core.metrics.MetricBase>` ，通过列表传入。\n",
        "            如验证时取得了更好的验证结果(如果有多个Metric，以列表中第一个Metric为准)，且save_path不为None，\n",
        "            则保存当前模型。Metric种类详见 :mod:`metrics模块 <fastNLP.core.metrics>` 。仅在传入dev_data时有效。\n",
        "        :param str,None metric_key:  :class:`Metric<fastNLP.core.metrics.MetricBase>` 有时会有多个指标，\n",
        "            比如 :class:`~fastNLP.core.metrics.SpanFPreRecMetric` 中包含了'f', 'pre', 'rec'。此时需\n",
        "            要指定以哪个指标为准。另外有些指标是越小效果越好，比如语言模型的困惑度，这种情况下，在key前面增加一个'-'来表\n",
        "            明验证时，值越小越好(比如: \"-ppl\")。仅在传入dev_data时有效。\n",
        "        :param int validate_every: 多少个step在验证集上验证一次; 如果为-1，则每个epoch结束验证一次。仅在传入dev_data时有效。\n",
        "        :param str,None save_path: 将模型保存路径，如果路径不存在，将自动创建文件夹。如果为None，则不保存模型。如果dev_data为None，则保存\n",
        "            最后一次迭代的模型。保存的时候不仅保存了参数，还保存了模型结构。即便使用DataParallel，这里也只保存模型。\n",
        "        :param bool use_tqdm: 是否使用tqdm来显示训练进度; 如果为False，则将loss打印在终端中。\n",
        "        :param str,int,torch.device,list(int) device: 将模型load到哪个设备。默认为None，即Trainer不对模型\n",
        "            的计算位置进行管理。支持以下的输入:\n",
        "    \n",
        "            1. str: ['cpu', 'cuda', 'cuda:0', 'cuda:1', ...] 依次为'cpu'中, 可见的第一个GPU中, 可见的第一个GPU中,\n",
        "            可见的第二个GPU中;\n",
        "    \n",
        "            2. torch.device：将模型装载到torch.device上。\n",
        "    \n",
        "            3. int: 将使用device_id为该值的gpu进行训练\n",
        "    \n",
        "            4. list(int)：如果多于1个device，将使用torch.nn.DataParallel包裹model, 并使用传入的device。\n",
        "    \n",
        "            5. None. 为None则不对模型进行任何处理，如果传入的model为torch.nn.DataParallel该值必须为None。\n",
        "    \n",
        "            已知可能会出现的问题：Adagrad优化器可能无法正常使用这个参数，请手动管理模型位置。\n",
        "    \n",
        "        :param list(callbacks) callbacks: 用于在train过程中起调节作用的回调函数。比如early stop，negative sampling等可以\n",
        "            通过callback机制实现。 可使用的callback参见 :mod:`callback模块 <fastNLP.core.callback>`\n",
        "        :param int check_code_level: 模型检查等级. -1: 不进行检查; 0: 仅出现错误时停止; 1: 如果有field没有被使用，\n",
        "            报告警告信息; 2: 有任何field没有被使用都报错. 检查的原理是通过使用很小的batch(默认2个sample)来运行代码，但是\n",
        "            这个过程理论上不会修改任何参数，只是会检查能否运行。但如果(1)模型中存在将batch_size写为某个固定值的情况；\n",
        "            (2)模型中存在累加前向计算次数的，可能会多计算1次。以上情况建议将check_code_level设置为-1。\n",
        "        \"\"\"\n",
        "        super(Trainer, self).__init__()\n",
        "        if not isinstance(model, nn.Module):\n",
        "            raise TypeError(f\"The type of model must be torch.nn.Module, got {type(model)}.\")\n",
        "\n",
        "        # check metrics and dev_data\n",
        "        if (not metrics) and dev_data is not None:\n",
        "            raise ValueError(\"No metric for dev_data evaluation.\")\n",
        "        if metrics and (dev_data is None):\n",
        "            raise ValueError(\"No dev_data for evaluations, pass dev_data or set metrics to None. \")\n",
        "\n",
        "        # check update every\n",
        "        assert update_every >= 1, \"update_every must be no less than 1.\"\n",
        "        self.update_every = int(update_every)\n",
        "\n",
        "        # check save_path\n",
        "        if not (save_path is None or isinstance(save_path, str)):\n",
        "            raise ValueError(\"save_path can only be None or `str`.\")\n",
        "        # prepare evaluate\n",
        "        metrics = _prepare_metrics(metrics)\n",
        "\n",
        "        # parse metric_key\n",
        "        # increase_better is True. It means the exp result gets better if the indicator increases.\n",
        "        # It is true by default.\n",
        "        self.increase_better = True\n",
        "        if metric_key is not None:\n",
        "            self.increase_better = False if metric_key[0] == \"-\" else True\n",
        "            self.metric_key = metric_key[1:] if metric_key[0] == \"+\" or metric_key[0] == \"-\" else metric_key\n",
        "        else:\n",
        "            self.metric_key = None\n",
        "        # prepare loss\n",
        "        losser = _prepare_losser(loss)\n",
        "\n",
        "        if isinstance(train_data, BatchIter):\n",
        "            if sampler is not None:\n",
        "                warnings.warn(\"sampler is ignored when train_data is a BatchIter.\")\n",
        "            if num_workers>0:\n",
        "                warnings.warn(\"num_workers is ignored when train_data is BatchIter.\")\n",
        "            if drop_last:\n",
        "                warnings.warn(\"drop_last is ignored when train_data is BatchIter.\")\n",
        "\n",
        "        if isinstance(model, nn.parallel.DistributedDataParallel):  # 如果是分布式的\n",
        "            # device为None\n",
        "            if device is not None:\n",
        "                warnings.warn(\"device is ignored when model is nn.parallel.DistributedDataParallel.\")\n",
        "                device = None\n",
        "            # Sampler要是分布式的\n",
        "            if sampler is None:\n",
        "                sampler = torch.utils.data.DistributedSampler(train_data)\n",
        "            elif not isinstance(sampler, torch.utils.data.DistributedSampler):\n",
        "                raise TypeError(\"When using nn.parallel.DistributedDataParallel, \"\n",
        "                                \"sampler must be None or torch.utils.data.DistributedSampler.\")\n",
        "            # 不能保存模型\n",
        "            if save_path:\n",
        "                raise RuntimeError(\"Saving model in Distributed situation is not allowed right now.\")\n",
        "        else:\n",
        "            # sampler check\n",
        "            if sampler is not None and not isinstance(sampler, (Sampler, torch.utils.data.Sampler)):\n",
        "                raise ValueError(f\"The type of sampler should be fastNLP.BaseSampler or pytorch's Sampler, got {type(sampler)}\")\n",
        "            if sampler is None:\n",
        "                sampler = RandomSampler()\n",
        "            elif hasattr(sampler, 'set_batch_size'):\n",
        "                sampler.set_batch_size(batch_size)\n",
        "\n",
        "        if isinstance(train_data, DataSet):\n",
        "            self.data_iterator = DataSetIter(\n",
        "                dataset=train_data, batch_size=batch_size, num_workers=num_workers, sampler=sampler, drop_last=drop_last)\n",
        "        elif isinstance(train_data, BatchIter):\n",
        "            self.data_iterator = train_data\n",
        "            train_data = train_data.dataset\n",
        "        else:\n",
        "            raise TypeError(\"train_data type {} not support\".format(type(train_data)))\n",
        "\n",
        "        model.train()\n",
        "        self.model = _move_model_to_device(model, device=device)\n",
        "        if _model_contains_inner_module(self.model):\n",
        "            self._forward_func = self.model.module.forward\n",
        "        else:\n",
        "            self._forward_func = self.model.forward\n",
        "        if check_code_level > -1:\n",
        "            # _check_code 是 fastNLP 帮助你检查代码是否正确的方法 。如果你在错误栈中看到这行注释，请认真检查你的field名与模型的输入\n",
        "            #   名是否匹配\n",
        "            dev_dataset = dev_data\n",
        "            if isinstance(dev_data, BatchIter):\n",
        "                dev_dataset = None\n",
        "                warnings.warn(\"dev_data is of BatchIter type, ignore validation checking.\")\n",
        "            check_batch_size = min(batch_size, 1)\n",
        "            if isinstance(self.model, nn.DataParallel):\n",
        "                _num_devices = len(self.model.device_ids)\n",
        "                if batch_size//_num_devices>1:  # 如果多卡是每个卡可以分多个数据的，则用每个卡给两个sample\n",
        "                    check_batch_size = max(len(self.model.device_ids)*2, check_batch_size)\n",
        "                else:\n",
        "                    check_batch_size = max(len(self.model.device_ids), check_batch_size)\n",
        "            _check_code(dataset=train_data, model=self.model, losser=losser, forward_func=self._forward_func, metrics=metrics,\n",
        "                        dev_data=dev_dataset, metric_key=self.metric_key, check_level=check_code_level,\n",
        "                        batch_size=check_batch_size)\n",
        "\n",
        "        self.train_data = train_data\n",
        "        self.dev_data = dev_data  # If None, No validation.\n",
        "        self.losser = losser\n",
        "        self.metrics = metrics\n",
        "        self.n_epochs = int(n_epochs)\n",
        "        self.batch_size = int(batch_size)\n",
        "        self.save_path = save_path\n",
        "        self.print_every = int(print_every)\n",
        "        self.validate_every = int(validate_every) if validate_every != 0 else -1\n",
        "        self.best_metric_indicator = None\n",
        "        self.best_dev_epoch = None\n",
        "        self.best_dev_step = None\n",
        "        self.best_dev_perf = None\n",
        "        self.n_steps = len(self.data_iterator) * self.n_epochs\n",
        "\n",
        "        if isinstance(optimizer, torch.optim.Optimizer):\n",
        "            self.optimizer = optimizer\n",
        "        elif isinstance(optimizer, Optimizer):\n",
        "            self.optimizer = optimizer.construct_from_pytorch(self.model.parameters())\n",
        "        elif optimizer is None:\n",
        "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=4e-3)\n",
        "        else:\n",
        "            raise TypeError(\"optimizer can only be torch.optim.Optimizer type, not {}.\".format(type(optimizer)))\n",
        "\n",
        "        self.logger = logger\n",
        "\n",
        "        self.use_tqdm = use_tqdm\n",
        "        if 'test_use_tqdm' in kwargs:\n",
        "            self.test_use_tqdm = kwargs.get('test_use_tqdm')\n",
        "        else:\n",
        "            self.test_use_tqdm = self.use_tqdm\n",
        "        self.pbar = None\n",
        "        self.print_every = abs(self.print_every)\n",
        "        self.kwargs = kwargs\n",
        "        if self.dev_data is not None:\n",
        "            self.tester = Tester(model=self.model,\n",
        "                                 data=self.dev_data,\n",
        "                                 metrics=self.metrics,\n",
        "                                 batch_size=kwargs.get(\"dev_batch_size\", self.batch_size),\n",
        "                                 device=None,  # 由上面的部分处理device\n",
        "                                 verbose=0,\n",
        "                                 use_tqdm=self.test_use_tqdm)\n",
        "\n",
        "        self.step = 0\n",
        "        self.start_time = None  # start timestamp\n",
        "\n",
        "        if isinstance(callbacks, Callback):\n",
        "            callbacks = [callbacks]\n",
        "\n",
        "        self.callback_manager = CallbackManager(env={\"trainer\": self},\n",
        "                                                callbacks=callbacks)\n",
        "\n",
        "\n",
        "    def train(self, load_best_model=True, on_exception='auto'):\n",
        "        \"\"\"\n",
        "        使用该函数使Trainer开始训练。\n",
        "\n",
        "        :param bool load_best_model: 该参数只有在初始化提供了dev_data的情况下有效，如果True, trainer将在返回之前重新加载dev表现\n",
        "                最好的模型参数。\n",
        "        :param str on_exception: 在训练过程遭遇exception，并被 :py:class:Callback 的on_exception()处理后，是否继续抛出异常。\n",
        "                支持'ignore','raise', 'auto': 'ignore'将捕获异常，写在Trainer.train()后面的代码将继续运行; 'raise'将异常抛出;\n",
        "                'auto'将ignore以下两种Exception: CallbackException与KeyboardInterrupt, raise其它exception.\n",
        "        :return dict: 返回一个字典类型的数据,\n",
        "                内含以下内容::\n",
        "\n",
        "                    seconds: float, 表示训练时长\n",
        "                    以下三个内容只有在提供了dev_data的情况下会有。\n",
        "                    best_eval: Dict of Dict, 表示evaluation的结果。第一层的key为Metric的名称，\n",
        "                                第二层的key为具体的Metric\n",
        "                    best_epoch: int，在第几个epoch取得的最佳值\n",
        "                    best_step: int, 在第几个step(batch)更新取得的最佳值\n",
        "\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        if self.n_epochs <= 0:\n",
        "            self.logger.info(f\"training epoch is {self.n_epochs}, nothing was done.\")\n",
        "            results['seconds'] = 0.\n",
        "            return results\n",
        "        try:\n",
        "            self._model_device = _get_model_device(self.model)\n",
        "            self._mode(self.model, is_test=False)\n",
        "            self._load_best_model = load_best_model\n",
        "            self.start_time = str(datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))\n",
        "            start_time = time.time()\n",
        "            self.logger.info(\"training epochs started \" + self.start_time)\n",
        "\n",
        "            try:\n",
        "                self.callback_manager.on_train_begin()\n",
        "                self._train()\n",
        "                self.callback_manager.on_train_end()\n",
        "\n",
        "            except BaseException as e:\n",
        "                self.callback_manager.on_exception(e)\n",
        "                if on_exception == 'auto':\n",
        "                    if not isinstance(e, (CallbackException, KeyboardInterrupt)):\n",
        "                        raise e\n",
        "                elif on_exception == 'raise':\n",
        "                    raise e\n",
        "\n",
        "            if self.dev_data is not None and self.best_dev_perf is not None:\n",
        "                self.logger.info(\n",
        "                    \"\\nIn Epoch:{}/Step:{}, got best dev performance:\".format(self.best_dev_epoch, self.best_dev_step))\n",
        "                self.logger.info(self.tester._format_eval_results(self.best_dev_perf))\n",
        "                results['best_eval'] = self.best_dev_perf\n",
        "                results['best_epoch'] = self.best_dev_epoch\n",
        "                results['best_step'] = self.best_dev_step\n",
        "                if load_best_model:\n",
        "                    model_name = \"best_\" + \"_\".join([self.model.__class__.__name__, self.metric_key, self.start_time])\n",
        "                    load_succeed = self._load_model(self.model, model_name)\n",
        "                    if load_succeed:\n",
        "                        self.logger.info(\"Reloaded the best model.\")\n",
        "                    else:\n",
        "                        self.logger.info(\"Fail to reload best model.\")\n",
        "        finally:\n",
        "            pass\n",
        "        results['seconds'] = round(time.time() - start_time, 2)\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    def _train(self):\n",
        "        if not self.use_tqdm:\n",
        "            from fastNLP.core.utils import _pseudo_tqdm as inner_tqdm\n",
        "        else:\n",
        "            inner_tqdm = tqdm\n",
        "        self.step = 0\n",
        "        self.epoch = 0\n",
        "        start = time.time()\n",
        "        with inner_tqdm(total=self.n_steps, postfix='loss:{0:<6.5f}', leave=False, dynamic_ncols=True) as pbar:\n",
        "            self.pbar = pbar\n",
        "            avg_loss = 0\n",
        "            data_iterator = self.data_iterator\n",
        "            self.batch_per_epoch = data_iterator.num_batches\n",
        "            for epoch in range(1, self.n_epochs + 1):\n",
        "                self.epoch = epoch\n",
        "                pbar.set_description_str(desc=\"Epoch {}/{}\".format(epoch, self.n_epochs))\n",
        "                # early stopping\n",
        "                self.callback_manager.on_epoch_begin()\n",
        "                for batch_x, batch_y in data_iterator:\n",
        "                    self.step += 1\n",
        "                    _move_dict_value_to_device(batch_x, batch_y, device=self._model_device)\n",
        "                    indices = data_iterator.get_batch_indices()\n",
        "                    # negative sampling; replace unknown; re-weight batch_y\n",
        "                    self.callback_manager.on_batch_begin(batch_x, batch_y, indices)\n",
        "                    prediction = self._data_forward(self.model, batch_x)\n",
        "\n",
        "                    # edit prediction\n",
        "                    self.callback_manager.on_loss_begin(batch_y, prediction)\n",
        "                    loss = self._compute_loss(prediction, batch_y).mean()\n",
        "                    avg_loss += loss.item()\n",
        "                    loss = loss / self.update_every\n",
        "\n",
        "                    # Is loss NaN or inf? requires_grad = False\n",
        "                    self.callback_manager.on_backward_begin(loss)\n",
        "                    self._grad_backward(loss)\n",
        "                    self.callback_manager.on_backward_end()\n",
        "\n",
        "                    self._update()\n",
        "                    self.callback_manager.on_step_end()\n",
        "\n",
        "                    if self.step % self.print_every == 0:\n",
        "                        avg_loss = float(avg_loss) / self.print_every\n",
        "                        if self.use_tqdm:\n",
        "                            print_output = \"loss:{:<6.5f}\".format(avg_loss)\n",
        "                            pbar.update(self.print_every)\n",
        "                        else:\n",
        "                            end = time.time()\n",
        "                            diff = timedelta(seconds=round(end - start))\n",
        "                            print_output = \"[epoch: {:>3} step: {:>4}] train loss: {:>4.6} time: {}\".format(\n",
        "                                epoch, self.step, avg_loss, diff)\n",
        "                        pbar.set_postfix_str(print_output)\n",
        "                        avg_loss = 0\n",
        "                    self.callback_manager.on_batch_end()\n",
        "\n",
        "                    if ((self.validate_every > 0 and self.step % self.validate_every == 0) or\n",
        "                        (self.validate_every < 0 and self.step % len(data_iterator) == 0)) \\\n",
        "                            and self.dev_data is not None:\n",
        "                        eval_res = self._do_validation(epoch=epoch, step=self.step)\n",
        "                        eval_str = \"Evaluation on dev at Epoch {}/{}. Step:{}/{}: \".format(epoch, self.n_epochs, self.step,\n",
        "                                                                                    self.n_steps)\n",
        "                        # pbar.write(eval_str + '\\n')\n",
        "                        self.logger.info(eval_str)\n",
        "                        self.logger.info(self.tester._format_eval_results(eval_res)+'\\n')\n",
        "                # ================= mini-batch end ==================== #\n",
        "\n",
        "                # lr decay; early stopping\n",
        "                self.callback_manager.on_epoch_end()\n",
        "            # =============== epochs end =================== #\n",
        "            pbar.close()\n",
        "            self.pbar = None\n",
        "        # ============ tqdm end ============== #\n",
        "\n",
        "    def _do_validation(self, epoch, step):\n",
        "        self.callback_manager.on_valid_begin()\n",
        "        res = self.tester.test()\n",
        "\n",
        "        is_better_eval = False\n",
        "        if self._better_eval_result(res):\n",
        "            if self.save_path is not None:\n",
        "                self._save_model(self.model,\n",
        "                                 \"best_\" + \"_\".join([self.model.__class__.__name__, self.metric_key, self.start_time]))\n",
        "            elif self._load_best_model:\n",
        "                self._best_model_states = {name: param.cpu().clone() for name, param in self.model.state_dict().items()}\n",
        "            self.best_dev_perf = res\n",
        "            self.best_dev_epoch = epoch\n",
        "            self.best_dev_step = step\n",
        "            is_better_eval = True\n",
        "        # get validation results; adjust optimizer\n",
        "        self.callback_manager.on_valid_end(res, self.metric_key, self.optimizer, is_better_eval)\n",
        "        return res\n",
        "\n",
        "    def _mode(self, model, is_test=False):\n",
        "        \"\"\"Train mode or Test mode. This is for PyTorch currently.\n",
        "\n",
        "        :param model: a PyTorch model\n",
        "        :param bool is_test: whether in test mode or not.\n",
        "\n",
        "        \"\"\"\n",
        "        if is_test:\n",
        "            model.eval()\n",
        "        else:\n",
        "            model.train()\n",
        "\n",
        "    def _update(self):\n",
        "        \"\"\"Perform weight update on a model.\n",
        "\n",
        "        \"\"\"\n",
        "        if self.step % self.update_every == 0:\n",
        "            self.optimizer.step()\n",
        "\n",
        "    def _data_forward(self, network, x):\n",
        "        x = _build_args(self._forward_func, **x)\n",
        "        y = network(**x)\n",
        "        if not isinstance(y, dict):\n",
        "            raise TypeError(\n",
        "                f\"The return value of {_get_func_signature(self._forward_func)} should be dict, got {type(y)}.\")\n",
        "        return y\n",
        "\n",
        "    def _grad_backward(self, loss):\n",
        "        \"\"\"Compute gradient with link rules.\n",
        "\n",
        "        :param loss: a scalar where back-prop starts\n",
        "\n",
        "        For PyTorch, just do \"loss.backward()\"\n",
        "        \"\"\"\n",
        "        if (self.step-1) % self.update_every == 0:\n",
        "            self.model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "    def _compute_loss(self, predict, truth):\n",
        "        \"\"\"Compute loss given prediction and ground truth.\n",
        "\n",
        "        :param predict: prediction dict, produced by model.forward\n",
        "        :param truth: ground truth dict, produced by batch_y\n",
        "        :return: a scalar\n",
        "        \"\"\"\n",
        "        return self.losser(predict, truth)\n",
        "\n",
        "    def _save_model(self, model, model_name, only_param=False):\n",
        "        \"\"\" 存储不含有显卡信息的state_dict或model\n",
        "        :param model:\n",
        "        :param model_name:\n",
        "        :param only_param:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if self.save_path is not None:\n",
        "            model_path = os.path.join(self.save_path, model_name)\n",
        "            if not os.path.exists(self.save_path):\n",
        "                os.makedirs(self.save_path, exist_ok=True)\n",
        "            if _model_contains_inner_module(model):\n",
        "                model = model.module\n",
        "            if only_param:\n",
        "                state_dict = model.state_dict()\n",
        "                for key in state_dict:\n",
        "                    state_dict[key] = state_dict[key].cpu()\n",
        "                torch.save(state_dict, model_path)\n",
        "            else:\n",
        "                model.cpu()\n",
        "                torch.save(model, model_path)\n",
        "                model.to(self._model_device)\n",
        "\n",
        "    def _load_model(self, model, model_name, only_param=False):\n",
        "        # 返回bool值指示是否成功reload模型\n",
        "        if self.save_path is not None:\n",
        "            model_path = os.path.join(self.save_path, model_name)\n",
        "            if only_param:\n",
        "                states = torch.load(model_path)\n",
        "            else:\n",
        "                states = torch.load(model_path).state_dict()\n",
        "            if _model_contains_inner_module(model):\n",
        "                model.module.load_state_dict(states)\n",
        "            else:\n",
        "                model.load_state_dict(states)\n",
        "        elif hasattr(self, \"_best_model_states\"):\n",
        "            model.load_state_dict(self._best_model_states)\n",
        "        else:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def _better_eval_result(self, metrics):\n",
        "        \"\"\"Check if the current epoch yields better validation results.\n",
        "\n",
        "        :return bool value: True means current results on dev set is the best.\n",
        "        \"\"\"\n",
        "        indicator, indicator_val = _check_eval_results(metrics, self.metric_key, self.metrics)\n",
        "        if self.metric_key is None:\n",
        "            self.metric_key = indicator\n",
        "        is_better = True\n",
        "        if self.best_metric_indicator is None:\n",
        "            # first-time validation\n",
        "            self.best_metric_indicator = indicator_val\n",
        "        else:\n",
        "            if self.increase_better is True:\n",
        "                if indicator_val > self.best_metric_indicator:\n",
        "                    self.best_metric_indicator = indicator_val\n",
        "                else:\n",
        "                    is_better = False\n",
        "            else:\n",
        "                if indicator_val < self.best_metric_indicator:\n",
        "                    self.best_metric_indicator = indicator_val\n",
        "                else:\n",
        "                    is_better = False\n",
        "        return is_better\n",
        "\n",
        "    @property\n",
        "    def is_master(self):\n",
        "        \"\"\"是否是主进程\"\"\"\n",
        "        return True"
      ],
      "metadata": {
        "id": "zf5kLJf-6q0a"
      },
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Модель TENER - такая же как и на гит https://github.com/fastnlp/TENER, но измененная для работы с моим классом embed\n",
        "\n",
        "class TENER(nn.Module):\n",
        "    def __init__(self, tag_vocab, embed, num_layers, d_model, n_head, feedforward_dim, dropout, encoding_type,\n",
        "                 after_norm=True, attn_type='adatrans',  bi_embed=None,\n",
        "                 fc_dropout=0.3, pos_embed=None, scale=False, dropout_attn=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param tag_vocab: fastNLP Vocabulary\n",
        "        :param embed: fastNLP TokenEmbedding\n",
        "        :param num_layers: number of self-attention layers\n",
        "        :param d_model: input size\n",
        "        :param n_head: number of head\n",
        "        :param feedforward_dim: the dimension of ffn\n",
        "        :param dropout: dropout in self-attention\n",
        "        :param after_norm: normalization place\n",
        "        :param attn_type: adatrans, naive\n",
        "        :param rel_pos_embed: position embedding\n",
        "        :param bi_embed: Used in Chinese scenerio\n",
        "        :param fc_dropout: dropout rate before the fc layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = embed\n",
        "        embed_size = self.embed.embed_size\n",
        "        self.bi_embed = None\n",
        "        if bi_embed is not None:\n",
        "            self.bi_embed = bi_embed\n",
        "            embed_size += self.bi_embed.embed_size\n",
        "\n",
        "        self.in_fc = nn.Linear(embed_size, d_model)\n",
        "\n",
        "        self.transformer = TransformerEncoder(num_layers, d_model, n_head, feedforward_dim, dropout,\n",
        "                                              after_norm=after_norm, attn_type=attn_type,\n",
        "                                              scale=scale, dropout_attn=dropout_attn,\n",
        "                                              pos_embed=pos_embed)\n",
        "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
        "        self.out_fc = nn.Linear(d_model, len(tag_vocab))\n",
        "        \n",
        "        \n",
        "        self.crf = ConditionalRandomField(len(tag_vocab), include_start_end_trans=True, allowed_transitions=None)\n",
        "        \n",
        "    def _forward(self, chars, target, bigrams=None):\n",
        "  \n",
        "        # chars - Navec index\n",
        "        mask = chars.ne(0)\n",
        "        \n",
        "        chars = self.embed.forward(chars)\n",
        "        if self.bi_embed is not None:\n",
        "            bigrams = self.bi_embed(bigrams)\n",
        "            chars = torch.cat([chars, bigrams], dim=-1)\n",
        "        \n",
        "        chars = self.in_fc(chars)\n",
        "        \n",
        "        chars = self.transformer(chars, mask)\n",
        "        chars = self.fc_dropout(chars)\n",
        "        chars = self.out_fc(chars)\n",
        "        logits = F.log_softmax(chars, dim=-1)\n",
        "        if target is None:\n",
        "            paths, _ = self.crf.viterbi_decode(logits, mask)\n",
        "            return {'pred': paths}\n",
        "        else:\n",
        "            loss = self.crf(logits, target, mask)\n",
        "            return {'loss': loss}\n",
        "\n",
        "    def forward(self, chars, target, bigrams=None):\n",
        "        return self._forward(chars, target, bigrams)\n",
        "\n",
        "    def predict(self, chars, bigrams=None):\n",
        "        \n",
        "        return self._forward(chars, target=None, bigrams=bigrams)"
      ],
      "metadata": {
        "id": "4sG1v5tnvw38"
      },
      "execution_count": 386,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Определение параметров обучения и модели\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = None\n",
        "\n",
        "normalize_embed = True\n",
        "pos_embed = None\n",
        "n_heads = 14 \n",
        "head_dims = 128\n",
        "num_layers = 2\n",
        "lr = 0.000035\n",
        "attn_type = 'adatrans'\n",
        "char_type = 'cnn'\n",
        "\n",
        "d_model = n_heads * head_dims \n",
        "dim_feedforward = int(2 * d_model)\n",
        "\n",
        "batch_size = 2\n",
        "warmup_steps = 0.01\n",
        "dropout=0.15\n",
        "fc_dropout=0.4\n",
        "after_norm = 1\n",
        "encoding_type = 'bioes'\n",
        "\n",
        "save_path = '/content/'"
      ],
      "metadata": {
        "id": "93oOsV2JVgJu"
      },
      "execution_count": 387,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = TENER(tag_vocab=tar_for_embed, embed=embedding, num_layers=num_layers,\n",
        "              d_model=d_model, n_head=n_heads,\n",
        "              feedforward_dim=dim_feedforward, dropout=dropout,\n",
        "              encoding_type=encoding_type,\n",
        "              after_norm=after_norm, attn_type=attn_type,\n",
        "              bi_embed=None, fc_dropout=fc_dropout,\n",
        "              pos_embed=pos_embed, scale=attn_type=='transformer')"
      ],
      "metadata": {
        "id": "JTAz4ifyROuP"
      },
      "execution_count": 388,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Копия датасета для тренировки для оценки обучения\n",
        "import copy\n",
        "DT = copy.copy(dt_train)\n",
        "\n",
        "DT.set_target('target')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olaacoV3TdUM",
        "outputId": "ae2b60b9-1704-4660-9fde-867524b1720c"
      },
      "execution_count": 389,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------------+---------+--------------------+--------------------+--------------------+\n",
              "| sentence       | seq_len | flag               | chars              | target             |\n",
              "+----------------+---------+--------------------+--------------------+--------------------+\n",
              "| ['государ...   | 2       | [True, True]       | [94791, 345483]... | [0, 0]             |\n",
              "| ['республ...   | 5       | [True, True, Fa... | [376103, 176303... | [0, 0, 8, 0, 3]... |\n",
              "| ['правопо...   | 4       | [False, True, T... | [329422, 51199,... | [8, 0, 0, 0]       |\n",
              "| ['паспорт...   | 1       | [True]             | [288176]           | [0]                |\n",
              "| ['государ...   | 5       | [True, True, Tr... | [94803, 345536,... | [0, 0, 0, 0, 8]... |\n",
              "| ['и', 'обес... | 6       | [True, False, F... | [146473, 254239... | [0, 3, 8, 0, 0,... |\n",
              "| ['далее', ...  | 2       | [True, True]       | [100318, 345483... | [0, 0]             |\n",
              "| ['паспорт...   | 2       | [True, True]       | [288176, 310038... | [0, 0]             |\n",
              "| ['правова...   | 5       | [False, False, ... | [329316, 142120... | [8, 8, 0, 0, 0]... |\n",
              "| ['далее', ...  | 3       | [True, True, Tr... | [100318, 310036... | [0, 0, 0]          |\n",
              "| ['паспорт...   | 2       | [True, True]       | [288176, 310038... | [0, 0]             |\n",
              "| ['правопо...   | 1       | [False]            | [329425]           | [8]                |\n",
              "| ...            | ...     | ...                | ...                | ...                |\n",
              "+----------------+---------+--------------------+--------------------+--------------------+"
            ]
          },
          "metadata": {},
          "execution_count": 389
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация Trainer\n",
        "optimizer = optim.SGD(model1.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "callbacks = []\n",
        "clip_callback = GradientClipCallback(clip_type='value', clip_value=5)\n",
        "evaluate_callback = EvaluateCallback(data=DT) # Оценка train будет происходить на вкладке evaluation on test_data\n",
        "\n",
        "if warmup_steps>0:\n",
        "    warmup_callback = WarmupCallback(warmup_steps, schedule='linear')\n",
        "    callbacks.append(warmup_callback)\n",
        "callbacks.extend([clip_callback, evaluate_callback])\n",
        "\n",
        "trainer = Trainer(dt_train, model1, optimizer, batch_size=30, sampler=BucketSampler(),\n",
        "                  num_workers=2, n_epochs=100, dev_data=dt_test,\n",
        "                  metrics=SpanFPreRecMetric(tag_vocab=tar_for_embed, encoding_type=encoding_type),\n",
        "                  dev_batch_size=30, callbacks=callbacks, device=device, test_use_tqdm=False,\n",
        "                  use_tqdm=True, print_every=300, save_path=save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX6IC55Fcuh3",
        "outputId": "e7fb7387-e50f-4278-af7d-eb558b41cf01"
      },
      "execution_count": 390,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input fields after batch(if batch size is 1):\n",
            "\tchars: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([1, 2]) \n",
            "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([1, 2]) \n",
            "target fields after batch(if batch size is 1):\n",
            "\tseq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([1]) \n",
            "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([1, 2]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(load_best_model=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a37f120dc4ce480399c6c04202d6acf6",
            "e2cd5a223a0b4b0bbbdd3ba76f643ed2",
            "e162120166374d28ab8808ea3a75ad1b",
            "6c7280e027e348ec9727d76d629ef43a",
            "fed4afbbffc74bce94e00ddcc483446d",
            "90025f5b83b14569a9b1862450350454",
            "87373633c413440c90f5e9d004d2abf6",
            "7afa0a112f194cc0a090f8f5199572c4",
            "eebe9db4d4464db2a301d87aab00bed7",
            "96eddac0af2c4684831008518fe0f0bc",
            "e9da5f01e8ff411dbd4a34c718b25003"
          ]
        },
        "id": "ZLyFUaCkseKl",
        "outputId": "a420fc15-5cbb-417c-d199-681a9b5e01c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training epochs started 2023-01-22-17-49-40\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1100 [00:00<?, ?it/s, loss:{0:<6.5f}]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a37f120dc4ce480399c6c04202d6acf6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate data in 32.63 seconds!\n",
            "Evaluate data in 7.77 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 1/100. Step:11/1100: \n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "\n",
            "Evaluate data in 32.39 seconds!\n",
            "Evaluate data in 7.73 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 2/100. Step:22/1100: \n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "\n",
            "Evaluate data in 32.57 seconds!\n",
            "Evaluate data in 7.81 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 3/100. Step:33/1100: \n",
            "SpanFPreRecMetric: f=0.069465, pre=0.347826, rec=0.038585\n",
            "\n",
            "Evaluate data in 32.32 seconds!\n",
            "Evaluate data in 10.18 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 4/100. Step:44/1100: \n",
            "SpanFPreRecMetric: f=0.023548, pre=0.5, rec=0.012058\n",
            "\n",
            "Evaluate data in 32.64 seconds!\n",
            "Evaluate data in 7.79 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 5/100. Step:55/1100: \n",
            "SpanFPreRecMetric: f=0.098715, pre=0.310638, rec=0.058682\n",
            "\n",
            "Evaluate data in 32.21 seconds!\n",
            "Evaluate data in 7.66 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 6/100. Step:66/1100: \n",
            "SpanFPreRecMetric: f=0.077827, pre=0.449153, rec=0.042605\n",
            "\n",
            "Evaluate data in 32.62 seconds!\n",
            "Evaluate data in 7.85 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 7/100. Step:77/1100: \n",
            "SpanFPreRecMetric: f=0.112045, pre=0.434783, rec=0.064309\n",
            "\n",
            "Evaluate data in 32.54 seconds!\n",
            "Evaluate data in 7.72 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 8/100. Step:88/1100: \n",
            "SpanFPreRecMetric: f=0.137745, pre=0.43038, rec=0.081994\n",
            "\n",
            "Evaluate data in 32.76 seconds!\n",
            "Evaluate data in 7.95 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 9/100. Step:99/1100: \n",
            "SpanFPreRecMetric: f=0.114407, pre=0.47093, rec=0.065113\n",
            "\n",
            "Evaluate data in 32.75 seconds!\n",
            "Evaluate data in 7.81 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 10/100. Step:110/1100: \n",
            "SpanFPreRecMetric: f=0.120842, pre=0.388646, rec=0.071543\n",
            "\n",
            "Evaluate data in 32.75 seconds!\n",
            "Evaluate data in 7.84 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 11/100. Step:121/1100: \n",
            "SpanFPreRecMetric: f=0.151093, pre=0.430189, rec=0.09164\n",
            "\n",
            "Evaluate data in 32.39 seconds!\n",
            "Evaluate data in 7.77 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 12/100. Step:132/1100: \n",
            "SpanFPreRecMetric: f=0.198867, pre=0.457971, rec=0.12701\n",
            "\n",
            "Evaluate data in 32.29 seconds!\n",
            "Evaluate data in 7.75 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 13/100. Step:143/1100: \n",
            "SpanFPreRecMetric: f=0.212874, pre=0.513846, rec=0.134244\n",
            "\n",
            "Evaluate data in 32.64 seconds!\n",
            "Evaluate data in 7.82 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 14/100. Step:154/1100: \n",
            "SpanFPreRecMetric: f=0.207582, pre=0.457534, rec=0.134244\n",
            "\n",
            "Evaluate data in 32.47 seconds!\n",
            "Evaluate data in 7.76 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 15/100. Step:165/1100: \n",
            "SpanFPreRecMetric: f=0.210928, pre=0.50303, rec=0.133441\n",
            "\n",
            "Evaluate data in 32.55 seconds!\n",
            "Evaluate data in 7.91 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 16/100. Step:176/1100: \n",
            "SpanFPreRecMetric: f=0.207325, pre=0.455041, rec=0.134244\n",
            "\n",
            "Evaluate data in 32.42 seconds!\n",
            "Evaluate data in 7.79 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 17/100. Step:187/1100: \n",
            "SpanFPreRecMetric: f=0.223733, pre=0.483957, rec=0.145498\n",
            "\n",
            "Evaluate data in 32.52 seconds!\n",
            "Evaluate data in 7.84 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 18/100. Step:198/1100: \n",
            "SpanFPreRecMetric: f=0.233454, pre=0.464115, rec=0.155949\n",
            "\n",
            "Evaluate data in 32.69 seconds!\n",
            "Evaluate data in 7.73 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 19/100. Step:209/1100: \n",
            "SpanFPreRecMetric: f=0.241984, pre=0.488998, rec=0.160772\n",
            "\n",
            "Evaluate data in 32.51 seconds!\n",
            "Evaluate data in 7.86 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 20/100. Step:220/1100: \n",
            "SpanFPreRecMetric: f=0.250295, pre=0.471111, rec=0.170418\n",
            "\n",
            "Evaluate data in 32.55 seconds!\n",
            "Evaluate data in 7.86 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 21/100. Step:231/1100: \n",
            "SpanFPreRecMetric: f=0.255841, pre=0.467949, rec=0.176045\n",
            "\n",
            "Evaluate data in 32.48 seconds!\n",
            "Evaluate data in 7.88 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 22/100. Step:242/1100: \n",
            "SpanFPreRecMetric: f=0.250298, pre=0.483871, rec=0.16881\n",
            "\n",
            "Evaluate data in 32.49 seconds!\n",
            "Evaluate data in 7.88 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 23/100. Step:253/1100: \n",
            "SpanFPreRecMetric: f=0.284275, pre=0.467772, rec=0.20418\n",
            "\n",
            "Evaluate data in 32.58 seconds!\n",
            "Evaluate data in 7.82 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 24/100. Step:264/1100: \n",
            "SpanFPreRecMetric: f=0.232132, pre=0.483461, rec=0.152733\n",
            "\n",
            "Evaluate data in 32.43 seconds!\n",
            "Evaluate data in 7.8 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 25/100. Step:275/1100: \n",
            "SpanFPreRecMetric: f=0.277316, pre=0.447227, rec=0.200965\n",
            "\n",
            "Evaluate data in 32.64 seconds!\n",
            "Evaluate data in 7.91 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 26/100. Step:286/1100: \n",
            "SpanFPreRecMetric: f=0.292901, pre=0.480734, rec=0.210611\n",
            "\n",
            "Evaluate data in 32.45 seconds!\n",
            "Evaluate data in 7.75 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 27/100. Step:297/1100: \n",
            "SpanFPreRecMetric: f=0.280285, pre=0.536364, rec=0.189711\n",
            "\n",
            "Evaluate data in 32.6 seconds!\n",
            "Evaluate data in 7.82 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 28/100. Step:308/1100: \n",
            "SpanFPreRecMetric: f=0.303433, pre=0.487544, rec=0.220257\n",
            "\n",
            "Evaluate data in 32.68 seconds!\n",
            "Evaluate data in 7.85 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 29/100. Step:319/1100: \n",
            "SpanFPreRecMetric: f=0.274783, pre=0.492723, rec=0.190514\n",
            "\n",
            "Evaluate data in 32.67 seconds!\n",
            "Evaluate data in 7.87 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 30/100. Step:330/1100: \n",
            "SpanFPreRecMetric: f=0.285052, pre=0.510373, rec=0.197749\n",
            "\n",
            "Evaluate data in 32.34 seconds!\n",
            "Evaluate data in 7.83 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 31/100. Step:341/1100: \n",
            "SpanFPreRecMetric: f=0.313921, pre=0.506261, rec=0.227492\n",
            "\n",
            "Evaluate data in 32.5 seconds!\n",
            "Evaluate data in 7.83 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 32/100. Step:352/1100: \n",
            "SpanFPreRecMetric: f=0.30363, pre=0.480836, rec=0.221865\n",
            "\n",
            "Evaluate data in 32.46 seconds!\n",
            "Evaluate data in 7.78 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 33/100. Step:363/1100: \n",
            "SpanFPreRecMetric: f=0.282421, pre=0.498982, rec=0.196945\n",
            "\n",
            "Evaluate data in 32.86 seconds!\n",
            "Evaluate data in 7.8 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 34/100. Step:374/1100: \n",
            "SpanFPreRecMetric: f=0.32361, pre=0.513089, rec=0.236334\n",
            "\n",
            "Evaluate data in 32.74 seconds!\n",
            "Evaluate data in 7.96 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 35/100. Step:385/1100: \n",
            "SpanFPreRecMetric: f=0.345584, pre=0.544041, rec=0.253215\n",
            "\n",
            "Evaluate data in 32.56 seconds!\n",
            "Evaluate data in 7.86 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 36/100. Step:396/1100: \n",
            "SpanFPreRecMetric: f=0.314969, pre=0.511754, rec=0.227492\n",
            "\n",
            "Evaluate data in 32.43 seconds!\n",
            "Evaluate data in 7.82 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 37/100. Step:407/1100: \n",
            "SpanFPreRecMetric: f=0.323625, pre=0.491803, rec=0.241158\n",
            "\n",
            "Evaluate data in 32.3 seconds!\n",
            "Evaluate data in 7.76 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 38/100. Step:418/1100: \n",
            "SpanFPreRecMetric: f=0.308292, pre=0.500904, rec=0.222669\n",
            "\n",
            "Evaluate data in 32.43 seconds!\n",
            "Evaluate data in 7.79 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 39/100. Step:429/1100: \n",
            "SpanFPreRecMetric: f=0.328423, pre=0.511036, rec=0.241961\n",
            "\n",
            "Evaluate data in 34.45 seconds!\n",
            "Evaluate data in 7.87 seconds!\n",
            "EvaluateCallback evaluation on data-test:\n",
            "SpanFPreRecMetric: f=0.0, pre=0.0, rec=0.0\n",
            "Evaluation on dev at Epoch 40/100. Step:440/1100: \n",
            "SpanFPreRecMetric: f=0.317787, pre=0.488333, rec=0.235531\n",
            "\n",
            "\n",
            "In Epoch:35/Step:385, got best dev performance:\n",
            "SpanFPreRecMetric: f=0.345584, pre=0.544041, rec=0.253215\n",
            "Reloaded the best model.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_eval': {'SpanFPreRecMetric': {'f': 0.345584,\n",
              "   'pre': 0.544041,\n",
              "   'rec': 0.253215}},\n",
              " 'best_epoch': 35,\n",
              " 'best_step': 385,\n",
              " 'seconds': 3926.11}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Для проверки модели\n",
        "for ch, y in DataSetIter(dt_train, batch_size=2):\n",
        "  print(ch['chars'])\n",
        "  mask = ch['chars'].ne(0)\n",
        "  print(mask)\n",
        "  out = model1.embed.forward(ch['chars'])\n",
        "  print(out.shape)\n",
        "  out = model1.in_fc(out)\n",
        "  print(out.shape)\n",
        "  out = model1.transformer(out, mask)\n",
        "  print(out.shape)\n",
        "  out = model1.fc_dropout(out)\n",
        "  print(out.shape)\n",
        "  out = model1.out_fc(out)\n",
        "  print(out.shape)\n",
        "  out = F.log_softmax(out, dim=-1)\n",
        "  print(out.shape)\n",
        "  paths, _ = model1.crf.viterbi_decode(out, mask)\n",
        "  print(paths)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB-_SD1ASxR4",
        "outputId": "f42f189e-3937-4d2f-c290-9a858c58f1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 94791, 345483, 500001, 500001, 500001, 500001],\n",
            "        [376103, 176303, 500000, 497635, 146473, 254239]])\n",
            "tensor([[True, True, True, True, True, True],\n",
            "        [True, True, True, True, True, True]])\n",
            "torch.Size([2, 6, 330])\n",
            "torch.Size([2, 6, 1792])\n",
            "torch.Size([2, 6, 1792])\n",
            "torch.Size([2, 6, 1792])\n",
            "torch.Size([2, 6, 9])\n",
            "torch.Size([2, 6, 9])\n",
            "tensor([[0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 8, 0, 3]])\n"
          ]
        }
      ]
    }
  ]
}