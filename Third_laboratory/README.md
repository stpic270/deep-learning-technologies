Задача NER от RuREBus [1]
В данной работе были реализованы модели TENER [2] и sbert [3] 

Введение
Распознавание именованных сущностей (NER) — иногда называемое фрагментированием, извлечением или идентификацией сущностей — представляет собой задачу идентификации и категоризации ключевой информации (сущностей) в тексте. Объектом может быть любое слово или ряд слов, которые постоянно относятся к одному и тому же объекту. Каждый обнаруженный объект классифицируется по заранее определенной категории. Например, модель машинного обучения NER может обнаружить слово «super.AI» в тексте и классифицировать его как «Company».

В основе любой модели NER лежит двухэтапный процесс:

1) Обнаружить именованный объект
2) Классифицировать сущность
За этим скрывается несколько вещей. Шаг первый включает в себя обнаружение слова или цепочки слов, образующих сущность. Каждое слово представляет собой токен: «The Great Lakes» — это строка из трех токенов, представляющая одну сущность. Пометка «Inside-outside-beginning» — это распространенный способ указать, где объекты начинаются и заканчиваются.
Второй шаг требует создания категорий сущностей. Вот некоторые распространенные категории сущностей:

Person - Элвис Пресли, Одри Хепберн, Дэвид Бекхэм.
Organization - Google, Mastercard, Оксфордский университет.
Time - 2006, 16:34, 2 часа ночи
Location - Трафальгарская площадь, МоМА, Мачу-Пикчу.
Work of art - «Гамлет», «Герника», «Изгнание на Мейн-стрит».

Алгоритм TENER:

в большинстве моделей NER используется CNN в качестве encoder для символов (рис. 1) В TENER encoder используется не только для извлечения контекстной информации в виде words embedding, но и для кодирования информации на уровне символов в слове, то есть chars embedding

![image](https://user-images.githubusercontent.com/58371161/214795070-7b849570-5f90-4ede-b04c-cefc0c320c4a.png)

(рис. 1) - Структура модели TENER для задачи NER [2]

По сравнению с кодировщиком символов на основе BiLSTM [4] CNN более эффективен, его потенциальное преимущество заключается в извлечении различных n-gram и прерывистых шаблонов символов,  например «un..ily» в “unhappily” и “uneasily”.

Дальше по структуре (рис. 1) следует transformer [2]. Авторы статьи заметили, что классическому transformer не хватает направленности по сравнению с моделью BiLSTM, не смотря на то, что transformer имеет position embedding. Поэтому в TENER присутствует изменный вариант учета position embedding и механизма attention, что как заявляют авторы, улучшает направленность, т.к. transformer лучше понимает с какой стороны пришел контекст. Также изменненый transformer благодаря улучшенному механизму внимания лучше сохраняет контекст [2]. Кроме того, авторы убрали масштабирование внимания, что более выгоднее в NER задаче, т.к. на показанных ранее примерах видно, что наибольшую значимость представляют только некоторые слова в предложениия, я вляющиеся сущностями. Формулы изменения механизма atention и position embedding можно найти в статье [2].

Последняя часть TENER - классический модуль CRF, чтобы использовать преимущество зависимости между tags.

