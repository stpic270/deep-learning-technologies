{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9EQof0pO7PSe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from sklearn.metrics import f1_score   \n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy import special\n",
        "from matplotlib.pyplot import imshow\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import keras\n",
        "import gc\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as k\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a1gmDarkp2KQ"
      },
      "outputs": [],
      "source": [
        "class AdaBound_N():\n",
        "\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\n",
        "                 eps=1e-8, weight_decay=0, amsbound=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= final_lr:\n",
        "            raise ValueError(\"Invalid final learning rate: {}\".format(final_lr))\n",
        "        if not 0.0 <= gamma < 1.0:\n",
        "            raise ValueError(\"Invalid gamma parameter: {}\".format(gamma))\n",
        "\n",
        "        self.defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\n",
        "                        weight_decay=weight_decay, amsbound=amsbound)\n",
        "        \n",
        "        self.param_groups = []\n",
        "\n",
        "        self.param_groups.append(dict(params=params, lr=lr, betas=betas, final_lr=final_lr, \n",
        "                                      gamma=gamma, eps=eps, weight_decay=weight_decay, amsbound=amsbound))\n",
        "\n",
        "        self.base_lrs = list(map(lambda group: group['lr'], self.param_groups))\n",
        "\n",
        "\n",
        "    def initialize_state(self, parameters):\n",
        "\n",
        "        L = len(parameters) # number of layers in the neural networks\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "\n",
        "        for l in range(L):\n",
        "\n",
        "            \n",
        "\n",
        "            self.m[\"dW\" + str(l + 1)] = np.zeros_like(parameters[l].weight)\n",
        "            self.m[\"db\" + str(l + 1)] = np.zeros_like(parameters[l].bias)\n",
        "\n",
        "            self.v[\"dW\" + str(l+1)] = np.zeros_like(parameters[l].weight)\n",
        "            self.v[\"db\" + str(l+1)] = np.zeros_like(parameters[l].bias)\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    def step(self, state=1):\n",
        "\n",
        "        for group, base_lr in zip(self.param_groups, self.base_lrs):\n",
        "            for p, l in zip(group['params'], range(len(group['params']))):\n",
        "                if p.grads is None:\n",
        "                    continue\n",
        "                grad = p.grads\n",
        "                \n",
        "                # State initialization\n",
        "                beta1, beta2 = group['betas']\n",
        "                if state == 1:\n",
        "                  self.initialize_state(self.param_groups[0]['params'])\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                self.m[\"dW\" + str(l + 1)] = beta1 * self.m[\"dW\" + str(l + 1)] + (1 - beta1) * grad['dW']\n",
        "                self.m[\"db\" + str(l + 1)] = beta1 * self.m[\"db\" + str(l + 1)] + (1 - beta1) * grad['db']\n",
        "\n",
        "                self.v[\"dW\" + str(l + 1)] = beta2 * self.v[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grad['dW'], 2)\n",
        "                self.v[\"db\" + str(l + 1)] = beta2 * self.v[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grad['db'], 2)\n",
        "\n",
        "                \n",
        "                denom_W = np.sqrt(self.v[\"dW\" + str(l + 1)]) + group['eps']\n",
        "                denom_b = np.sqrt(self.v[\"db\" + str(l + 1)]) + group['eps']\n",
        "\n",
        "                bias_correction1 = 1 - beta1 ** state\n",
        "                bias_correction2 = 1 - beta2 ** state\n",
        "                step_size = group['lr'] * np.sqrt(bias_correction2)/ bias_correction1\n",
        "\n",
        "                final_lr = group['final_lr'] * group['lr'] / base_lr\n",
        "                lower_bound = final_lr * (1 - 1 / (group['gamma'] * state + 1))\n",
        "                upper_bound = final_lr * (1 + 1 / (group['gamma'] * state + group['eps']))\n",
        "\n",
        "                step_size_W = np.full_like(denom_W, step_size)\n",
        "                step_size_b = np.full_like(denom_b, step_size)\n",
        "\n",
        "                step_size_W = np.clip(step_size_W / denom_W, lower_bound, upper_bound) * self.m[\"dW\" + str(l + 1)] \n",
        "                step_size_b = np.clip(step_size_b / denom_b, lower_bound, upper_bound) * self.m[\"db\" + str(l + 1)]\n",
        "\n",
        "#               update parameters\n",
        "                p.weight -= step_size_W\n",
        "                p.bias -= step_size_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NplMU8P3p9ZV"
      },
      "outputs": [],
      "source": [
        "class Ada():\n",
        "\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsbound=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "\n",
        "        self.defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsbound=amsbound)\n",
        "        \n",
        "        self.param_groups = []\n",
        "\n",
        "        self.param_groups.append(dict(params=params, lr=lr, betas=betas,eps=eps, weight_decay=weight_decay, amsbound=amsbound))\n",
        "\n",
        "        self.base_lrs = list(map(lambda group: group['lr'], self.param_groups))\n",
        "\n",
        "\n",
        "    def initialize_state(self, parameters):\n",
        "\n",
        "        L = len(parameters) # number of layers in the neural networks\n",
        "        self.v = {}\n",
        "        self.s = {}\n",
        "\n",
        "        for l in range(L):\n",
        "\n",
        "            \n",
        "\n",
        "            self.v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[l].weight)\n",
        "            self.v[\"db\" + str(l + 1)] = np.zeros_like(parameters[l].bias)\n",
        "\n",
        "            self.s[\"dW\" + str(l+1)] = np.zeros_like(parameters[l].weight)\n",
        "            self.s[\"db\" + str(l+1)] = np.zeros_like(parameters[l].bias)\n",
        "\n",
        "    \n",
        "    def step(self, state=0):\n",
        "        v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "        s_corrected = {}\n",
        "\n",
        "        for group, base_lr in zip(self.param_groups, self.base_lrs):\n",
        "            for p, l in zip(group['params'], range(len(group['params']))):\n",
        "                if p.grads is None:\n",
        "                    continue\n",
        "                grad = p.grads\n",
        "                \n",
        "                # State initialization\n",
        "                beta1, beta2 = group['betas']\n",
        "                if state == 0:\n",
        "                  self.initialize_state(self.param_groups[0]['params'])\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                self.v[\"dW\" + str(l + 1)] = beta1 * self.v[\"dW\" + str(l + 1)] + (1 - beta1) * grad['dW']\n",
        "                self.v[\"db\" + str(l + 1)] = beta1 * self.v[\"db\" + str(l + 1)] + (1 - beta1) * grad['db']\n",
        "\n",
        "                v_corrected[\"dW\" + str(l + 1)] = self.v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, state+1)+ group['eps'])\n",
        "                v_corrected[\"db\" + str(l + 1)] = self.v[\"db\" + str(l + 1)] / (1 - np.power(beta1, state+1)+ group['eps'])\n",
        "\n",
        "                self.s[\"dW\" + str(l + 1)] = beta2 * self.s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grad['dW'], 2)\n",
        "                self.s[\"db\" + str(l + 1)] = beta2 * self.s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grad['db'], 2)\n",
        "\n",
        "                s_corrected[\"dW\" + str(l + 1)] = self.s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, state+1) + group['eps'])\n",
        "                s_corrected[\"db\" + str(l + 1)] = self.s[\"db\" + str(l + 1)] / (1 - np.power(beta2, state+1) + group['eps'])\n",
        "\n",
        "#               update parameters\n",
        "                p.weight = p.weight - group['lr'] * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + group['eps'])\n",
        "                p.bias = p.bias - group['lr'] * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + group['eps'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-C5VYPMWjlzN"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def reluDerivative(x):\n",
        "  \n",
        "    x[x<=0] = 0\n",
        "    x[x>0] = 1\n",
        "    return x\n",
        "\n",
        "def zero_pad(X, pad):\n",
        "\n",
        "  X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0,0)), mode=\"constant\", constant_values=(0, 0))\n",
        "\n",
        "  return X_pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_fQkJNbuf9Eh"
      },
      "outputs": [],
      "source": [
        "def conv_single_step(a_slice_prev, W, b):\n",
        "\n",
        "  s = a_slice_prev * W\n",
        "  Z = np.sum(s)\n",
        "  Z = (Z + float(b))\n",
        "\n",
        "  return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XWcTFYCRjF6u"
      },
      "outputs": [],
      "source": [
        "def pool_forward(A_prev, hparameters, mode=\"max\"):\n",
        "\n",
        "  m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "\n",
        "  stride = hparameters['stride']\n",
        "  f = hparameters['f']\n",
        "\n",
        "  n_H = int((n_H_prev - f)/stride) + 1\n",
        "  n_W = int((n_W_prev - f)/stride) + 1\n",
        "  n_C = n_C_prev\n",
        "\n",
        "  A = np.zeros((m, n_H, n_W, n_C))\n",
        "\n",
        "  for i in range(m):\n",
        "    for h in range(n_H):\n",
        "      vert_start = h * stride\n",
        "      vert_end = h * stride + f\n",
        "      for w in range(n_W):\n",
        "        horiz_start = w * stride\n",
        "        horiz_end = w * stride + f\n",
        "        for c in range(n_C):\n",
        "          a_slice_prev = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "          if mode == \"max\":\n",
        "            A[i, h, w, c] = np.max(a_slice_prev)\n",
        "          elif mode == \"average\":\n",
        "            A[i, h, w, c] = np.mean(a_slice_prev)\n",
        "  cache = (A_prev, hparameters)\n",
        "\n",
        "  return A, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0RAmtjTEvlMX"
      },
      "outputs": [],
      "source": [
        "def create_mask_from_window(x):\n",
        "\n",
        "  mask = (x == np.max(x))\n",
        "\n",
        "  return mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r-dDc3inwL_h"
      },
      "outputs": [],
      "source": [
        "def distribute_value(dz, shape):\n",
        "\n",
        "  n_H, n_W = shape[0], shape[1]\n",
        "  average = dz/(n_H * n_W)\n",
        "  a = np.ones((n_H, n_W)) * average\n",
        "\n",
        "  return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eu79z5M-wTW5"
      },
      "outputs": [],
      "source": [
        "def pool_backward(dA, cache, mode = \"max\"):\n",
        "\n",
        "  A_prev, hparameters = cache\n",
        "\n",
        "  stride = hparameters['stride']\n",
        "  f = hparameters['f']\n",
        "\n",
        "  m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "  m, n_H, n_W, n_C = dA.shape\n",
        "\n",
        "  dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
        "\n",
        "  for i in range(m):\n",
        "    da_prev = dA_prev[i]\n",
        "    for h in range(n_H):\n",
        "      for w in range(n_W):\n",
        "        for c in range(n_C):\n",
        "          vert_start = h * stride\n",
        "          vert_end = h * stride + f\n",
        "          horiz_start = w * stride\n",
        "          horiz_end = w * stride + f\n",
        "\n",
        "          if mode == 'max':\n",
        "            a_slice = da_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "            mask = create_mask_from_window(a_slice)\n",
        "            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
        "\n",
        "          if mode == 'average':\n",
        "            shape = (f, f)\n",
        "            da = dA[i, h, w, c]\n",
        "            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
        "\n",
        "\n",
        "  return dA_prev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_frfh2Pi1O9T"
      },
      "outputs": [],
      "source": [
        "class Conv2D:\n",
        "    \"\"\"\n",
        "    An implementation of the convolutional layer. We convolve the input with out_channels different filters\n",
        "    and each filter spans all channels in the input.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, pooling=False):\n",
        "        \"\"\"\n",
        "        :param in_channels: the number of channels of the input data\n",
        "        :param out_channels: the number of channels of the output(aka the number of filters applied in the layer)\n",
        "        :param kernel_size: the specified size of the kernel(both height and width)\n",
        "        :param stride: the stride of convolution\n",
        "        :param padding: the size of padding. Pad zeros to the input with padding size.\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.pooling = pooling\n",
        "\n",
        "        self.cache = None\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        self.weight = 1e-3 * np.random.randn(self.kernel_size, self.kernel_size, self.in_channels, self.out_channels)\n",
        "        self.bias = np.zeros((1, 1, 1, self.out_channels))\n",
        "\n",
        "    def conv_forward(self, x, activation='relu'):\n",
        "\n",
        "\n",
        "        A_prev = x\n",
        "        m, n_H_prev, n_W_prev, n_C_prev = x.shape\n",
        "        W = self.weight\n",
        "        b = self.bias\n",
        "        f, f, n_C_prev, n_C = W.shape\n",
        "\n",
        "        stride = self.stride\n",
        "        pad = self.padding\n",
        "\n",
        "        n_H = int((n_H_prev + 2*pad - f)/stride) + 1\n",
        "        n_W = int((n_W_prev + 2*pad - f)/stride) + 1\n",
        "\n",
        "        Z = np.zeros((m, n_H, n_W, n_C))\n",
        "        A = np.zeros((m, n_H, n_W, n_C))\n",
        "        \n",
        "        A_prev_pad = zero_pad(A_prev, pad)\n",
        "        for i in range(m):\n",
        "          a_prev_pad = A_prev_pad[i]\n",
        "\n",
        "          for h in range(n_H):\n",
        "            vert_start = h * stride\n",
        "            vert_end = h * stride + f\n",
        "            for w in range(n_W):\n",
        "              horiz_start = w * stride \n",
        "              horiz_end = w * stride + f\n",
        "              for c in range(n_C):\n",
        "                a_slice_prev = a_prev_pad[vert_start: vert_end, horiz_start: horiz_end, :]\n",
        "                weights = W[:,:,:, c]\n",
        "                biases = b[:, :, :, c]\n",
        "\n",
        "                z = Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
        "\n",
        "                if activation == 'relu':\n",
        "                  A[i, h, w, c] = relu(z)\n",
        "\n",
        "        # Making sure your output shape is correct\n",
        "        assert(Z.shape == (m, n_H, n_W, n_C))\n",
        "\n",
        "        if self.pooling == True:\n",
        "            A, self.pool_cache = pool_forward(A, hparameters=dict(f=2, stride=2))\n",
        "            \n",
        "\n",
        "        self.cache = (A_prev, Z)\n",
        "\n",
        "\n",
        "\n",
        "        return A\n",
        "\n",
        "    def Backward(self, dA):\n",
        "\n",
        "        if self.pooling == True:\n",
        "            dA = pool_backward(dA, self.pool_cache)\n",
        "        \n",
        "        stride = self.stride\n",
        "        pad = self.padding\n",
        "        W = self.weight\n",
        "        b = self.bias\n",
        "        \n",
        "        (A_prev, Z) = self.cache\n",
        "        dZ = reluDerivative(Z) * dA\n",
        "\n",
        "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "        f, f, n_C_prev, n_C = W.shape\n",
        "        m, n_H, n_W, c = dZ.shape               \n",
        "\n",
        "        dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
        "        dW = np.zeros((f, f, n_C_prev, n_C))\n",
        "        db = np.zeros((1, 1, 1, n_C))\n",
        "\n",
        "        A_prev_pad = zero_pad(A_prev, pad)\n",
        "        dA_prev_pad = zero_pad(dA_prev, pad)\n",
        "\n",
        "        for i in range(m):\n",
        "          a_prev_pad = A_prev_pad[i]\n",
        "          da_prev_pad = dA_prev_pad[i]\n",
        "          for h in range(n_H):\n",
        "            for w in range(n_W):\n",
        "              for c in range(n_C):\n",
        "                vert_start = h * stride\n",
        "                vert_end = h * stride + f\n",
        "                horiz_start = w * stride\n",
        "                horiz_end = w * stride + f\n",
        "\n",
        "                a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "                da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c] * dZ[i, h, w, c]\n",
        "                dW[:, :, :, c] += a_slice * dZ[i, h, w, c]\n",
        "                db[:, :, :, c] += dZ[i, h, w, c]\n",
        "          \n",
        "          dA_prev[i] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
        "        \n",
        "        dW /= m\n",
        "        db /= m\n",
        "\n",
        "        grads = dict(dA_prev=dA_prev, dW=dW, db=db)\n",
        "        self.grads = grads\n",
        "        return grads\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "kLPqHj3Pnlqb"
      },
      "outputs": [],
      "source": [
        "class Linear():   \n",
        "\n",
        "  def __init__(self, in_n, out_n, activation='relu'):\n",
        "\n",
        "    self.in_n = in_n\n",
        "    self.out_n = out_n\n",
        "    self.activation = activation\n",
        "    self.eps = 0.000001\n",
        "    self.initialize_parameters_deep([in_n, out_n])\n",
        "\n",
        "  def initialize_parameters_deep(self, layer_dims):\n",
        "  \n",
        "    \n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1, L):\n",
        "      self.weight = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "      self.bias = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "\n",
        "  def linear_forward(self, A, W, b):\n",
        "\n",
        "    Z = np.dot(W, A) + b\n",
        "    cache = A, W, b\n",
        "\n",
        "    return Z, cache\n",
        "\n",
        "  def linear_activation_forward(self, A_prev, W, b, activation):\n",
        "\n",
        "    if activation == \"relu\":\n",
        "      Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
        "      A, activation_cache = self.relu(Z)\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "      Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
        "      A, activation_cache = self.sigmoid(Z)\n",
        "\n",
        "    if activation =='softmax':\n",
        "      Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
        "      A, activation_cache = self.softmax(Z)\n",
        "\n",
        "    cache = (linear_cache, activation_cache)\n",
        "    return A, cache\n",
        "\n",
        "  def L_linear_forward(self, X):\n",
        "\n",
        "    A = X\n",
        "    self.caches = []\n",
        "    \n",
        "    \n",
        "\n",
        "    AL, cache = self.linear_activation_forward(A, self.weight, self.bias, activation=self.activation)\n",
        "    self.caches.append(cache)\n",
        "\n",
        "    return AL\n",
        "\n",
        "  def compute_cost(self, AL, Y):\n",
        "    \n",
        "    AL = (AL==0) * self.eps + AL\n",
        "    Y = Y.reshape(AL.shape)\n",
        "    loss=-np.sum(Y*np.log(AL))\n",
        "    self.dAL = AL - Y\n",
        "\n",
        "    return loss/float(AL.shape[1])\n",
        "    \n",
        "\n",
        "  def linear_backward(self, dZ, cache):\n",
        "\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
        "    db = 1/m * np.sum(dZ, axis = 1, keepdims =True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "  def linear_activation_backward(self, dA, cache, activation):\n",
        "\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if self.activation == \"relu\":\n",
        "      dZ = self.relu_backward(dA, activation_cache)\n",
        "      dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
        "\n",
        "    if self.activation == \"sigmoid\":\n",
        "      dZ = self.sigmoid_backward(dA, activation_cache)\n",
        "      dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
        "\n",
        "    if self.activation == \"softmax\":\n",
        "      dZ = dA\n",
        "      dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "  def Backward(self, dA_prev=0.5):\n",
        "\n",
        "    grads = {}\n",
        "    L = len(self.caches) \n",
        "    if self.activation == 'softmax':\n",
        "      dAL = self.dAL\n",
        "    else:\n",
        "      dAL = dA_prev\n",
        "\n",
        "    current_cache = self.linear_activation_backward(dAL, self.caches[L-1], activation=self.activation)\n",
        "    d_Aprev_temp, dW_temp, db_temp = current_cache\n",
        "    grads[\"dA_prev\"] = d_Aprev_temp\n",
        "    grads[\"dW\"] = dW_temp\n",
        "    grads[\"db\"] = db_temp\n",
        "\n",
        "    for l in reversed(range(L - 1)):\n",
        "      print('lol')\n",
        "      current_cache = self.linear_activation_backward(grads[\"dA\" + str(l + 1)], self.caches[l], activation=\"relu\")\n",
        "      d_Aprev_temp, dW_temp, db_temp = current_cache\n",
        "      grads[\"dA\" + str(l)] = d_Aprev_temp\n",
        "      grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "      grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    self.grads = grads\n",
        "    return grads\n",
        "\n",
        "          \n",
        "    return parameters\n",
        "\n",
        "  def softmax(self, Z):\n",
        "\n",
        "    A = special.softmax(Z)\n",
        "    A = np.clip(A, self.eps, 1)\n",
        "\n",
        "    return A, Z\n",
        "\n",
        "  def sigmoid(self, Z):\n",
        "\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "\n",
        "    return A, Z\n",
        "\n",
        "  def sigmoid_backward(self, dA, Z):\n",
        "\n",
        "    g = self.sigmoid(Z)[0] * (1 - self.sigmoid(Z)[0])\n",
        "    dZ = np.multiply(g, dA)\n",
        "\n",
        "    return dZ\n",
        "\n",
        "  def relu(self, Z):\n",
        "\n",
        "    A = Z * (Z > 0)\n",
        "\n",
        "    return A, Z\n",
        "\n",
        "  def relu_backward(self, dA, Z):\n",
        "\n",
        "    g = 1 * (Z > 0)\n",
        "    dZ = np.multiply(g, dA)\n",
        "\n",
        "    return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KrteA5VRPPL0"
      },
      "outputs": [],
      "source": [
        "class VGG16():\n",
        "    def __init__(self, num_classes=10, shrink=7):\n",
        "\n",
        "        self.layer1 = Conv2D(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer2 = Conv2D(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, pooling=True)\n",
        "        self.layer3 = Conv2D(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer4 = Conv2D(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, pooling=True)\n",
        "        self.layer5 = Conv2D(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer6 = Conv2D(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer7 = Conv2D(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, pooling=True)\n",
        "        self.layer8 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer9 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer10 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1, pooling=True)\n",
        "        self.layer11 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer12 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer13 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1, pooling=True)\n",
        "        self.Lnn1 = Linear(shrink*shrink*512, 4096)\n",
        "        self.Lnn2 = Linear(4096, 4096)\n",
        "        self.Lnn3 = Linear(4096, num_classes, activation='softmax')\n",
        "\n",
        "        \n",
        "    def forward(self, x, batch_size=1):\n",
        "        out = self.layer1.conv_forward(x)\n",
        "        out = self.layer2.conv_forward(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer3.conv_forward(out)\n",
        "        out = self.layer4.conv_forward(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer5.conv_forward(out)\n",
        "        out = self.layer6.conv_forward(out)\n",
        "        out = self.layer7.conv_forward(out)\n",
        "       # print(out.shape)\n",
        "        out = self.layer8.conv_forward(out)\n",
        "        out = self.layer9.conv_forward(out)\n",
        "        out = self.layer10.conv_forward(out)\n",
        "       # print(out.shape)\n",
        "        out = self.layer11.conv_forward(out)\n",
        "        out = self.layer12.conv_forward(out)\n",
        "        out = self.layer13.conv_forward(out)\n",
        "       # print(out.shape)\n",
        "        out = out.reshape(-1, batch_size)\n",
        "        #print(out.shape)\n",
        "        out = self.Lnn1.L_linear_forward(out)\n",
        "        out = self.Lnn2.L_linear_forward(out)\n",
        "        out = self.Lnn3.L_linear_forward(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, Layers, batch_size=1):\n",
        "\n",
        "      self.GR = []\n",
        "      grads = self.Lnn3.Backward()\n",
        "      self.GR.append(grads)\n",
        "\n",
        "      for l in Layers[0:2]: \n",
        "        grads = l.Backward(grads['dA_prev'])\n",
        "        self.GR.append(grads)\n",
        "      A_prev = self.layer13.cache[0]\n",
        "      dim = int(A_prev.shape[1] / 2)\n",
        "      grads['dA_prev'] = grads['dA_prev'].reshape(batch_size, dim, dim, 512)\n",
        "      for l in Layers[2:]: \n",
        "        grads = l.Backward(grads['dA_prev'])\n",
        "        self.GR.append(grads)\n",
        "\n",
        "      GR = self.GR\n",
        "\n",
        "      return GR\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v6v76z7_v0z3"
      },
      "outputs": [],
      "source": [
        "def sp_prop(model):\n",
        "\n",
        "  spisok = []\n",
        "  spisok_l = []\n",
        "\n",
        "  for l in model.__dict__.keys():\n",
        "    spisok.append(model.__dict__[l])\n",
        "\n",
        "  for l in reversed(spisok):\n",
        "    spisok_l.append(l)\n",
        "  \n",
        "  return spisok_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "z3FkNsC2VD_m"
      },
      "outputs": [],
      "source": [
        "x= np.random.randn(2, 32, 32, 1)\n",
        "batch_size = x.shape[0]\n",
        "model = VGG16(num_classes=10, shrink=x.shape[1]//32)\n",
        "params = sp_prop(model)\n",
        "params.reverse()\n",
        "opt = AdaBound_N(params=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cC31IV-JJd5N"
      },
      "outputs": [],
      "source": [
        "AA = model.forward(x, batch_size)\n",
        "loss = model.Lnn3.compute_cost(AA, np.array(np.eye(20)[1]).reshape(-1, 2))\n",
        "Layers = sp_prop(model)\n",
        "grads = model.backward(Layers[1:], x.shape[0])\n",
        "opt.step(state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnp7tmyzGqHO",
        "outputId": "e33716bf-f97a-49ee-8864-a5c69630a41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "Ry1jRWmUn77L",
        "outputId": "9c7641e2-a5ca-4140-94af-6dc6b01875a3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for i in range(3):  \n",
        "  pyplot.subplot(330 + 1 + i)\n",
        "  pyplot.imshow(x_train[i], cmap=pyplot.get_cmap('gray'))\n",
        "  pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I4XCCuOgoNik"
      },
      "outputs": [],
      "source": [
        "# expand new axis, channel axis \n",
        "x_train, x_test = np.expand_dims(x_train, axis=-1), np.expand_dims(x_test, axis=-1)\n",
        "# it's always better to normalize \n",
        "x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
        "# resize the input shape , i.e. old shape: 28, new shape: 32\n",
        "x_train, x_test = tf.image.resize(x_train, [32,32]), tf.image.resize(x_test, [32,32]) # if we want to resize \n",
        "# one-hot \n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPZ5JBJdoQjv",
        "outputId": "87992fdc-87da-42e8-d99e-3ff1ff843777"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000, 32, 32, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beTfqCdYoTqJ",
        "outputId": "d45dee4c-5d18-4c2a-9360-687e20fa4040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 32, 32, 1) (60000, 10)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pSCEVnmcoVkH"
      },
      "outputs": [],
      "source": [
        "# Batch_size\n",
        "X_train, Y_train, X_val, Y_val = [], [], [], []\n",
        "batch_size = 3\n",
        "for i in range(0, 60, batch_size):\n",
        "  start_idx, end_idx = i, i + batch_size\n",
        "  X_train.append(x_train[start_idx:end_idx])\n",
        "  Y_train.append(y_train[start_idx:end_idx].reshape(10, batch_size))\n",
        "\n",
        "for i in range(0, 10, batch_size):\n",
        "  start_idx, end_idx = i, i + batch_size\n",
        "  X_val.append(x_test[start_idx:end_idx])\n",
        "  Y_val.append(y_test[start_idx:end_idx].reshape(10, batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM6FcOE-o7Er",
        "outputId": "1dfe2d46-486b-4e35-e8fe-43e63fb71ee4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 32, 32, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "X_val[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "V58v9Ze4tuz-"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "for i in range(10):\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNgRlu_G2R2j",
        "outputId": "e3db8524-9b74-4442-8259-a5ceadb6fa4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shrink =  1 batch_size =  3\n",
            "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "[[0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333\n",
            "  0.03333333 0.03333333 0.03333333 0.03333333]\n",
            " [0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333\n",
            "  0.03333333 0.03333333 0.03333333 0.03333333]\n",
            " [0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333\n",
            "  0.03333333 0.03333333 0.03333333 0.03333333]]\n",
            "loss =  3.4011973816621555\n",
            "f1_macro =  0.0\n",
            "accuracy =  0.0\n",
            "[[0.03324549 0.03324622 0.03324612 0.03357575 0.03357984 0.03357447\n",
            "  0.03323162 0.03323266 0.03323238 0.03323994]\n",
            " [0.03323975 0.03323976 0.03323546 0.03323432 0.03323527 0.03324057\n",
            "  0.03324143 0.03324134 0.03354985 0.033555  ]\n",
            " [0.03355047 0.03324048 0.03324049 0.03324073 0.0335307  0.03353443\n",
            "  0.03353145 0.03323866 0.03323821 0.03323712]]\n",
            "loss =  3.3945269899573205\n",
            "f1_macro =  0.5\n",
            "accuracy =  0.6666666666666666\n",
            "[[0.03306292 0.03306101 0.03306252 0.03402219 0.03404694 0.03402144\n",
            "  0.03301236 0.03301068 0.03301245 0.03306449]\n",
            " [0.03306322 0.03306292 0.03305359 0.03304981 0.03305352 0.0330349\n",
            "  0.03303306 0.03303427 0.03397985 0.03400561]\n",
            " [0.03398021 0.03307782 0.03307662 0.03307859 0.03395306 0.03397362\n",
            "  0.03395498 0.03305321 0.03305216 0.03305197]]\n",
            "loss =  3.3811386033250006\n",
            "f1_macro =  0.5\n",
            "accuracy =  0.6666666666666666\n",
            "[[0.03279082 0.03277651 0.03279243 0.03468685 0.03478458 0.03469023\n",
            "  0.03270706 0.03269154 0.0327087  0.03278422]\n",
            " [0.03277217 0.03278382 0.03278682 0.0327689  0.03278531 0.03274223\n",
            "  0.03272526 0.03274344 0.0346096  0.03470253]\n",
            " [0.03461122 0.03281373 0.03279912 0.03281353 0.03457413 0.03465779\n",
            "  0.03457728 0.03277936 0.03276322 0.03277759]]\n",
            "loss =  3.360585477819677\n",
            "f1_macro =  1.0\n",
            "accuracy =  1.0\n",
            "[[0.03235672 0.03227581 0.03235837 0.03573332 0.03608687 0.03574189\n",
            "  0.03222121 0.03213176 0.03222371 0.03235457]\n",
            " [0.03227987 0.03235398 0.03235762 0.03227393 0.03235385 0.03226881\n",
            "  0.03217611 0.03227028 0.03561468 0.03595185]\n",
            " [0.03561789 0.03238674 0.03230948 0.03238826 0.03554746 0.03586727\n",
            "  0.03555676 0.03234172 0.03226058 0.03233862]]\n",
            "loss =  3.3251103531778363\n",
            "f1_macro =  1.0\n",
            "accuracy =  1.0\n",
            "[[0.03159043 0.03126852 0.0315889  0.03752924 0.03876641 0.03754951\n",
            "  0.0313701  0.03100522 0.03137325 0.03157882]\n",
            " [0.03126172 0.03157414 0.03159501 0.03126526 0.03158526 0.03144696\n",
            "  0.03108579 0.03144483 0.0373496  0.03854004]\n",
            " [0.03735481 0.03163665 0.03132502 0.03163555 0.03723603 0.03838208\n",
            "  0.03726026 0.03157621 0.0312536  0.03157079]]\n",
            "loss =  3.2554743995127553\n",
            "f1_macro =  1.0\n",
            "accuracy =  1.0\n",
            "[[0.03003824 0.02895352 0.03002656 0.04098751 0.04523759 0.04102686\n",
            "  0.02968077 0.02848325 0.02967816 0.03000989]\n",
            " [0.02892823 0.0299965  0.03003811 0.02895605 0.03002075 0.02980497\n",
            "  0.02861737 0.02980037 0.04065527 0.04473928]\n",
            " [0.04068027 0.03011865 0.02907725 0.03011724 0.04046233 0.04438575\n",
            "  0.04051322 0.030021   0.02894189 0.03000317]]\n",
            "loss =  3.105855757287811\n",
            "f1_macro =  1.0\n",
            "accuracy =  1.0\n",
            "[[0.02613804 0.02310016 0.026118   0.04815168 0.06477383 0.04826523\n",
            "  0.02557007 0.02227973 0.02556422 0.02607587]\n",
            " [0.02300886 0.02604576 0.02613574 0.02307701 0.02609587 0.02575874\n",
            "  0.02250877 0.02573959 0.04738587 0.06312727]\n",
            " [0.04746526 0.02628226 0.02332476 0.02627203 0.0470186  0.06217793\n",
            "  0.04714865 0.02614755 0.02312764 0.02611501]]\n",
            "loss =  2.7590703994257786\n",
            "f1_macro =  1.0\n",
            "accuracy =  1.0\n",
            "[[0.01293101 0.0079717  0.0128927  0.05479912 0.15845247 0.05515272\n",
            "  0.01231118 0.00720862 0.01229084 0.0128495 ]\n",
            " [0.00785362 0.01280976 0.01291751 0.00793322 0.0128723  0.01250959\n",
            "  0.00742011 0.01248949 0.05269808 0.1467955 ]\n",
            " [0.05291944 0.01309642 0.00819015 0.01307119 0.05188266 0.14146214\n",
            "  0.05228812 0.0129723  0.00803594 0.01292262]]\n",
            "loss =  1.9055795459010734\n",
            "f1_macro =  1.0\n",
            "accuracy =  1.0\n",
            "[[7.92724387e-06 1.00000000e-06 7.86897360e-06 1.16679325e-03\n",
            "  4.31775837e-01 1.18756836e-03 6.77756126e-06 1.00000000e-06\n",
            "  6.78171012e-06 7.73490916e-06]\n",
            " [1.00000000e-06 7.68253946e-06 7.94443183e-06 1.00000000e-06\n",
            "  7.89179244e-06 7.12515438e-06 1.00000000e-06 7.12914099e-06\n",
            "  1.01509728e-03 3.02492614e-01]\n",
            " [1.02259701e-03 8.26608964e-06 1.00000000e-06 8.24255130e-06\n",
            "  9.66672260e-04 2.59278823e-01 9.83702242e-04 8.06874669e-06\n",
            "  1.00000000e-06 8.00856358e-06]]\n",
            "loss =  1.1284661326225105\n",
            "f1_macro =  1.0\n",
            "accuracy =  1.0\n"
          ]
        }
      ],
      "source": [
        "x = X_val[0]\n",
        "\n",
        "shrink = x.shape[1]//32\n",
        "batch_size = x.shape[0]\n",
        "print('shrink = ', shrink, 'batch_size = ', batch_size)\n",
        "\n",
        "model = VGG16(num_classes=10, shrink=shrink)\n",
        "params = sp_prop(model)\n",
        "#print(params)\n",
        "params.reverse()\n",
        "opt = AdaBound_N(params, lr=1e-3, betas=(0.9, 0.999), final_lr=7.5e-3, gamma=1e-5,\n",
        "                 eps=1e-8, weight_decay=0, amsbound=False)\n",
        "\n",
        "y = Y_val[2]\n",
        "print(y.reshape(y.shape[1], y.shape[0]))\n",
        "\n",
        "running_loss = 0\n",
        "Layers = sp_prop(model)\n",
        "num_epoch = 10\n",
        "\n",
        "for i in range(1, num_epoch + 1):\n",
        "\n",
        "  y_true, y_preds, TR, PR = [], [], [], []\n",
        "  running_loss = 0\n",
        "  opt.initialize_state(params)\n",
        "\n",
        "  for x, y in zip(X_val[2:3], Y_val[2:3]):\n",
        "\n",
        "    out = model.forward(x, batch_size)\n",
        "    loss = model.Lnn3.compute_cost(out, y)\n",
        "    running_loss += loss\n",
        "    grads = model.backward(Layers[1:], batch_size)\n",
        "    opt.step(i)\n",
        "\n",
        "    out, y = out.reshape(out.shape[1], out.shape[0]), y.reshape(y.shape[1], y.shape[0])\n",
        "    print(out)\n",
        "    y_preds.extend(out)\n",
        "    y_true.extend(y)\n",
        "\n",
        "    PR.extend(np.argmax(out, axis=1))\n",
        "    TR.extend(np.argmax(y, axis=1))\n",
        "\n",
        "  \n",
        "  \n",
        "  y_true = np.array(y_true, dtype=np.int16)\n",
        "  y_preds = np.array(y_preds, dtype=np.int16)\n",
        "\n",
        "  print('loss = ', running_loss)\n",
        "  print('f1_macro = ', f1_score(TR, PR, average='macro'))\n",
        "  print('accuracy = ', accuracy_score(TR, PR))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J5HnPpaYWL-N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}