{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9EQof0pO7PSe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from sklearn.metrics import f1_score   \n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy import special\n",
        "from matplotlib.pyplot import imshow\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import keras\n",
        "import gc\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as k\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a_SpruDvrIpg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AdaBound_N():\n",
        "\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\n",
        "                 eps=1e-8, weight_decay=0, amsbound=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= final_lr:\n",
        "            raise ValueError(\"Invalid final learning rate: {}\".format(final_lr))\n",
        "        if not 0.0 <= gamma < 1.0:\n",
        "            raise ValueError(\"Invalid gamma parameter: {}\".format(gamma))\n",
        "\n",
        "        self.defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\n",
        "                        weight_decay=weight_decay, amsbound=amsbound)\n",
        "        \n",
        "        self.param_groups = []\n",
        "\n",
        "        self.param_groups.append(dict(params=params, lr=lr, betas=betas, final_lr=final_lr, \n",
        "                                      gamma=gamma, eps=eps, weight_decay=weight_decay, amsbound=amsbound))\n",
        "\n",
        "        self.base_lrs = list(map(lambda group: group['lr'], self.param_groups))\n",
        "\n",
        "\n",
        "    def initialize_state(self, parameters):\n",
        "\n",
        "        L = len(parameters) # number of layers in the neural networks\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "\n",
        "        for l in range(L):\n",
        "\n",
        "            \n",
        "\n",
        "            self.m[\"dW\" + str(l + 1)] = np.zeros_like(parameters[l].weight)\n",
        "            self.m[\"db\" + str(l + 1)] = np.zeros_like(parameters[l].bias)\n",
        "\n",
        "            self.v[\"dW\" + str(l+1)] = np.zeros_like(parameters[l].weight)\n",
        "            self.v[\"db\" + str(l+1)] = np.zeros_like(parameters[l].bias)\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    def step(self, state=1):\n",
        "\n",
        "        for group, base_lr in zip(self.param_groups, self.base_lrs):\n",
        "            for p, l in zip(group['params'], range(len(group['params']))):\n",
        "                if p.grads is None:\n",
        "                    continue\n",
        "                grad = p.grads\n",
        "                \n",
        "                # State initialization\n",
        "                beta1, beta2 = group['betas']\n",
        "                if state == 1:\n",
        "                  self.initialize_state(self.param_groups[0]['params'])\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                self.m[\"dW\" + str(l + 1)] = beta1 * self.m[\"dW\" + str(l + 1)] + (1 - beta1) * grad['dW']\n",
        "                self.m[\"db\" + str(l + 1)] = beta1 * self.m[\"db\" + str(l + 1)] + (1 - beta1) * grad['db']\n",
        "\n",
        "                self.v[\"dW\" + str(l + 1)] = beta2 * self.v[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grad['dW'], 2)\n",
        "                self.v[\"db\" + str(l + 1)] = beta2 * self.v[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grad['db'], 2)\n",
        "\n",
        "                \n",
        "                denom_W = np.sqrt(self.v[\"dW\" + str(l + 1)]) + group['eps']\n",
        "                denom_b = np.sqrt(self.v[\"db\" + str(l + 1)]) + group['eps']\n",
        "\n",
        "                bias_correction1 = 1 - beta1 ** state\n",
        "                bias_correction2 = 1 - beta2 ** state\n",
        "                step_size = group['lr'] * np.sqrt(bias_correction2)/ bias_correction1\n",
        "\n",
        "                final_lr = group['final_lr'] * group['lr'] / base_lr\n",
        "                lower_bound = final_lr * (1 - 1 / (group['gamma'] * state + 1))\n",
        "                upper_bound = final_lr * (1 + 1 / (group['gamma'] * state))\n",
        "\n",
        "                step_size_W = np.full_like(denom_W, step_size)\n",
        "                step_size_b = np.full_like(denom_b, step_size)\n",
        "\n",
        "                step_size_W = np.clip(step_size_W / denom_W, lower_bound, upper_bound) * self.m[\"dW\" + str(l + 1)] \n",
        "                step_size_b = np.clip(step_size_b / denom_b, lower_bound, upper_bound) * self.m[\"db\" + str(l + 1)]\n",
        "\n",
        "#               update parameters\n",
        "                p.weight -= step_size_W\n",
        "                p.bias -= step_size_b\n",
        "\n",
        "                \n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Ada():\n",
        "\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsbound=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "\n",
        "        self.defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsbound=amsbound)\n",
        "        \n",
        "        self.param_groups = []\n",
        "\n",
        "        self.param_groups.append(dict(params=params, lr=lr, betas=betas,eps=eps, weight_decay=weight_decay, amsbound=amsbound))\n",
        "\n",
        "        self.base_lrs = list(map(lambda group: group['lr'], self.param_groups))\n",
        "\n",
        "\n",
        "    def initialize_state(self, parameters):\n",
        "\n",
        "        L = len(parameters) # number of layers in the neural networks\n",
        "        self.v = {}\n",
        "        self.s = {}\n",
        "\n",
        "        for l in range(L):\n",
        "\n",
        "            \n",
        "\n",
        "            self.v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[l].weight)\n",
        "            self.v[\"db\" + str(l + 1)] = np.zeros_like(parameters[l].bias)\n",
        "\n",
        "            self.s[\"dW\" + str(l+1)] = np.zeros_like(parameters[l].weight)\n",
        "            self.s[\"db\" + str(l+1)] = np.zeros_like(parameters[l].bias)\n",
        "\n",
        "    \n",
        "    def step(self, state=0):\n",
        "        v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "        s_corrected = {}\n",
        "\n",
        "        for group, base_lr in zip(self.param_groups, self.base_lrs):\n",
        "            for p, l in zip(group['params'], range(len(group['params']))):\n",
        "                if p.grads is None:\n",
        "                    continue\n",
        "                grad = p.grads\n",
        "                \n",
        "                # State initialization\n",
        "                beta1, beta2 = group['betas']\n",
        "                if state == 0:\n",
        "                  self.initialize_state(self.param_groups[0]['params'])\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                self.v[\"dW\" + str(l + 1)] = beta1 * self.v[\"dW\" + str(l + 1)] + (1 - beta1) * grad['dW']\n",
        "                self.v[\"db\" + str(l + 1)] = beta1 * self.v[\"db\" + str(l + 1)] + (1 - beta1) * grad['db']\n",
        "\n",
        "                v_corrected[\"dW\" + str(l + 1)] = self.v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, state+1)+ group['eps'])\n",
        "                v_corrected[\"db\" + str(l + 1)] = self.v[\"db\" + str(l + 1)] / (1 - np.power(beta1, state+1)+ group['eps'])\n",
        "\n",
        "                self.s[\"dW\" + str(l + 1)] = beta2 * self.s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grad['dW'], 2)\n",
        "                self.s[\"db\" + str(l + 1)] = beta2 * self.s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grad['db'], 2)\n",
        "\n",
        "                s_corrected[\"dW\" + str(l + 1)] = self.s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, state+1) + group['eps'])\n",
        "                s_corrected[\"db\" + str(l + 1)] = self.s[\"db\" + str(l + 1)] / (1 - np.power(beta2, state+1) + group['eps'])\n",
        "\n",
        "#               update parameters\n",
        "                p.weight = p.weight - group['lr'] * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + group['eps'])\n",
        "                p.bias = p.bias - group['lr'] * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + group['eps'])\n"
      ],
      "metadata": {
        "id": "YoJFk3t_8GhJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-C5VYPMWjlzN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GlnSCKw7ACo1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def reluDerivative(x):\n",
        "  \n",
        "    x = (x > 0) * 1\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uAFPx1L7fsXX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def zero_pad(X, pad):\n",
        "\n",
        "  X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0,0)), mode=\"constant\", constant_values=(0, 0))\n",
        "\n",
        "  return X_pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kLPqHj3Pnlqb"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Linear():   \n",
        "\n",
        "  def __init__(self, in_n, out_n, activation='relu'):\n",
        "\n",
        "    self.in_n = in_n\n",
        "    self.out_n = out_n\n",
        "    self.activation = activation\n",
        "    self.eps = 0.00000001\n",
        "    self.initialize_parameters_deep([in_n, out_n])\n",
        "\n",
        "  def initialize_parameters_deep(self, layer_dims):\n",
        "  \n",
        "    \n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1, L):\n",
        "      self.weight = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "      self.bias = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "\n",
        "  def linear_forward(self, A, W, b):\n",
        "\n",
        "    Z = np.dot(W, A) + b\n",
        "    cache = A, W, b\n",
        "\n",
        "    return Z, cache\n",
        "\n",
        "  def linear_activation_forward(self, A_prev, W, b, activation):\n",
        "\n",
        "    if activation == \"relu\":\n",
        "      Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
        "      A, activation_cache = self.relu(Z)\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "      Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
        "      A, activation_cache = self.sigmoid(Z)\n",
        "\n",
        "    if activation =='softmax':\n",
        "      Z, linear_cache = self.linear_forward(A_prev, W, b)\n",
        "      A, activation_cache = self.softmax(Z)\n",
        "\n",
        "    cache = (linear_cache, activation_cache)\n",
        "    return A, cache\n",
        "\n",
        "  def L_linear_forward(self, X):\n",
        "\n",
        "    A = X\n",
        "    self.caches = []\n",
        "    \n",
        "    \n",
        "\n",
        "    AL, cache = self.linear_activation_forward(A, self.weight, self.bias, activation=self.activation)\n",
        "    self.caches.append(cache)\n",
        "\n",
        "    return AL\n",
        "\n",
        "  def compute_cost(self, AL, Y):\n",
        "    \n",
        "    AL = (AL==0) * self.eps + AL\n",
        "    Y = Y.reshape(AL.shape)\n",
        "    loss=-np.sum(Y*np.log(AL))\n",
        "    self.dAL = AL - Y\n",
        "\n",
        "    return loss/float(AL.shape[1])\n",
        "    \n",
        "\n",
        "  def linear_backward(self, dZ, cache):\n",
        "\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
        "    db = 1/m * np.sum(dZ, axis = 1, keepdims =True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "  def linear_activation_backward(self, dA, cache, activation):\n",
        "\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if self.activation == \"relu\":\n",
        "      dZ = self.relu_backward(dA, activation_cache)\n",
        "      dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
        "\n",
        "    if self.activation == \"sigmoid\":\n",
        "      dZ = self.sigmoid_backward(dA, activation_cache)\n",
        "      dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
        "\n",
        "    if self.activation == \"softmax\":\n",
        "      dZ = dA\n",
        "      dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "  def Backward(self, dA_prev=0.5):\n",
        "\n",
        "    grads = {}\n",
        "    L = len(self.caches) \n",
        "    if self.activation == 'softmax':\n",
        "      dAL = self.dAL\n",
        "    else:\n",
        "      dAL = dA_prev\n",
        "\n",
        "    current_cache = self.linear_activation_backward(dAL, self.caches[L-1], activation=self.activation)\n",
        "    d_Aprev_temp, dW_temp, db_temp = current_cache\n",
        "    grads[\"dA_prev\"] = d_Aprev_temp\n",
        "    grads[\"dW\"] = dW_temp\n",
        "    grads[\"db\"] = db_temp\n",
        "\n",
        "    for l in reversed(range(L - 1)):\n",
        "      print('lol')\n",
        "      current_cache = self.linear_activation_backward(grads[\"dA\" + str(l + 1)], self.caches[l], activation=\"relu\")\n",
        "      d_Aprev_temp, dW_temp, db_temp = current_cache\n",
        "      grads[\"dA\" + str(l)] = d_Aprev_temp\n",
        "      grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "      grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    self.grads = grads\n",
        "    return grads\n",
        "\n",
        "  def update_parameters(params, grads, learning_rate):\n",
        "    parameters = params.copy()\n",
        "    L = len(parameters) // 2 \n",
        "\n",
        "    for l in range(L):\n",
        "          \n",
        "      parameters[\"W\"+str(l+1)] -= learning_rate * grads[\"dW\"+str(l+1)] \n",
        "      parameters[\"b\"+str(l+1)] -= learning_rate * grads[\"db\"+str(l+1)] \n",
        "          \n",
        "          \n",
        "    return parameters\n",
        "\n",
        "  def softmax(self, Z):\n",
        "\n",
        "    A = special.softmax(Z)\n",
        "\n",
        "    return A, Z\n",
        "\n",
        "  def sigmoid(self, Z):\n",
        "\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "\n",
        "    return A, Z\n",
        "\n",
        "  def sigmoid_backward(self, dA, Z):\n",
        "\n",
        "    g = self.sigmoid(Z)[0] * (1 - self.sigmoid(Z)[0])\n",
        "    dZ = np.multiply(g, dA)\n",
        "\n",
        "    return dZ\n",
        "\n",
        "  def relu(self, Z):\n",
        "\n",
        "    A = Z * (Z > 0)\n",
        "\n",
        "    return A, Z\n",
        "\n",
        "  def relu_backward(self, dA, Z):\n",
        "\n",
        "    g = 1 * (Z > 0)\n",
        "    dZ = np.multiply(g, dA)\n",
        "\n",
        "    return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zC5dz4Tbbggj"
      },
      "outputs": [],
      "source": [
        "def getWindows(input, output_size, kernel_size, padding=1, stride=1, dilate=0):\n",
        "    working_input = input\n",
        "    working_pad = padding\n",
        "\n",
        "    # pad the input if necessary\n",
        "    if working_pad != 0:\n",
        "        working_input = zero_pad(working_input, padding)\n",
        "\n",
        "    if dilate != 0:\n",
        "        working_input = np.insert(working_input, range(1, input.shape[1]), 0.5, axis=1)\n",
        "        working_input = np.insert(working_input, range(1, input.shape[2]), -0.5, axis=2)\n",
        "\n",
        "    b, out_h, out_w, in_c = output_size\n",
        "    b, _, _, out_c = input.shape\n",
        "    batch_str, kern_h_str, kern_w_str, channel_str = working_input.strides\n",
        "\n",
        "    return np.lib.stride_tricks.as_strided(\n",
        "        working_input,\n",
        "        (b, out_h, out_w, out_c, kernel_size, kernel_size),\n",
        "        (batch_str, stride * kern_h_str, stride * kern_w_str, channel_str, kern_h_str, kern_w_str)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ytPdTajPf19i"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Conv2D:\n",
        "    \"\"\"\n",
        "    An implementation of the convolutional layer. We convolve the input with out_channels different filters\n",
        "    and each filter spans all channels in the input.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, pooling=False):\n",
        "        \"\"\"\n",
        "        :param in_channels: the number of channels of the input data\n",
        "        :param out_channels: the number of channels of the output(aka the number of filters applied in the layer)\n",
        "        :param kernel_size: the specified size of the kernel(both height and width)\n",
        "        :param stride: the stride of convolution\n",
        "        :param padding: the size of padding. Pad zeros to the input with padding size.\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.pooling = pooling\n",
        "\n",
        "        self.cache = None\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        self.weight = 7.5e-3 * np.random.randn(self.kernel_size, self.kernel_size, self.in_channels, self.out_channels)\n",
        "        self.bias = np.zeros((1, 1, 1, self.out_channels))\n",
        "\n",
        "    def conv_forward(self, x, activation='relu'):\n",
        "\n",
        "\n",
        "        A_prev = x\n",
        "        m, n_H_prev, n_W_prev, n_C_prev = x.shape\n",
        "        W = self.weight\n",
        "        b = self.bias\n",
        "        f, f, n_C_prev, n_C = W.shape\n",
        "\n",
        "        stride = self.stride\n",
        "        pad = self.padding\n",
        "\n",
        "        n_H = int((n_H_prev + 2*pad - f)/stride) + 1\n",
        "        n_W = int((n_W_prev + 2*pad - f)/stride) + 1\n",
        "\n",
        "        if self.pooling:\n",
        "          n_H = int(n_H/2)\n",
        "          n_W = int(n_W/2)\n",
        "\n",
        "        max, min = np.amax(x), np.amin(x)\n",
        "        \n",
        "\n",
        "        windows = getWindows(x, (m, n_H, n_W, n_C), self.kernel_size, self.padding, self.stride, dilate=0)\n",
        "        self.windows = windows\n",
        "        #windows = np.nan_to_num(windows, nan = max)\n",
        "       # windows = np.clip(windows, min, max)\n",
        "        \n",
        "        out = np.einsum('bhwikl,klio->bhwo', windows, self.weight)\n",
        "# Check out on nan valye   \n",
        "\n",
        "#        if True in np.isnan(out):\n",
        "#          out = np.nan_to_num(out, nan = max)\n",
        "#          out = np.clip(out, np.amin(out), max)\n",
        "\n",
        "        self.out_prev = out\n",
        "\n",
        "        # add bias to kernels\n",
        "        out += self.bias\n",
        "        self.cache_activation = out\n",
        "\n",
        "        out = relu(out)\n",
        "        self.out_later = out\n",
        "        # Making sure your output shape is correct\n",
        "        assert(out.shape == (m, n_H, n_W, n_C))\n",
        "\n",
        "        self.cache = (A_prev, windows)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "    def Backward(self, dout):\n",
        "\n",
        "        m = dout.shape[0]\n",
        "        stride = self.stride\n",
        "        padding = self.padding\n",
        "        W = self.weight\n",
        "        b = self.bias\n",
        "\n",
        "        max, min = np.amax(dout), np.amin(dout)\n",
        "        self.dout_pred = dout\n",
        "\n",
        "        dout = reluDerivative(self.cache_activation) * dout\n",
        "       # self.dout_after = dout\n",
        "        \n",
        "        (A_prev, windows) = self.cache\n",
        "\n",
        "        dout_windows = getWindows(dout, A_prev.shape, self.kernel_size, padding=padding, stride=1, dilate=0)\n",
        "\n",
        " #       Check dout_windows on nan value\n",
        " #       if True in np.isnan(dout_windows):\n",
        " #         dout_windows = np.nan_to_num(dout_windows, nan = max)\n",
        " #         dout_windows = np.clip(dout_windows, min, max)\n",
        "\n",
        "        self.dout_windows = dout_windows\n",
        "        rot_kern = np.rot90(self.weight, 2, axes=(2, 3))\n",
        "       # self.rot_kern = rot_kern\n",
        "\n",
        "        db = np.sum(dout, axis=(0, 1, 2))\n",
        "        dW = np.einsum('bhwikl,bhwo->klio', windows, dout)\n",
        "        dA_prev = np.einsum('bhwokl,klio->bhwi', dout_windows, self.weight) \n",
        "        \n",
        " #      Check dA_prev on exploding gradient\n",
        " #       if np.sum(dA_prev) >2 or np.sum(dA_prev)< -2: \n",
        " #         dA_prev = np.clip(dA_prev, min, max)         \n",
        "        \n",
        "        dW /= m \n",
        "        db /= m\n",
        "\n",
        "        grads = dict(dA_prev=dA_prev, dW=dW, db=db)\n",
        "        self.grads = grads\n",
        "        return grads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "e0bIcrWj5ppD"
      },
      "outputs": [],
      "source": [
        "\n",
        "class VGG16_V():\n",
        "    def __init__(self, num_classes=10, shrink=7, batch_size=1):\n",
        "\n",
        "        self.layer1 = Conv2D(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer2 = Conv2D(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, pooling=True)\n",
        "        self.layer3 = Conv2D(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer4 = Conv2D(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, pooling=True)\n",
        "        self.layer5 = Conv2D(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer6 = Conv2D(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer7 = Conv2D(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, pooling=True)\n",
        "        self.layer8 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer9 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer10 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1, pooling=True)\n",
        "        self.layer11 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer12 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.layer13 = Conv2D(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1, pooling=True)\n",
        "        self.Lnn1 = Linear(shrink*shrink*512, 4096)\n",
        "        self.Lnn2 = Linear(4096, 4096)\n",
        "        self.Lnn3 = Linear(4096, num_classes, activation='softmax')\n",
        "\n",
        "        \n",
        "    def forward(self, x, batch_size=1):\n",
        "        out = self.layer1.conv_forward(x)\n",
        "        out = self.layer2.conv_forward(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer3.conv_forward(out)\n",
        "        out = self.layer4.conv_forward(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer5.conv_forward(out)\n",
        "        out = self.layer6.conv_forward(out)\n",
        "        out = self.layer7.conv_forward(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer8.conv_forward(out)\n",
        "        out = self.layer9.conv_forward(out)\n",
        "        out = self.layer10.conv_forward(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer11.conv_forward(out)\n",
        "        out = self.layer12.conv_forward(out)\n",
        "        out = self.layer13.conv_forward(out)\n",
        "        #print(out.shape)\n",
        "        out = out.reshape(-1, batch_size)\n",
        "        #print(out.shape)\n",
        "        out = self.Lnn1.L_linear_forward(out)\n",
        "        out = self.Lnn2.L_linear_forward(out)\n",
        "        out = self.Lnn3.L_linear_forward(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, Layers, batch_size=1):\n",
        "\n",
        "      self.GR = []\n",
        "      grads = self.Lnn3.Backward()\n",
        "      self.GR.append(grads)\n",
        "\n",
        "      for l in Layers[0:2]: \n",
        "        grads = l.Backward(grads['dA_prev'])\n",
        "        self.GR.append(grads)\n",
        "      A_prev = self.layer13.cache[0]\n",
        "      dim = int(A_prev.shape[1] / 2)\n",
        "      grads['dA_prev'] = grads['dA_prev'].reshape(batch_size, dim, dim, 512)\n",
        "      for l in Layers[2:]: \n",
        "        grads = l.Backward(grads['dA_prev'])\n",
        "        self.GR.append(grads)\n",
        "\n",
        "      GR = self.GR\n",
        "\n",
        "      return GR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zvrQoZ8OkCrD"
      },
      "outputs": [],
      "source": [
        "# Recieve Layers from model\n",
        "def sp_prop_V(Model):\n",
        "\n",
        "  spisok = []\n",
        "  spisok_l = []\n",
        "\n",
        "  for l in Model.__dict__.keys():\n",
        "    spisok.append(Model.__dict__[l])\n",
        "\n",
        "  for l in reversed(spisok):\n",
        "    spisok_l.append(l)\n",
        "    \n",
        "#  del spisok_l[0]\n",
        "  \n",
        "  return spisok_l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "for i in range(10):\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "KT2DrsNIg46g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "aCBV4DZxVG0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035e967b-e318-4b97-cf4c-aa9f14aa298b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):  \n",
        "  pyplot.subplot(330 + 1 + i)\n",
        "  pyplot.imshow(x_train[i], cmap=pyplot.get_cmap('gray'))\n",
        "  pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "NDcSfTEpadeo",
        "outputId": "3a17b276-2c81-4159-c9be-e814a576df0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# expand new axis, channel axis \n",
        "x_train, x_test = np.expand_dims(x_train, axis=-1), np.expand_dims(x_test, axis=-1)\n",
        "# it's always better to normalize \n",
        "x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
        "# resize the input shape , i.e. old shape: 28, new shape: 32\n",
        "x_train, x_test = tf.image.resize(x_train, [32,32]), tf.image.resize(x_test, [32,32]) # if we want to resize \n",
        "# one-hot \n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)"
      ],
      "metadata": {
        "id": "lKp56VZBaREh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CI5oLMek5Tx",
        "outputId": "1fa99847-e750-4440-f11b-04caab82ecd1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000, 32, 32, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape, y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itU-4c1uYANx",
        "outputId": "2f307afa-38d8-4d67-9f22-1c84878ff045"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 32, 32, 1) (60000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch_size\n",
        "X_train, Y_train, X_val, Y_val = [], [], [], []\n",
        "batch_size = 3\n",
        "for i in range(0, 60, batch_size):\n",
        "  start_idx, end_idx = i, i + batch_size\n",
        "  X_train.append(x_train[start_idx:end_idx])\n",
        "  Y_train.append(y_train[start_idx:end_idx].reshape(10, batch_size))\n",
        "\n",
        "for i in range(0, 10, batch_size):\n",
        "  start_idx, end_idx = i, i + batch_size\n",
        "  X_val.append(x_test[start_idx:end_idx])\n",
        "  Y_val.append(y_test[start_idx:end_idx].reshape(10, batch_size))"
      ],
      "metadata": {
        "id": "ffBv_S4ER104"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCNF6s2Rjwzx",
        "outputId": "f0774179-62d1-4d7f-bb35-09e32b7fbaf3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 32, 32, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = X_val[0]\n",
        "\n",
        "shrink = x.shape[1]//32\n",
        "batch_size = x.shape[0]\n",
        "print('shrink = ', shrink, 'batch_size = ', batch_size)\n",
        "\n",
        "model = VGG16_V(num_classes=10, shrink=shrink)\n",
        "params = sp_prop_V(model)\n",
        "#print(params)\n",
        "params.reverse()\n",
        "opt = AdaBound_N(params, lr=1e-3, betas=(0.9, 0.999), final_lr=7.5e-3, gamma=1e-5,\n",
        "                 eps=1e-8, weight_decay=0, amsbound=False)\n",
        "\n",
        "y = Y_val[2]\n",
        "print(y.reshape(y.shape[1], y.shape[0]))\n",
        "\n",
        "running_loss = 0\n",
        "Layers = sp_prop_V(model)\n",
        "num_epoch = 10\n",
        "\n",
        "for i in range(1, num_epoch + 1):\n",
        "\n",
        "  y_true, y_preds, TR, PR = [], [], [], []\n",
        "  running_loss = 0\n",
        "  opt.initialize_state(params)\n",
        "\n",
        "  for x, y in zip(X_val[2:3], Y_val[2:3]):\n",
        "\n",
        "    out = model.forward(x, batch_size)\n",
        "    loss = model.Lnn3.compute_cost(out, y)\n",
        "    running_loss += loss\n",
        "    grads = model.backward(Layers[1:], batch_size)\n",
        "    opt.step(i)\n",
        "\n",
        "    out, y = out.reshape(out.shape[1], out.shape[0]), y.reshape(y.shape[1], y.shape[0])\n",
        "    print(out)\n",
        "    y_preds.extend(out)\n",
        "    y_true.extend(y)\n",
        "\n",
        "    PR.extend(np.argmax(out, axis=1))\n",
        "    TR.extend(np.argmax(y, axis=1))\n",
        "\n",
        "  \n",
        "  \n",
        "  y_true = np.array(y_true, dtype=np.int16)\n",
        "  y_preds = np.array(y_preds, dtype=np.int16)\n",
        "\n",
        "  print('loss = ', running_loss)\n",
        "  print('f1_macro = ', f1_score(TR, PR, average='macro'))\n",
        "  print('accuracy = ', accuracy_score(TR, PR))"
      ],
      "metadata": {
        "id": "0U0vpb7shFov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b8ddca79-e6b7-4e72-c40b-0e6cb4a524f9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shrink =  1 batch_size =  3\n",
            "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "[[0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333\n",
            "  0.03333333 0.03333333 0.03333333 0.03333333]\n",
            " [0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333\n",
            "  0.03333333 0.03333333 0.03333333 0.03333333]\n",
            " [0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333\n",
            "  0.03333333 0.03333333 0.03333333 0.03333333]]\n",
            "loss =  3.4011973816621595\n",
            "f1_macro =  0.16666666666666666\n",
            "accuracy =  0.3333333333333333\n",
            "[[0.0332149  0.03321411 0.03321504 0.03354191 0.03354694 0.0335434\n",
            "  0.03324828 0.03324787 0.03324689 0.0332295 ]\n",
            " [0.0332327  0.03323057 0.03325035 0.03325054 0.03325019 0.03325457\n",
            "  0.03325434 0.03325334 0.03355481 0.03355698]\n",
            " [0.03355508 0.03326721 0.03326664 0.03326812 0.03351235 0.03351505\n",
            "  0.03351294 0.03325434 0.0332563  0.03325476]]\n",
            "loss =  3.3950269023941506\n",
            "f1_macro =  0.5\n",
            "accuracy =  0.6666666666666666\n",
            "[[0.03301582 0.03301525 0.03301651 0.0339658  0.03398217 0.03396646\n",
            "  0.03305829 0.03305715 0.03305734 0.03302746]\n",
            " [0.03302929 0.03302917 0.03307747 0.0330764  0.03307763 0.03306012\n",
            "  0.03305918 0.03305912 0.03399833 0.03401347]\n",
            " [0.0339983  0.03311565 0.03311539 0.03311614 0.03393022 0.03394413\n",
            "  0.03392807 0.03306992 0.03307075 0.03306897]]\n",
            "loss =  3.3819857157747353\n",
            "f1_macro =  0.5\n",
            "accuracy =  0.6666666666666666\n",
            "[[0.03273626 0.03272428 0.03273628 0.03456266 0.03461647 0.03456279\n",
            "  0.03280243 0.03278957 0.03280209 0.03275072]\n",
            " [0.03274427 0.03275274 0.03283976 0.03282785 0.03283986 0.0328063\n",
            "  0.03279706 0.03280567 0.03460636 0.03465867]\n",
            " [0.03460527 0.032891   0.03288222 0.03289102 0.03449865 0.03454715\n",
            "  0.03449741 0.03281046 0.03280345 0.03281127]]\n",
            "loss =  3.36368768708712\n",
            "f1_macro =  0.5\n",
            "accuracy =  0.6666666666666666\n",
            "[[0.03233855 0.03228686 0.0323369  0.03543059 0.03559102 0.03543394\n",
            "  0.03242313 0.03237351 0.03242105 0.0323494 ]\n",
            " [0.03230564 0.03235147 0.032488   0.03244027 0.03248634 0.0324363\n",
            "  0.03239205 0.03243448 0.0354952  0.03565561]\n",
            " [0.03549479 0.03255607 0.03251672 0.03255564 0.03533301 0.03548283\n",
            "  0.03533251 0.03243423 0.03239082 0.0324331 ]]\n",
            "loss =  3.336072414917698\n",
            "f1_macro =  0.5\n",
            "accuracy =  0.6666666666666666\n",
            "[[nan nan nan nan nan nan nan nan nan nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan]]\n",
            "loss =  nan\n",
            "f1_macro =  0.0\n",
            "accuracy =  0.0\n",
            "[[nan nan nan nan nan nan nan nan nan nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan]]\n",
            "loss =  nan\n",
            "f1_macro =  0.0\n",
            "accuracy =  0.0\n",
            "[[nan nan nan nan nan nan nan nan nan nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan]]\n",
            "loss =  nan\n",
            "f1_macro =  0.0\n",
            "accuracy =  0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-35f496999dc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-df2df3db3262>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mstep_size_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mstep_size_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_size_W\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mstep_size_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_size_b\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PPlThG1OTavg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}